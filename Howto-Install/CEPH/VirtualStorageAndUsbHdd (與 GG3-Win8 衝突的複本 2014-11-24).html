<OL>
  <LI><a href="http://www.yasith.info/2010/01/how-to-check-hard-disk-performance-and.html" 
   target="newwindow">Hard Disk Info</a>&nbsp;&nbsp;
   <a href="http://www.google.com.tw/search?hl=zh-TW&as_q=Debian+hard+disk+speed&as_epq=&as_oq=&as_eq=Microsoft+Windows&as_nlo=&as_nhi=&lr=&cr=&as_qdr=y&as_sitesearch=&as_occt=any&safe=images&as_filetype=&as_rights=" 
   target="newwindow">Debian hard disk speed</a>&nbsp;&nbsp;
   <a href="http://www.damtp.cam.ac.uk/user/ejb48/sshspeedtests.html" 
   target="newwindow">ssh speed tests</a>&nbsp;&nbsp;
  <LI><a href="http://www.howtoforge.com/high-availability-storage-with-glusterfs-3.0.x-on-debian-squeeze-automatic-file-replication-across-two-storage-servers" 
   target="newwindow">Storage With GlusterFS 3</a>&nbsp;&nbsp;
<a href="http://www.google.com.tw/search?hl=zh-TW&lr=&tbs=qdr%3Ay&q=Debian+Kvm+virtual+storage+server+-Microsoft+-Windows&oq=Debian+Kvm+virtual+storage+server+-Microsoft+-Windows&gs_l=serp.3...10518.21193.0.23721.20.20.0.0.0.0.67.719.20.20.0...0.0...1c.9-b6LYdGMa8" 
   target="newwindow">Debian Kvm virtual storage server</a>
  <LI><a href="http://intrl.startech.com/Cards-Adapters/USB-3.0/Cards/2-Port-PCI-Express-SuperSpeed-USB-3-Card-Adapter~PEXUSB3S2" 
   target="newwindow">PCIE USB 3.0 Card</a>&nbsp;&nbsp;
   <a href="http://www.bonafidereviews.com/review-of-usb-3-0-gigabyte-ultra-durable-3-pci-express-card-ga-usb3-0/" 
   target="newwindow">GA-USB3.0</a>&nbsp;&nbsp;
   <a href="http://blog.testfreaks.com/review/review-of-pci-express-usb-3-0-host-controller-card-and-usb3-0-extension-cable/" 
   target="newwindow">Review USB3.0</a>
  <LI><a href="#StorageTerms" target="newwindow">Storage Terminologies</a>&nbsp;&nbsp;
   <a href="#Ceph" 
   target="newwindow">Ceph Storage</a>&nbsp;&nbsp;
  <LI><a href="" 
   target="newwindow"></a>
</OL>

<a name="StorageTerms"></a><h3>Storage: objects, blocks, and files 
<a href="http://docs.openstack.org/trunk/openstack-compute/install/content/terminology-storage.html">(Source Origin)</a></h3>

<p>Many cloud computing use cases require persistent remote storage.  Storage 
solutions are often divided into three categories: object storage, block storage, and 
file storage.</p>

<p>Note that some storage solutions support multiple categories. For example, 
NexentaStor supports both block storage and file storage (with announcements for 
future support for object storage), GlusterFS supports file storage and object 
storage, and  Ceph Storage supports object storage, block storage, and file storage.</p>

<h4><a href="http://www.wwpi.com/index.php?option=com_content&view=article&id=14659:object-storage-where-cloud-computing-and-big-data-meet&catid=317:ctr-exclusives&Itemid=2701734" target="newwindow">Object storage</a></h4>
<p>In OpenStack: Object Storage service (Swift) 
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Related concepts: Amazon S3, Rackspace 
Cloud Files, Ceph Storage</p>

<p>With <span class="italic">object storage</span>, files are exposed through
  an HTTP interface, typically with a REST API. All client data access is done at 
  the user level: the operating system is unaware of the presence of the remote 
  storage system. In OpenStack, the Object Storage service provides this type of
  functionality. Users access and modify files by making HTTP requests. Because the
  data accesss interface provided by an object storage system is at a low level of
  abstraction, people often build on top of object storage to build file-based
  applications that provide a higher level of abstraction. For example, the OpenStack
  Image service can be configured to use the Object Storage service as a backend.
  Another use for object storage solutions is as a content delivery network (CDN) for
  hosting static web content (e.g., images, and media files), since object storage
  already provides an HTTP interface. </p>

<h4>Block storage (<a href="http://en.wikipedia.org/wiki/Storage_area_network" 
target="newwindow">SAN</a>)</h4>
<p>In OpenStack: Volumes (<code class="systemitem">nova-volume</code> service) 
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Relatd concepts: Amazon Elastic Block Store (EBS), 
  Ceph RADOS Block Device (RBD), iSCSI</p>
<p>With <span class="italic">block storage</span>, files are exposed through a
  low-level computer bus interface such as SCSI or ATA, that is accessible over the
  network. Block storage is synonymous with SAN (storage area network). Clients access
  data through the operating system at the device level: users access the data by
  mounting the remote device in a similar manner to how they would mount a local,
  physical disk (e.g., using the "mount" command in Linux). In OpenStack, the
      <code class="systemitem">nova-volume</code> service that forms part of
  the Compute service provides this type of functionality, and uses iSCSI to expose
  remote data as a SCSI disk that is attached to the network. </p>
<p>Because the data is exposed as a physical device, the end-user is responsible for
  creating partitions and formatting the exposed disk device. In addition, in
  OpenStack Compute a device can only be attached to one server at a time, so block
  storage cannot be used for to share data across virtual machine instances
  concurrently. </p>

<h4>File storage (<a href="http://en.wikipedia.org/wiki/Network-attached_storage" 
target="newwindow">NAS</a>)</h4>
<p>In OpenStack: none 
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Related concepts: NFS, Samba/CIFS, GlusterFS, Dropbox, 
Google Drive</p>
<p>With <span class="italic">file storage</span>, files are exposed through a
  distributed file system protocol. Filesystem storage is synonymous with NAS (network
  attached storage). Clients access data through the operating system at the file
  system level: users access the data by mounting a remote file system. Examples of
  file storage include NFS and GlusterFS. The operating system needs to have the
  appropriate client software installed to be able to access the remote file system. </p>
<p>Currently, OpenStack Compute does not have any native support for this type of
  file storage inside of an instance. However, there is a 
<a  href="http://gluster.org/community/documentation//index.php/OSConnect" 
target="_top">Gluster storage connector for OpenStack</a> that enables the use of the
  GlusterFS file system as a back-end for the Image service.</p>

<h4 class="entry-title">CDN-as-a-Platform for Cloud Services 
<a href="http://www.verivue.com/blog/index.php/content-cloud-computing-strategies/cdn-as-a-platform-for-cloud-services/" target="newwindow">(Source Origin)</a></h4>

<div class="entry-meta">
  Posted on July 24, 2012 by Larry Peterson</div>

<div class="entry-content">

<p>I've recently posted several articles about the advantages of 
running network functions on virtualized commodity servers, and while 
it's important to make the general case for replacing purpose-built 
hardware appliances with commodity servers, there's a more focused story
 centered around CDNs that's worth telling.</p>
<p>It involves a slight change in perspective. Rather than view a CDN as
 one of many services that can be deployed on a virtualized platform-in 
an earlier <a title="article" 
href="http://www.verivue.com/blog/index.php/content-cloud-computing-strategies/network-virtualization/">article</a> 
I talked about a spectrum of services ranging from BRAS to CDN-think of the 
CDN <em>as the platform</em> on which an important subset of network functions 
are deployed... those related to content delivery. Virtual machines are an 
enabling technology, but the important point is that a CDN serves as the 
cornerstone for a rich collection of functions that enable network 
operators to sell web acceleration solutions to their business and 
enterprise customers.</p>

<p>In other words, it's fair to view a CDN as a platform that hosts 
functions to accelerate B2C and B2B transactions over the Internet, 
especially transactions that involve cloud-based services. In this 
scenario, the CDN runs at the edge of the operator's network, where in 
addition to caching static objects, it also hosts client-facing and 
cloud-facing optimizations. The client-facing optimizations, often 
collectively called <em>front-end optimization</em>, include an 
assortment of techniques aimed at reducing transaction response time, as
 well as SSL termination, TCP enhancements for mobile applications, and 
business logic offload. The cloud-facing optimizations, sometimes called
 <em>WAN acceleration</em>, include symmetric strategies for compression
 and de-duplication. (It is notable that these latter techniques are 
typically symmetric because the CDN provides a point-of-presence both in
 the data center and at the edge of the network.)</p>
<p>An architecture for CDN-as-a-Platform has two major elements. The 
first is edge service nodes that not only cache static objects, but also
 run a policy engine that governs how the CDN interacts with clients 
that make requests and public/private clouds that host business 
services. This policy engine receives and processes client HTTP 
requests, dispatches the request to the appropriate module for 
processing, and when communication with the data center is required 
(e.g., to satisfy cache misses or to access dynamic state), selects the 
appropriate module to optimize communication with those servers. In 
Verivue's case, some of these modules are part of the OneVantage product
 portfolio (some run their own VMs and others are cache plug-ins), while
 others are provided by third-party service venders (these are typically
 isolated in their own VMs).</p>
<p>The second element of the CDN-as-a-Platform architecture is the 
lynchpin - a&nbsp; unified approach to service management. The management 
suite is responsible for provisioning virtual machines, configuring the 
services deployed in those virtual machines, proactively monitoring the 
deployment, and collecting traffic logs for billing and analytics. The 
core of the management suite is a data model that presents operators 
with a comprehensive and coherent picture of the available network 
functions that they must manage.</p>
<p>A clear understanding of&nbsp; this data model is starting to emerge.
 It includes objects that model the central stake-holders, including CDN
 Operators, Service Providers, and Content Providers (i.e., business 
customers); objects that model the deployed infrastructure, including 
virtual/physical nodes and network interfaces; objects that model the 
set of services and modules instantiated on that infrastructure; and 
objects that model the set of policy directives that govern how those 
services and modules behave.</p>
<p>These rules and policies, in turn, include: (1) routing directives 
that govern how end-users are routed to the best service node to process
 a given request, (2) delivery directives that govern how the selected 
service node delivers the resources named in the request, (3) anchor 
directives that govern how the service node interacts with cloud-hosted 
business services that anchor the request, and (4) analytic directives 
that govern how the service node gathers and processes traffic 
statistics. In other words, these rules collectively control what 
service node is selected to serve a given end-user, what module(s) are 
invoked at that node to customize delivery for that particular user, how
 those modules are parameterized to serve a particular end-user, and how
 data is collected.</p>
<p>Coordinating these policy directives across a set of services and 
modules requires a unifying abstraction that defines the scope for each 
directive. We call the scope a <em>delivery domain</em>, and it 
corresponds to the subset of URIs to which a given set of rules and 
policies is to be applied. A delivery domain is represented (identified)
 in one of two ways. The first is a CDN-Prefix, which corresponds to the
 FQDN at the beginning of a URI; it effectively carves out a region of 
the URI name space to which a set of rules and policies are to be 
applied. The second is a URI-Filter, which is given by a regular 
expression; it effectively identifies a subset of URIs belonging to a 
CDN-Prefix that is to be treated in a uniform way.</p>
<p>In summary, a CDN-powered platform allows business and enterprise 
customers to accelerate their cloud-hosted web services by effectively 
extending the cloud from the data center out to the edge of the 
operator's network. Service nodes deployed at the network edge provide 
the optimal vantage point to co-locate caching, front-end optimization, 
and WAN acceleration technologies, where the management suite plays a 
central role in coordinating the resulting data center-to-edge cloud on 
behalf of B2C and B2B customers.</p>

<div id="author-link">
  <a href="http://www.verivue.com/blog/index.php/author/lpeterson/">View all 
     posts by Larry Peterson</a></div>


<h4>Trends in Cloud Storage: CDN 
<a href="http://www.verivue.com/blog/index.php/content-cloud-computing-strategies/trends-in-cloud-storage/" target="newwindow">(Source Origin)</a></h4>

<div class="entry-meta">
Posted on August 20, 2012 by Larry Peterson</div>

<div class="entry-content">
<p>Here's a modest insight. When designing a cloud storage system,
 there is value in decoupling the system's archival capacity (its 
ability to persistently store large volumes of data) from the system's 
delivery capacity (its ability to deliver popular objects to a scalable 
number of users). The archival half need not support scalable 
performance, and likewise, the delivery half need not guarantee 
persistence.</p>
<p>In practical terms, this translates into an end-to-end storage 
solution that includes a high-capacity and highly resilient object store
 in the data center, augmented with caches throughout the network to 
take advantage of aggregated delivery bandwidth from edge sites. This is
 similar to what Amazon offers today: S3 implements a resilient object 
store in the data center, augmented with CloudFront to scale delivery 
through a distributed set of edge caches.</p>
<p>The object store runs in the data center, ingests data from some 
upstream source (e.g., video prepared using a Content Management 
System), and delivers it to users via edge caches. The ingest interface 
is push-based and likely includes one or more popular APIs (e.g., FTP, 
WebDAV, S3), while the delivery interface is pull-based and corresponds 
to HTTP GET requests from the CDN.</p>
<p>In past articles I have written extensively about how to architect a 
CDN that can be deployed throughout an operator network, claiming that a
 well-designed CDN should be agnostic as to the source of content. But 
it is increasingly the case that content delivered over a CDN is sourced
 from a data center as part of a cloud-based storage solution. This begs
 the question: is there anything we can learn by looking at storage from
 such an end-to-end perspective?</p>
<p>I see three points worth making, although in way of a disclaimer, I'm
 starting from the perspective of the CDN, and looking back to what I'd 
like to see from a data center based object store. The way I see it, 
though, there's more value in storing data if you have a good approach 
to distributing it to users that want to access it.</p>
<p>First, it makes little sense to build an object store using 
traditional SAN or NAS technology. This is for two reasons. One has to 
do with providing the right level of abstraction. In this case, the CDN 
running at the network edge is perfectly capable of dealing with a large
 set of objects, meaning there is no value in managing those objects 
with full file system semantics (i.e., NAS is a bad fit). Similarly, the
 storage system needs to understand complete objects and not just blocks
 (i.e., SAN is not a good fit). The second reason is related to cost. It
 is simply more cost effective to build a scalable object store from 
commodity components. This argument is well understood, and leverages 
the ability to achieve scalable performance and resiliency in software.</p>
<p>Second, a general-purpose CDN that is able to deliver a wide range of
 content-from software updates to video, from large files to small 
objects, from live (linear) streams to on-demand video, from 
over-the-top to managed video-should not be handicapped by an object 
store that isn't equally flexible. In particular, it is important that 
the ingest function be low-latency and redundant, so it is possible to 
deliver both on-demand and live video. (Even live video needs to be 
staged through an object store to support time shifting.)</p>
<p>Third, it is not practical to achieve scalable delivery from a data 
center. Data centers typically provide massive internal bandwidth, 
making it possible to build scalable storage from commodity servers, but
 Internet-facing bandwidth is generally limited. This is just repeating 
the argument in favor of delivering content via a CDN-scalable delivery 
is best achieved from the edge.</p>

<h4>About Larry Peterson</h4>
As Chief Scientist, Larry Peterson provides technical leadership 
and expertise for research and development projects. He is also the 
Robert E. Kahn Professor of Computer Science at Princeton University, 
where he served as Chairman of the Computer Science Department from 
2003-2009. He also serves as Director of the PlanetLab Consortium, a 
collection of academic, industrial, and government institutions 
cooperating to design and evaluate next-generation network services and 
architectures.

Larry has served as Editor-in-Chief of the ACM Transactions on Computer 
Systems, has been on the Editorial Board for the IEEE/ACM Transactions 
on Networking and the IEEE Journal on Select Areas in Communication and 
is the co-author of the best selling networking textbook Computer 
Networks: A Systems Approach. He is a member of the National Academy of 
Engineering, a Fellow of the ACM and the IEEE, and the 2010 recipient of
 the IEEE Kobayahi Computer and Communication Award. He received his 
Ph.D. degree from Purdue University in 1985. 

<a name="Ceph"></a><h3>Ceph: Open Source Storage</h3>

<EM><P> As the size and performance requirements of storage systems have increased, 
file system designers have looked to new architectures to facilitate system 
scalability.  Ceph's architecture consists of an object storage, block storage and 
a POSIX-compliant file system. It's in the most significant storage system that has 
been accepted into the Linux kernel. Ceph has both kernel and userland 
implementations.  The CRUSH algorithm controlled, scalable, decentralized placement 
of replicated data. In addition, it has a fully leveraged, highly scalable metadata 
layer. Ceph offers compatibility with S3, Swift and Google Storage and is a drop in 
replacement for HDFS (and other File Systems).

<P> Ceph is unique because it's massively scalable to the exabyte level. The storage 
system is self-managing and self-healing which means limited system administrator 
involvement. It runs on commodity hardware, has no single point of failure, 
leverages an intelligent storage node system and it open source.</em>

<p> <a href="http://www.informationweek.com/news/storage/systems/232901454" 
   target="newwindow">(Source Origin:)</a> A <a href="http://ceph.com">Ceph 
system</a> is built on industry-standard servers and consists of nodes which handle 
either file-based, block, or SAN-based or object-based storage. A Ceph cluster 
consists of a portable operating system interface 
(<a href="http://en.wikipedia.org/wiki/POSIX">POSIX</a>)-compliant file system, 
storage nodes, a metadata server daemon (or computer program), and monitor daemons 
that track the state of the cluster and the nodes in the cluster.  Ceph uses an 
algorithm called CRUSH (controlled scalable decentralized placement of replicated 
data) to define where objects store data in the cluster and also track modified 
content for placement on the appropriate media.

<p>
Ceph can also be deployed as a block-based storage system. In this configuration, Ceph 
is mounted as a thin-provisioned block device. When data is written to Ceph, it is 
automatically striped and replicated across the cluster. The Ceph RADOS Block Device 
(RDB) works with KVM and supports the import and export of virtual machine images and 
provides snapshot capability.
</p>

<p>
The system can also serve as a file system where it maps the directories and file names 
of the file system to objects stored in RADOS clusters. The size of these clusters can 
be expanded or contracted and the workload is automatically rebalanced. 
<p>
Like file system clusters such as Gluster and Lustre, Ceph is scalable to multiple 
exabytes of data. Ceph is included in the Linux kernel and integrated into the 
OpenStack project. 
<p>
Because, like other open source projects, Ceph can be difficult to install, configure, 
and maintain, Inktank became the official sponsor of Ceph and will provide not only 
installation and configuration, performance testing, and infrastructure assessment 
services, but support for Ceph itself. The company has developed a community for Ceph 
users where they can chat about Ceph implementation and other issues.
<p>
Ceph was designed by Sage Weil, CEO and founder of Inktank, as part of a PhD thesis at 
the University of California at Santa Cruz. Weil released Ceph into the open source 
community in 1997. Weil is also a co-founder of DreamHost, the hosting company that 
developed Ceph and spun it off to Inktank.
<p>
<a href="http://en.wikipedia.org/wiki/Ceph#Etymology">Ceph is named after</a> the UC 
Santa Cruz mascot, Sammy, a banana slug mollusk. Ceph is short for cephalopods, a class 
of mollusks. Since nearly all mollusks release ink, it's likely that Inktank's name 
also derives from UC Santa Cruz's mascot.


<OL>
  <LI><a href="http://en.wikipedia.org/wiki/Ceph" 
   target="newwindow">Ceph Wiki</a>&nbsp;&nbsp; 
   <a href="#CephIntro" 
   target="newwindow">Ceph Intro</a>&nbsp;&nbsp;Refs:&nbsp;&nbsp;
   <a href="http://storageconference.org/2012/Speakers/Weil.html" 
   target="newwindow">ceph: distributed storage</a>&nbsp;&nbsp;
   <a href="http://upload.wikimedia.org/wikipedia/commons/2/27/Banana_slug_at_UCSC.jpg" 
   target="newwindow">ceph.=cephalopods</a>
  <LI><b>Tutorials:</b>&nbsp;&nbsp;
   <a href="#RadosAndCephFS" 
   target="newwindow">RadosAndCephFS</a>&nbsp;&nbsp;
   <a href="#CephKvm" 
   target="newwindow">Ceph Storage on Kvm-virt</a>&nbsp;&nbsp;
   <a href="#CephTutorial" 
   target="newwindow">Ceph cluster and RBD volume</a>&nbsp;&nbsp;
  <LI><a href="http://ceph.com/ceph-storage/file-system/" 
   target="newwindow">ceph fs</a>&nbsp;&nbsp;
      <a href="http://ceph.com/ceph-storage/block-storage/" 
   target="newwindow">Block Storage</a>&nbsp;&nbsp;
      <a href="http://ceph.com/ceph-storage/object-storage/" 
   target="newwindow">Object Storage</a>&nbsp;&nbsp;
   <a href="http://ceph.com/resources/publications/" 
   target="newwindow">publications</a>&nbsp;&nbsp;
   <a href="http://eu.ceph.com/docs/" 
   target="newwindow">eu ceph</a>&nbsp;&nbsp;
   <a href="http://ceph.com/docs/master/" 
   target="newwindow">ceph docs</a>&nbsp;&nbsp;
      <a href="http://ceph.com/" 
   target="newwindow">ceph</a>&nbsp;&nbsp;
  <LI><a href="#CephOpenStack" 
   target="newwindow">Ceph In OpenStack</a>&nbsp;&nbsp;
   <a href="#NFSOverRBD" 
   target="newwindow">NFS Over RBD</a>&nbsp;&nbsp;
   <a href="http://www.sebastien-han.fr/blog/2012/06/24/use-rbd-on-a-client/" 
   target="newwindow">Use RBD</a>&nbsp;&nbsp;

  <LI><a href="http://ceph.com/docs/master/rbd/rbd/" 
   target="newwindow">CephDoc RBD</a>&nbsp;&nbsp;
   <a href="#CephBenchmark" 
   target="newwindow">Ceph and RBD benchmarks</a>&nbsp;&nbsp;
  <LI><a href="http://www.google.com.tw/search?hl=zh-TW&lr=&biw=849&bih=912&tbs=qdr%3Ay&
q=Ceph+Filesystem+Tutorial&oq=Ceph+Filesystem+Tutorial&
gs_l=serp.12..0i10i30.8129.22742.0.31412.9.9.0.0.0.0.53.381.9.9.0...0.0...1c.E7Z7XGNAg3c" 
   target="newwindow">Ceph Filesystem Tutorial In Google</a>&nbsp;&nbsp;
<a href="http://www.google.com.tw/search?lr=&biw=852&bih=912&tbs=qdr%3Ay&
q=Ceph+rbd+Tutorial&oq=Ceph+rbd+Tutorial
&gs_l=serp.3...5278.10017.0.11391.13.11.0.0.0.2.112.593.10j1.11.0...0.0...1c.1.lWTM_584O6Q" 
   target="newwindow">Ceph rbd Tutorial</a>&nbsp;&nbsp;
  <LI><a href="http://ceph.com/docs/master/man/8/" 
   target="newwindow">Ceph Admin. Commands</a>&nbsp;&nbsp;
   <a href="http://ceph.com/docs/master/man/1/" 
   target="newwindow">Ceph Shell Command</a>&nbsp;&nbsp;
   <a href="http://ceph.com/docs/master/man/8/ceph/" 
   target="newwindow">Ceph Control utility</a>&nbsp;&nbsp;
  <LI><a href="http://ceph.com/w/index.php?title=Cluster_configuration" 
   target="newwindow">Ceph Cluster Config</a>&nbsp;&nbsp;
   <a href="http://ceph.com/docs/master/rados/config-cluster/" 
   target="newwindow">Ceph Cluster Config Docs</a>&nbsp;&nbsp;
   <a href="http://ceph.com/docs/master/ops/" 
   target="newwindow">Ceph Operations</a>&nbsp;&nbsp;
   <a href="http://ceph.com/docs/master/dev/" 
   target="newwindow">Ceph Internals</a>&nbsp;&nbsp;
  <LI><a href="http://ceph.com/w/index.php?title=Rbd" 
   target="newwindow">Ceph Rbd</a>&nbsp;&nbsp;
   <a href="http://ceph.com/w/index.php?title=QEMU-RBD" 
   target="newwindow">QEMU-RBD</a>&nbsp;&nbsp;
   <a href="http://jcftang.github.com/" 
   target="newwindow">Ceph Cluster</a>&nbsp;&nbsp;
   
  <LI><a href="http://en.wikipedia.org/wiki/ISCSI" 
   target="newwindow">iSCSI Wiki</a>&nbsp;&nbsp;
   <a href="#CephiScsi" target="newwindow">Ceph ISCSI Wiki</a>&nbsp;&nbsp;
   <a href="http://pjack1981.blogspot.tw/2011/09/export-ceph-rbd-with-iscsi.html" 
   target="newwindow">Rbd via ISCSI</a>&nbsp;&nbsp;
   <a href="http://wiki.debian.org/SAN/iSCSI" 
   target="newwindow">Debian iSCSI Wiki</a>&nbsp;&nbsp;
  <a href="http://www.howtoforge.com/using-iscsi-on-debian-squeeze-initiator-and-target" 
   target="newwindow">Debian iSCSI</a>&nbsp;&nbsp;
   <a href="http://christophe.varoqui.free.fr/refbook.html" 
   target="newwindow">MultiPath</a>&nbsp;&nbsp;
  <LI><a href="#CephFromHastexo" 
   target="newwindow">Ceph And Rados from Hastexo.com</a>&nbsp;&nbsp;
   <a href="" 
   target="newwindow"></a>&nbsp;&nbsp;
   <a href="" 
   target="newwindow"></a>&nbsp;&nbsp;
  <LI><a href="" 
   target="newwindow"></a>&nbsp;&nbsp;
   <a href="" 
   target="newwindow"></a>&nbsp;&nbsp;
   <a href="" 
   target="newwindow"></a>&nbsp;&nbsp;
</OL>

</a>

<a name="CephIntro"></a><h3>Ceph: Scalable distributed file system
 <a href="http://www.ibm.com/developerworks/linux/library/l-ceph/index.html">(Source 
Origin)</a></h3>
<p><em>Exploring the Ceph file system and ecosystem</em></p>
</div>


<div class="author">M. Tim Jones, Independent author</div>

<p></p>
<p><strong>Summary:</strong>&nbsp; Linux continues to invade the scalable computing 
 space and, in particular, the scalable storage space. A recent addition to Linux's
 impressive selection of file systems is Ceph, a distributed file system that
 incorporates replication and fault tolerance while maintaining POSIX
 compatibility. Explore the architecture of Ceph and learn how it provides
 fault tolerance and simplifies the management of massive amounts of
 data.</p>

<h4>Intro.</h4>

 <p>As an architect in the storage industry, I have an affinity to file
     systems. These systems are the user interfaces to storage systems, and
     although they all tend to offer a similar set of features, they also can
     provide notably different features. Ceph is no different, and it offers
     some of the most interesting features you'll find in a file system. </p>
 <p>Ceph began as a PhD research project in storage systems by Sage Weil at the
     University of California, Santa Cruz (UCSC). But as of late March 2010,
     you can now find Ceph in the mainline Linux kernel (since 2.6.34).
     Although Ceph may not be ready for production environments, it's still
     useful for evaluation purposes. This article explores the Ceph file system
     and the unique features that make it an attractive alternative for
     scalable distributed storage. </p>

<h4>Why "Ceph"?</h4>
     
<p>"Ceph" is an odd name for a file system and breaks the typical acronym trend that 
   most follow.  The name is a reference to the mascot at UCSC (Ceph's origin), which 
   happens to be "Sammy," the banana slug, a shell-less mollusk in the cephalopods 
   class. Cephalopods, with their multiple tentacles, provide a great metaphor for a 
   distributed file system. </p>

<h4><a name="goals"><span class="atitle">Ceph goals</span></a></h4>
 
<a name="name"></a>            
 <p>Developing a distributed file system is a complex endeavor, but it's immensely 
    valuable if the right problems are solved. Ceph's goals can be simply defined 
    as: </p>

 <ul>
     <li>Easy scalability to multi-petabyte capacity</li>
     <li>High performance over varying workloads (input/output operations per
    second [IOPS] and bandwidth) </li>
     <li>Strong reliability</li>
 </ul>

 <p>Unfortunately, these goals can compete with one another (for example,
     scalability can reduce or inhibit performance or impact reliability). Ceph
     has developed some very interesting concepts (such as dynamic metadata
     partitioning and data distribution and replication), which this article
     explores shortly. Ceph's design also incorporates fault-tolerance features
     to protect against single points of failure, with the assumption that
     storage failures on a large scale (petabytes of storage) will be the norm
     rather than the exception. Finally, its design does not assume particular
     workloads but includes the ability to adapt to changing distributed
     workloads to provide the best performance. It does all of this with the
     goal of POSIX compatibility, allowing it to be transparently deployed for
     existing applications that rely on POSIX semantics (through Ceph-proposed
     enhancements). Finally, Ceph is open source distributed storage and part
     of the mainline Linux kernel (2.6.34). </p>          

<h4><a name="architecture"><span class="atitle">Ceph architecture</span></a></h4>
 <p>Now, let's explore the Ceph architecture and its core elements at a high
     level. I then dig down another level to identify some of the key aspects
     of Ceph to provide a more detailed exploration. </p>
 <p>The Ceph ecosystem can be broadly divided into four segments (see Figure
     1): clients (users of the data), metadata servers (which cache and
     synchronize the distributed metadata), an object storage cluster (which
     stores both data and metadata as objects and implements other key
     responsibilities), and finally the cluster monitors (which implement the
     monitoring functions). </p>

 
 <br><a name="fig1"><b>Figure 1. Conceptual architecture of the Ceph ecosystem</b>
     </a><br>
  <img alt="Conceptual flowchart showing the architecture of the Ceph ecosystem: 
   clients, metadata server cluster, object storage cluster, and cluster monitors" 
   src="http://www.ibm.com/developerworks/linux/library/l-ceph/figure1.gif" 
   height="317" width="381">
 <br>

 <p>As Figure 1 shows, clients perform metadata operations (to identify the
     location of data) using the metadata servers. The metadata servers manage
     the location of data and also where to store new data. Note that metadata
     is stored in the storage cluster (as indicated by "Metadata I/O"). Actual
     file I/O occurs between the client and object storage cluster. In this
     way, higher-level POSIX functions (such as open, close, and rename) are
     managed through the metadata servers, whereas POSIX functions (such as read
     and write) are managed directly through the object storage cluster. </p>
 <p>Another perspective of the architecture is provided in Figure 2. A set of 
    servers access the Ceph ecosystem through a client
     interface, which understands the relationship between metadata servers and
     object-level storage. The distributed storage system can be viewed in a
     few layers, including a format for the storage devices (the Extent and
     B-tree-based Object File System [EBOFS] or an alternative) and an
     overriding management layer designed to manage data replication, failure
     detection, and recovery and subsequent data migration called <em>Reliable
    Autonomic Distributed Object Storage</em> (RADOS). Finally, monitors
     are used to identify component failures, including subsequent
     notification. </p>

 
     <br><a name="fig2"><b>Figure 2. Simplified layered view of
    the Ceph ecosystem </b></a><br>
     <img alt="Block diagram showing a simplified layered view of the Ceph ecosystem, 
      including the server, metadata servers, and object storage ddaemon" 
      src="http://www.ibm.com/developerworks/linux/library/l-ceph/figure2.gif" 
      height="270" width="465">
 <br>

<h4><a name="components"><span class="atitle">Ceph components</span></a></h4>
 <p>With the conceptual architecture of Ceph under your belts, you can dig down
     another level to see the major components implemented within the Ceph
     ecosystem. One of the key differences between Ceph and traditional file
     systems is that rather than focusing the intelligence in the file system
     itself, the intelligence is distributed around the ecosystem.</p>
 
 <p>Figure 3
     shows a simple Ceph ecosystem. The
     Ceph Client is the user of the Ceph file system. The Ceph Metadata Daemon
     provides the metadata services, while the Ceph Object Storage Daemon
     provides the actual storage (for both data and metadata). Finally, the
     Ceph Monitor provides cluster management. Note that there can be many Ceph
     clients, many object storage endpoints, numerous metadata servers
     (depending on the capacity of the file system), and at least a redundant
     pair of monitors. So, how is this file system distributed? </p>

 
     <br><a name="fig3"><b>Figure 3. Simple Ceph
    ecosystem</b></a><br>
     <img alt="Block diagram of a simple Ceph ecosystem" 
      src="http://www.ibm.com/developerworks/linux/library/l-ceph/figure3.gif" 
      height="402" width="561">
 <br>

<h4><a name="client"><span class="smalltitle">Ceph client</span></a></h4>
 
    <a name="kernel"></a>

<h5>Kernel or user space</h5>
     
     <p>Early versions of Ceph utilized Filesystems in User SpacE (FUSE), which
    pushes the file system into user space and can greatly simplify its
    development. But today, Ceph has been integrated into the mainline
    kernel, making it faster, because user space context switches are no
    longer necessary for file system I/O. </p>
    </div></div>
 
 <p>As Linux presents a common interface to the file systems (through the
     virtual file system switch [VFS]), the user's perspective of Ceph is
     transparent. The administrator's perspective will certainly differ, given
     the potential for many servers encompassing the storage system (see the 
     <a href="#resources">Resources</a> section for information on creating a
     Ceph cluster). From the users' point of view, they have access to a large
     storage system and are not aware of the underlying metadata servers,
     monitors, and individual object storage devices that aggregate into a
     massive storage pool. Users simply see a mount point, from which
     standard file I/O can be performed. </p>
 <p>The Ceph file system - or at least the client interface - is
     implemented in the Linux kernel. Note that in the vast majority of file
     systems, all of the control and intelligence is implemented within the
     kernel's file system source itself. But with Ceph, the file system's
     intelligence is distributed across the nodes, which simplifies the client
     interface but also provides Ceph with the ability to massively scale (even
     dynamically). </p>
 <p>Rather than rely on allocation lists (metadata to map blocks on a disk to a
     given file), Ceph uses an interesting alternative. A file from the Linux
     perspective is assigned an inode number (INO) from the metadata server,
     which is a unique identifier for the file. The file is then carved into
     some number of objects (based on the size of the file). Using the INO and
     the object number (ONO), each object is assigned an object ID (OID). Using
     a simple hash over the OID, each object is assigned to a placement group.
     The <em>placement group</em> (identified as a PGID) is a conceptual
     container for objects. Finally, the mapping of the placement group to
     object storage devices is a pseudo-random mapping using an algorithm called
    <em>Controlled Replication Under Scalable Hashing</em> (CRUSH). In
     this way, mapping of placement groups (and replicas) to storage devices
     does not rely on any metadata but instead on a pseudo-random mapping function.
     This behavior is ideal, because it minimizes the overhead of storage and
     simplifies the distribution and lookup of data. </p>
 <p>The final component for allocation is the cluster map. The <em>cluster
    map</em> is an efficient representation of the devices representing
     the storage cluster. With a PGID and the cluster map, you can locate any
     object. </p>

         

 <h5><a name="metadata_server"><span class="smalltitle">The Ceph metadata
     server</span></a></h5>
 <p>The job of the metadata server (cmds) is to manage the file system's
     namespace. Although both metadata and data are stored in the object
     storage cluster, they are managed separately to support scalability. In
     fact, metadata is further split among a cluster of metadata servers that
     can adaptively replicate and distribute the namespace to avoid hot spots.
     As shown in Figure 4, the metadata servers manage portions of the
     namespace and can overlap (for redundancy and also for performance). The
     mapping of metadata servers to namespace is performed in Ceph using
     dynamic subtree partitioning, which allows Ceph to adapt to changing
     workloads (migrating namespaces between metadata servers) while preserving
     locality for performance. </p>

 
     <br><a name="fig4"><b>Figure 4. Partitioning of the Ceph
    namespace for metadata servers</b></a><br>
     <img alt="Diagram showing the partitions of the Ceph namespace for metadata 
      servers" 
      src="http://www.ibm.com/developerworks/linux/library/l-ceph/figure4.gif" 
      height="249" width="458">
 <br>

 <p>But because each metadata server simply manages the namespace for the
     population of clients, its primary application is an intelligent metadata
     cache (because actual metadata is eventually stored within the object
     storage cluster). Metadata to write is cached in a short-term journal,
     which eventually is pushed to physical storage. This behavior allows the
     metadata server to serve recent metadata back to clients (which is common
     in metadata operations). The journal is also useful for failure recovery: 
     if the metadata server fails, its journal can be replayed to ensure
     that metadata is safely stored on disk. </p>
 <p>Metadata servers manage the inode space, converting file names to metadata.
     The metadata server transforms the file name into an inode, file size, and
     striping data (layout) that the Ceph client uses for file I/O. </p>

 <h5><a name="monitors"><span class="smalltitle">Ceph monitors</span></a></h5>
 <p>Ceph includes monitors that implement management of the cluster map, but  
some elements of fault management are implemented in the
     object store itself. When object storage devices fail or new devices are
     added, monitors detect and maintain a valid cluster map. This function is
     performed in a distributed fashion where map updates are communicated with
     existing traffic. Ceph uses Paxos, which is a family of algorithms for
     distributed consensus. </p>

 <h5><a name="object_storage">Ceph object storage</a></h5>
 <p>Similar to traditional object storage, Ceph storage nodes include not only
     storage but also intelligence. Traditional drives are simple targets that
     only respond to commands from initiators. But object storage devices are
     intelligent devices that act as both targets and initiators to support
     communication and collaboration with other object storage devices. </p>
 <p>From a storage perspective, Ceph object storage devices perform the mapping
     of objects to blocks (a task traditionally done at the file system layer
     in the client). This behavior allows the local entity to best decide how
     to store an object. Early versions of Ceph implemented a custom low-level
     file system on the local storage called <em>EBOFS</em>. This system
     implemented a nonstandard interface to the underlying storage tuned for
     object semantics and other features (such as asynchronous notification of
     commits to disk). Today, the B-tree file system (BTRFS) can be used at the
     storage nodes, which already implements some of the necessary features
     (such as embedded integrity). </p>
 <p>Because the Ceph clients implement CRUSH and do not have knowledge of the
     block mapping of files on the disks, the underlying storage devices can
     safely manage the mapping of objects to blocks. This allows the storage
     nodes to replicate data (when a device is found to have failed).
     Distributing the failure recovery also allows the storage system to scale,
     because failure detection and recovery are distributed across the
     ecosystem. Ceph calls this RADOS (see <a href="#fig3">Figure 3</a>). </p>

<h5><a name="features">Other features of interest</a></h5>
 <p>As if the dynamic and adaptive nature of the file system weren't enough,
     Ceph also implements some interesting features visible to the user. Users
     can create snapshots, for example, in Ceph on any subdirectory (including
     all of the contents). It's also possible to perform file and capacity
     accounting at the subdirectory level, which reports the storage size and
     number of files for a given subdirectory (and all of its nested contents). </p>

<h5><a name="status"><span class="atitle">Ceph status and future</span></a></h5>
 <p>Although Ceph is now integrated into the mainline Linux kernel, it's
     properly noted there as experimental. File systems in this state are
     useful to evaluate but are not yet ready for production environments. But
     given Ceph's adoption into the Linux kernel and the motivation by its
     originators to continue its development, it should be available soon to
     solve your massive storage needs. </p>

<h5><a name="other"><span class="atitle">Other distributed file systems</span></a></h5>
 <p>Ceph isn't unique in the distributed file system space, but it is unique in
     the way that it manages a large storage ecosystem. Other examples of
     distributed file systems include the Google File System (GFS), the General
     Parallel File System (GPFS), and Lustre, to name just a few. The ideas
     behind Ceph appear to offer an interesting future for distributed file
     systems, as massive scales introduce unique challenges to the massive
     storage problem. </p>

<h5><a name="N1017D"><span class="atitle">Going further</span></a></h5>
 <p>Ceph is not only a file system
     but an object storage ecosystem with enterprise-class features. In the 
     <a href="#resources">Resources</a> section, you'll find information on
     how to set up a simple Ceph cluster (including metadata servers, object
     servers, and monitors). Ceph fills a gap in distributed storage, and it
     will be interesting to see how the open source offering evolves in the
     future. </p>
        
<br>
<p><a name="resources"><span class="atitle">Resources</span></a></p>
<p><b>Learn</b></p>

<ul>
  <li>The Ceph creators' paper 
    "<a href="http://www.ssrc.ucsc.edu/Papers/weil-osdi06.pdf">Ceph: A Scalable, 
     High-Performance Distributed File System</a>" (PDF) and Sage Weil's PhD 
     dissertation, "<a href="http://ceph.com/papers/weil-thesis.pdf">Ceph: 
     Reliable, Scalable, and High-Performance Distributed Storage</a>" (PDF), 
     reveal the original ideas behind Ceph.<br><br></li>
  <li>The <a href="http://ssrc.cse.ucsc.edu/proj/petascale.html">Storage Systems
    Research Center's Petabyte-Scale Storage site</a> offers 
     additional technical information about Ceph. 
 <br><br></li>
  <li>Visit the <a href="http://ceph.com/">Ceph home page</a> for the latest 
    information.<br><br></li>
  <li>"<a href="http://www.ssrc.ucsc.edu/Papers/weil-sc06.pdf">CRUSH: Controlled, 
   Scalable, Decentralized Placement of Replicated Data</a>" (PDF)  and 
"<a href="http://www.pdsi-scidac.org/SC07/resources/weil-20071111-rados-pdsw.pdf">RADOS: 
    A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters</a>" 
    (PDF) discuss two of the most interesting aspects of the Ceph file system.
    <br><br></li>
  <li>"<a href="http://lwn.net/Articles/258516/">The Ceph filesystem</a>" on LWN.net  
     provides an early take on the Ceph file system (including a set of entertaining 
     comments).<br><br></li>
  <li>
    "<a href="http://www.ece.umd.edu/%7Eposulliv/ceph/cluster_build.html">Building a 
    Small Ceph Cluster</a> gives instructions for building a Ceph cluster along with 
    tips for distribution of assets.  This article walks you through getting the Ceph 
    source, building a new kernel, and then deploying the various elements of the 
    Ceph ecosystem. <br><br></li>
  <li> At the <a href="http://en.wikipedia.org/wiki/Paxos_algorithm">Paxos Wikipedia 
    page</a>, learn more about how Ceph metadata servers utilize Paxos as a consensus 
    protocol among the distributed entities.<br><br></li>
  <li>In "<a href="http://www.ibm.com/developerworks/linux/library/l-virtual-filesystem-switch/index.html">Anatomy of the Linux virtual file system switch</a>" 
    (developerWorks, August 2009), learn more about the VFS, a flexible mechanism that 
    Linux includes to allow multiple file systems to exist concurrently.<br><br></li>
  <li>In "<a href="https://www.ibm.com/developerworks/linux/library/l-nilfs-exofs/">Next-generation 
    Linux file systems: NiLFS(2) and exofs</a>" (developerWorks, October 2009), learn 
    more about exofs, another Linux file system that utilizes object storage. exofs 
    maps object storage device-based storage into a traditional Linux file 
    system.<br><br></li>
  <li>At the <a href="http://btrfs.wiki.kernel.org/index.php/Main_Page">kernel wiki
    site for BTRFS</a> and in "<a href="http://www.ibm.com/developerworks/linux/library/l-kernel-advances/">Linux Kernel Advances</a>" 
    (developerWorks, March 2009), you can learn how to use the BTRFS on individual 
    object storage nodes. <br><br></li>
  <li>In the 
    <a href="http://www.ibm.com/developerworks/linux/index.html">developerWorks Linux 
    zone</a>, find hundreds of 
    <a href="http://www.ibm.com/developerworks/views/linux/libraryview.jsp">how-to
    articles and tutorials</a>, as well as downloads, discussion forums,  and a wealth 
    other resources for Linux developers and administrators.               
    <br><br></li>
  <li> Stay current with 
<a href="http://www.ibm.com/developerworks/offers/techbriefings/events.html">developerWorks 
technical events and webcasts</a> focused on a variety of IBM products and IT industry 
topics. 
    <br><br></li>
  <li> Attend a 
    <a href="http://www.ibm.com/developerworks/offers/techbriefings/index.html">free 
    developerWorks Live! briefing</a> to get up-to-speed quickly on IBM products and 
    tools as well as IT industry trends. 
    <br><br></li>
  <li> Watch 
<a href="http://www.ibm.com/developerworks/offers/lp/demos/index.html">developerWorks 
    on-demand demos</a> ranging from product installation and setup demos for beginners, 
    to advanced functionality for experienced developers.
    <br><br></li>
  <li>Follow 
    <a href="http://www.twitter.com/developerworks/">developerWorks on 
    Twitter</a>, or subscribe to a feed of 
  <a href="http://search.twitter.com/search?q=%23linux+from%3Adeveloperworks+-RT+">Linux 
    tweets on developerWorks</a>.<br><br></li></ul><p><b>Get products and 
    technologies</b></p><ul>
  <li><a href="http://www.ibm.com/developerworks/downloads/index.html">Evaluate IBM 
    products</a> in the way that suits you best: Download a product trial, try a 
    product online, use a product in a cloud environment, or spend a few 
    hours in the  
   <a href="http://www.ibm.com/developerworks/downloads/soasandbox/index.html">SOA 
     Sandbox</a> learning how to implement Service Oriented Architecture efficiently.
    <br><br></li>
</ul><p><b>Discuss</b></p>
<ul>
  <li> Get involved in the <a href="http://www.ibm.com/developerworks/community">My 
   developerWorks community</a>.  Connect with other developerWorks users while 
   exploring the developer-driven blogs, forums, groups, and wikis. 
    <br><br></li>
</ul>

<p><a name="author"><span class="atitle">About the author</span></a></p>
<div><img src="http://www.ibm.com/developerworks/i/p-mjones.jpg" 
class="dw-author-img" alt="M. Tim Jones" height="80" width="64">
<p><a name="author1"></a>M. Tim Jones is an embedded firmware architect and the 
author of <em>Artificial Intelligence: A Systems Approach, GNU/Linux Application 
Programming</em> (now in its second edition), <em>AI Application Programming</em> 
(in its second edition), and <em>BSD Sockets Programming from a Multilanguage 
Perspective</em>.  His engineering background ranges from the development of kernels 
for geosynchronous spacecraft to embedded systems architecture and networking 
protocols development. Tim is a Consultant Engineer for Emulex Corp. in Longmont, 
Colorado.</p></div></div>

<a name="RadosAndCephFS"></a><h3>The RADOS Object Store and Ceph Filesystem 
<a href="http://hpc.admin-magazine.com/Articles/The-RADOS-Object-Store-and-Ceph-Filesystem">(Source Origin)</a></h3>
<div class="attribute-author">By Martin Loschwitz</div><br>
	
<div class="attribute-body">
<TABLE>
 <TR><TD>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     <TD>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     <TD><img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/hpc/articles/the-rados-object-store-and-ceph-filesystem/49009-1-eng-US/The-RADOS-Object-Store-and-Ceph-Filesystem_medium.jpg" 
alt="The RADOS Object Store and Ceph Filesystem"  
height="137" width="115">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            
<TD><b>Scalable storage is a key component in cloud environments. RADOS and Ceph 
enter the field, promising to support seamlessly scalable storage.</b>
 <TR><TD>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     <TD>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 <TR><TD>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     <TD>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</TABLE></div>

<p>Cloud computing is, without a doubt, the IT topic of our time. No 
issue pushes infrastructure managers with large-scale enterprises as 
hard as how to best implement the cloud. The IaaS principle is quite 
simple: The goal is to provide capacity to users in the form of 
computational power and storage in a way that means as little work as 
possible for the user and that keeps the entire platform as flexible as 
possible.</p><p>Put more tangibly, this means customers can request CPU 
cycles and disk space as they see fit and continue to use both as the 
corresponding services are needed. For IT service providers, this means 
defining your own setup to be as scalable as possible: It should be 
possible to accommodate peak loads without difficulty, and if the 
platform grows - which will be the objective of practically any 
enterprise - a permanent extension should also be accomplished easily.</p><p>In
 practical terms, implementing this kind of solution will tend to be 
more complex. Scalable virtualization environments are something that is
 easy to achieve: Xen and KVM, in combination with the current crop of 
management tools, make it easy to manage virtualization hosts. Scale out
 also is no longer an issue: If the platform needs more computational 
performance, you can add more machines that integrate seamlessly with 
the existing infrastructure.</p><p>Things start to become more 
interesting when you look at storage. The way IT environments store data
 has remained virtually unchanged in the past few years. In the early 
1990s, data centers comprised many servers with local storage, all of 
which suffered from legacy single points of failure. As of the 
mid-1990s, Fibre Channel HBAs and matching SAN storage entered the 
scene, offering far more redundancy than their predecessors, but at a 
far higher price. People who preferred a lower budget approach turned to
<a href=" http://en.wikipedia.org/wiki/DRBD">DRBD</a> with standard hardware 
a few years ago, thus avoiding what can be hugely expensive SANs. However, all 
of these approaches share a problem: They do not scale out seamlessly.</p>

<h4>Scale Out and Scale Up</h4>

<p>Admins
 and IT managers distinguish between two basic types of scalability. 
Vertical scalability (scale up) is based on the idea of extending the 
resources of existing devices, whereas horizontal scalability (scale 
out) relies on adding more resources to the existing set (Figure 1). 
Databases are a classic example of a scale-out solution: Typically, 
slave servers are added to support load distribution.</p>
<div class="" id=""><div class="content-view-embed">
<div class="class-image">

<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/rados-f01/49039-1-eng-US/rados-F01_reference.jpg" 
alt="rados-F01" title="rados-F01" height="323" width="600">
</div>

<div class="attribute-caption" style="width: 600px">
<p>Figure 1: Two kinds of scalability.</p></div>
         </div>
</div>
</div><p>Scale out is completely new ground when it comes to storage. 
Local storage in servers, SAN storage, or servers with DRBD will 
typically only scale vertically (more disks!), not horizontally. When 
the case is full, you need a second storage device, and this will not 
typically support integration with the existing storage to provide a 
single unit, thus making maintenance far more difficult. In terms of SAN
 storage, two SANs just cost twice as much as one.</p>

<h4><a href="http://en.wikipedia.org/wiki/Object_storage_device">Object 
Stores</a></h4>

<p>If you are planning a cloud and thinking about seamlessly scalable 
storage, don't become despondent at this point: Authors of the popular 
cloud applications are fully aware of this problem and now offer 
workable solutions known as object stores.</p><p>Object stores follow a 
simple principle: All servers that become part of an object store run 
software that manages and exports the server's local disk space. All 
instances of this software collaborate on the cluster, thus providing 
the illusion of a single, large data store. To support internal storage 
management, the object store software does not save data in its original
 format on the individual storage nodes, but as binary objects. Most 
exciting is that the number of individual nodes joining forces to create
 the large object store is arbitrary. You can even add storage nodes on 
the fly.</p><p>Because the object storage software also has internal 
mechanisms to handle redundancy, and the whole thing works with standard
 hardware, a solution of this kind combines the benefits of SANs or DRBD
 storage and seamless horizontal scalability. RADOS has set out to be 
the king of the hill in this sector, in combination with the matching 
Ceph filesystem.</p>

<h4>How RADOS Works</h4>

<p>RADOS (reliable autonomic distributed object store, although many people 
mistakenly say "autonomous") has been under development at DreamHost, led by 
Sage A. Weil, for a number of years and is basically the result of a doctoral 
thesis at the University of California, Santa Cruz. RADOS implements 
precisely the functionality of an object store as described earlier, 
distinguishing between three different layers to do so:</p>

<ol>
  <li>Object Storage Devices (OSDs). An OSD in RADOS is always a folder 
within an existing filesystem. All OSDs together form the object store 
proper, and the binary objects that RADOS generates from the files to be
 stored reside in the store. The hierarchy within the OSDs is flat: 
files with UUID-style names but no subfolders.</li>
  <li>Monitoring servers (MONs): MONs form the interface to the RADOS 
store and support access to the objects within the store. They handle 
communication with all external applications and work in a decentralized
 way: There are no restrictions in terms of numbers, and any client can 
talk to any MON. MONs manage the MONmap (a list of all MONs) and the 
OSDmap (a list of all OSDs). The information from these two lists lets 
clients compute which OSD they need to contact to access a specific 
file. In the style of a 
<a href="http://en.wikipedia.org/wiki/Paxos_%28computer_science%29">Paxos</a> 
cluster, MONs also ensure RADOS's functionality in terms of respecting 
<a href="http://en.wikipedia.org/wiki/Quorum_%28distributed_computing%29">quorum 
rules<a>.</li>
  <li>Metadata servers (MDS): MDSs provide POSIX metadata for objects in the 
RADOS object store for Ceph clients.</li>
</ol>

<h4>What About Ceph?</h4>

<p>Most articles about RADOS just refer to Ceph in the title, causing some 
confusion. Weil described the relationship between RADOS and Ceph as two parts 
of the same solution: RADOS is the "lower" part and Ceph the "upper" part. One 
thing is for sure: The best looking object store in the world is useless if it 
doesn't give you any options for accessing the data you store in it.</p>

<p>However, it is precisely these options that Ceph offers for RADOS: It is 
a filesystem that accesses the object store in the background and thus 
makes its data directly usable in the application. The metadata servers 
help accomplish this task by providing the metadata required for each 
file that Ceph accesses in line with the POSIX standard when a user 
requests a file via Ceph.</p><p>Because DreamHost didn't consider until 
some later stage of development that RADOS could be used as a back end 
for tools other than filesystems, they generated confusion regarding the
 names of RADOS and Ceph. For example, the official DreamHost guides 
refer simply to "Ceph" when they actually mean "RADOS and Ceph."</p>

<h4>First RADOS Setup</h4>

<p>Theory is one thing, but to gain some understanding of RADOS and Ceph, 
it makes much more sense to experiment on a "live object." You don't need 
much for a complete RADOS-Ceph setup: Three servers with local storage 
will do fine. Why three? Remember that RADOS autonomically provides a 
high-availability option. The MONs use the PAXOS implementation referred
 to earlier to guarantee that there will always be more than one copy of
 an object in a RADOS cluster. Although you could turn a single node 
into a RADOS store, this wouldn't give you much in the line of high 
availability. A RADOS cluster comprising two nodes is even more 
critical: In a normal case, the cluster would have a quorum, but the 
failure of a single node would then make the other node useless because 
RADOS needs a quorum, and by definition, one can't make a quorum. In 
other words, you need three nodes to be on the safe side, so the failure
 of single node won't be an issue.</p><p>Incidentally, nothing can stop 
you from using virtual machines with RADOS for your experiments - RADOS 
doesn't have any functions that require specific hardware features.</p>

<h4>Finding the Software</h4>

<p>Before experimenting, you need to install RADOS and Ceph. Ceph, which is 
a plain vanilla filesystem driver on Linux systems (e.g., ext3 or ext4), 
made its way into the Linux kernel in Linux 2.6.34 and is thus available
 for any distribution with this or a later kernel version (Figure 2). 
The situation isn't quite as easy with RADOS; however, the 
<a href="http://ceph.com/docs/master/install/" 
target="_blank">documentation</a> points to prebuilt packages, or at least gives 
you an installation guide, for all of the popular distributions. Note that although 
the documentation refers to "ceph," the packages contain all of the 
components you need for RADOS. After installing the packages, it's time 
to prepare RADOS.</p>

<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/rados-f02/49035-1-eng-US/rados-F02_reference.jpg" title="rados-F02" height="361" width="600">
</div>

<div class="attribute-caption" style="width: 600px">
<p>Figure 2: After loading the ceph kernel module, the filesystem is available 
on Linux. Ceph was first introduced in kernel 2.6.34.</p>
 </div>

<h4>Preparing the OSDs</h4>

<p>RADOS needs OSDs. As I mentioned earlier, any folder on a filesystem can act as 
an OSD; however, the filesystem must support extended attributes. The RADOS authors 
recommend Btrfs but also mention XFS as an alternative for anyone who is still a bit 
wary of using Btrfs. For simplicity's sake, I will assume in the following examples 
that you have a directory named <i>osd.ID</i> in <i>/srv</i> on three servers, where 
<i>ID </i> stands for the server's hostname in each case. If your three servers are 
named <i>alice</i>, <i>bob</i>, and <i>charlie</i>, you would have a folder named 
<i>/srv/osd.alice</i> on server <i>alice</i>, and so on.</p><p>If you will be using 
a local filesystem set up specially for this purpose, be sure to mount it in 
<i>/srv/osd.ID</i>. Finally, each of the hosts in <i>/srv</i> also needs a 
<i>mon.ID</i> folder, where <i>ID</i> again stands for the hostname.</p><p>In this 
sample setup, the central RADOS configuration in <i>/etc/ceph/ceph.conf</i> might 
look like Listing 1.</p>

<h4>Listing 1: Sample <i>/etc/ceph/ceph.conf</i></h4>

<pre>
[global]
   auth supported = cephx
   keyring = /etc/ceph/$name.keyring

[mon]
   mon data = /srv/mon.$id

[mds]

[osd]
   osd data = /srv/osd.$id
   osd journal = /srv/osd.$id.journal
   osd journal size = 1000

[mon.a]
   host = alice
   mon addr = 10.42.0.101:6789

[mon.b]
   host = bob
   mon addr = 10.42.0.102:6789

[mon.c]
   host = charlie
   mon addr = 10.42.0.103:6789

[osd.0]
   host = alice

[osd.1]
   host = bob

[osd.2]
   host = charlie
[mds.a]
   host = alice
</pre>

<p>The configuration file defines the following details: each of the three hosts 
provides an OSD and a MON server; host <i>alice</i> is also running an MDS to ensure 
that any Ceph clients will find POSIX-compatible metadata on access. Authentication 
between the nodes is encrypted: The keyring for this is stored in the <i>/etc/ceph</i> 
folder and goes by the name of <i>$name.keyring</i>, where RADOS will automatically 
replace <i>name</i> with the actual value later.</p><p>Most importantly, the nodes 
in the RADOS cluster must reach one another directly using the hostnames from your 
<i>ceph.conf</i> file. This could mean adding these names to your <i>/etc/hosts</i> 
file. Additionally, you need to be able to log in to all of the RADOS nodes as 
<i>root</i> later on for the call to <i>mkcephfs</i>, and root needs to be able 
to call <i>sudo</i> without an additional password prompt on all of the nodes. After 
fulfilling these conditions, the next step is to create the keyring to support mutual 
authentication between the RADOS nodes:</p>

<pre>
 mkcephfs -a -c /etc/ceph/ceph.conf -k /etc/ceph/admin.keyring
</pre>

<p>Now you need to ensure that <i>ceph.conf</i> exists on all the hosts belonging 
to the cluster (Figure 3). If this is the case, you just need to start RADOS on 
all of the nodes: Typing</p>

<pre>
 /etc/init.d/ceph start
</pre>

<p>will do the trick. After a couple of seconds, the three nodes should have 
joined the cluster; you can check this by typing</p>

<pre>
 ceph -k /etc/ceph/admin.keyring -c /etc/ceph/ceph.conf health
</pre>

<p>which should give you the output shown in Figure 4.</p>

<div>
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/rados-f03/49031-1-eng-US/rados-F03_reference.jpg" alt="rados-F03" height="146" width="600">
</div>

<div class="attribute-caption" style="width: 600px">
<p>Figure 3: To discover which Ceph services are running on a host, type ps. In this 
example, the host is an OSD, MON, and MDS.</p>
</div>
         
<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/rados-f04/49027-1-eng-US/rados-F04_reference.jpg" alt="rados-F04" height="349" width="600">
</div>

<div class="attribute-caption" style="width: 600px">
<p>Figure 4: Ceph has its own health options that tell you whether the RADOS Paxos 
cluster is working properly.</p>
 </div>

<h4>Using Ceph to Mount the Filesystem</h4>

<p>To mount the newly created filesystem on another host on one of the RADOS nodes, 
you can use the normal <i>mount</i> command - the target host is one of the MON 
servers (i.e., <i>alice</i> in this example) with a MON address set to 
<i>10.42.0.101:6789</i> in <i>ceph.conf</i>.  Because Cephx authentication is being 
used, I need to identify the login credentials automatically generated by Ceph before 
I can mount the filesystem. The following command on one of the RADOS nodes outputs 
the credentials:</p>

<pre>
ceph-authtool -l /etc/ceph/admin.keyring
</pre>

<p>The mount process then follows:</p>

<pre>
 mount -t ceph 10.0.0.1:6789:/ /mnt/osd -vv -o name=admin,secret=mykey 
</pre>

<p>where <i>mykey</i> needs to be replaced by the value for <i>key</i> that you 
determined with the last command. The mountpoint in this example is <i>/mnt/osd</i>, 
which can be used it as a normal directory from now on.</p>

<h4>The Crush Map</h4>

<p>RADOS and Ceph use a fair amount of magic in the background to safeguard 
the setup against any kind of failure, starting with the mount. Any of the 
existing MON servers can act as a mountpoint; however, this doesn't mean
 communications are only channeled between the client and this one MON. 
Instead, Ceph on the client receives the MON and OSDmaps from the MON 
server it contacts and then references them to compute which OSD is best
 to use for a specific file before going on to handle the communication 
with this OSD.</p><p>The <i>Crush map</i> is another step for improving 
redundancy. It defines which hosts belong to a RADOS cluster, how many 
OSDs exist in the cluster, and how to distribute the files over these 
hosts for best effect. The Crush map makes RADOS rack-aware, allowing 
admins to manipulate the internal replication of RADOS in terms of 
individual servers, racks, or security zones in the data center. The 
setup shown for the example here also has a rudimentary default Crush 
map. If you want to experiment with your own Crush map, the 
<a href="http://ceph.com/w/index.php?title=Custom_data_placement_with_CRUSH" 
target="_blank">Ceph wiki</a> gives you the most important information for 
getting started.</p>

<h4>Extending the Existing Setup</h4>

<p>How do you go about extending an existing RADOS cluster, by adding more 
nodes to increase the amount of available storage? If you want to add a 
node named <i>daisy</i> to the existing setup, the first step would be 
to define an ID for this node. In this example, IDs 0 through 3 are 
already assigned, and the new node would have an ID of 4, so you need to
type:</p>

<pre>
 ceph osd create 4
</pre>

<p>Then you need to extend the 
<i>ceph.conf</i> files on the existing cluster nodes, adding an entry for 
daisy. On daisy, you also need to create the folder structure needed for daisy 
to act as an OSD (i.e., the directories in <i>/srv</i>, as in the previous examples). 
Next, copy the new <i>ceph.conf</i> to the <i>/etc/ceph</i> folder on daisy.</p>

<p>Daisy also needs to know the current MON structure - after all, she will need
to register with an existing MON later on. This means daisy needs the current 
MONmap for the RADOS cluster. You can read the MONmap on one of the existing RADOS 
nodes by typing</p>

<pre>
 ceph mon getmap -o /tmp/monmap
</pre>

<p>(Figure 5), and then 
use <i>scp</i> to copy it to daisy (this example assumes you are storing the MONmap 
in <i>/tmp/monmap</i> on daisy). Now, you need to initialize the OSD directory on 
daisy:</p>

<pre>
 ceph-osd -c /etc/ceph/ceph.conf -i 4 --mkfs --monmap /tmp/monmap --mkkey
</pre>

<p>If the additional cluster node uses an ID other than <i>4</i>, you need to modify 
the numeric value that follows <i>-i</i>.</p>

<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/rados-f05/49023-1-eng-US/rados-F05_reference.jpg" alt="rados-F05" height="347" width="600">
</div>

<div class="attribute-caption" style="width: 600px">
<p>Figure 5: The MONmap contains information about MONs in the RADOS cluster. New OSDs 
rely on this information.</p>
 </div>

<p>Finally, you need to introduce the existing cluster to daisy. In this example, 
the last command created a <i>/etc/ceph/osd.4.keyring</i> file on daisy, which you 
can copy to one of the existing MONs with <i>scp</i>. Following this,</p>

<pre>
 ceph auth add osd.ID osd 'allow *' mon 'allow rwx' -i /etc/ceph/osd.4.keyring 
</pre>

<p>on the same node adds the new OSD to the existing authentication structure 
(Figure 6). Typing <i>/etc/init.d/ceph</i> on daisy launches RADOS, and the new OSD 
registers with the existing RADOS cluster. The final step is to modify the existing 
Crush map so the new OSD is used. In this example, you would type</p>

<pre>
 ceph osd crush add 4 osd.4 1.0 pool=default host=daisy
</pre>

<p>to do this. The new OSD is now 
part of the existing RADOS/Ceph cluster.</p>

<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/rados-f06/49019-1-eng-US/rados-F06_reference.jpg" alt="rados-F06" height="349" width="600">
</div>

<div class="attribute-caption" style="width: 600px">
 <p>Figure 6: Typing "ceph auth list" tells Ceph to 
reveal the keys that a MON instance already knows and what the 
credentials allow the node to do.</p>
 </div>

<h4>Conclusions</h4>

<p>It isn't difficult to set up the combination of RADOS and Ceph. But this simple 
configuration doesn't leverage many of the exciting features that RADOS offers. For 
example, the Crush map functionality gives you the option of deploying huge RADOS
setups over multiple racks in the data center while offering intelligent failsafes. 
Because RADOS also offers you the option of dividing its storage into individual 
pools of variable sizes, you can achieve more granularity in terms of different 
tasks and target groups.</p>

<p>Also, I haven't looked at the RADOS front ends beyond Ceph. After all, Ceph 
is just one front end of many; in this case, it supports access to files
in the object store via the Linux filesystem. However, more options for
 accessing the data on RADOS exist. The RADOS block device, or <i>rbd</i>
 for short, is a good choice when you need to support access to files in
 the object store at the block device level. For example, this would be 
the case for virtual machines that will typically accept block devices 
as a back end for virtual disks, thus avoiding slower solutions with 
disk images. In this way, you can exploit RADOS's potential as an 
all-encompassing storage system for large virtualization solutions while
 solving another problem in the context of the cloud.</p><p>Speaking of the cloud, 
besides <i>rbd</i>, <i>librados</i> provides various interfaces for HTTP access - 
for example, a variant compatible with Amazon's S3 and a Swift-compatible variant. 
A generic REST interface is also available. As you can see, RADOS has a good 
selection of interfaces to the world outside.</p><p>At the time of 
writing, the RADOS Ceph components were still pre-series, but the 
developers were looking to release version 1.0, which could already be 
available as officially stable and "enterprise-ready."</p>
	</div>
	
<a name="RadosAndCephFS2"></a><h3>The RADOS object store and Ceph filesystem: Part 2
<a href="http://hpc.admin-magazine.com/Articles/RADOS-and-Ceph-Part-2">(Source 
Origin)</a></h3>
	
		<div class="attribute-author">
By Martin Loschwitz	</div>
	
<div class="attribute-body">
<p>Two issues ago, <u><a href="http://hpc.admin-magazine.com/Articles/The-RADOS-Object-Store-and-Ceph-Filesystem/%28language%29/eng-US" 
target="_blank">ADMIN magazine introduced RADOS and Ceph</a></u><b>[1]</b>
 and explained what these tools are all about. In this second article, I
 will take a closer look and explain the basic concepts that play a role
 in their development. How does the cluster take care of internal 
redundancy of the stored objects, for example, and what possibilities 
exist besides Ceph for accessing the data in the object store?</p>

<p>The first part of this workshop demonstrated how an additional 
node can be added to an existing cluster using</p>

<pre>
 ceph osd crush add 4 osd.4 1.0 pool=default host=daisy
</pre>

<p>assuming that <i>daisy</i> 
is the hostname of the new server. This command integrates the host <i>daisy</i>
 in the cluster and gives it the same weight (1,0) as all the other 
nodes. Removing a node from the cluster configuration is as easy as 
adding one. The command for that is:</p>

<pre>
 ceph osd crush remove osd.4
</pre>

<p>The <i>pool=default</i> parameter for adding the node already 
refers to an important feature: pools.</p> 
<p><b>Working with 
Pools</b></p>

<p>RADOS offers the option of dividing the storage of the entire 
object store into individual fragments called pools. One pool, however, does not 
correspond to a contiguous storage area, as with a partition; rather, it
 is a logical layer consisting of binary data tagged as belonging to the
 corresponding pool. Pools allow configurations in which individual 
users can only access specific pools, for example. The pools <i>metadata</i>, 
<i>data</i>, and <i>rbd</i> are available in the default configuration. A list of 
existing pools can be called up with <i>rados lspools</i> (Figure 1).</p>

<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/radceph-f01/61844-1-eng-US/radceph-F01_medium.jpg" style="border: 0px solid ;" alt="radceph-F01" title="radceph-F01" height="80" width="200">
</div>

<div class="attribute-caption" style="width: 200px">
                <p>Figure 1: Listing all current pools in RADOS.</p>
 </div>

<p>If you want to add another pool to the configuration, you can 
use the</p>

<pre>
 rados mkpool &lt;Name&gt;
</pre>

<p>command, replacing <i>&lt;Name&gt;</i> with a unique name. If 
an existing pool is no longer needed,</p>

<pre>rados rmpool &lt;Name&gt;</pre>

<p>removes it.</p>

<p><b>Inside RADOS</b></p> 
<p>One
 of the explicit design goals for RADOS is to provide seamlessly 
scalable storage. Admins should be able to add any number of storage 
nodes to a RADOS object store at any time. Redundancy is important, and 
RADOS takes this into account by managing replication of the data 
automatically, without the admin of the RADOS cluster having to 
intervene manually. Combined with scalability, however, this process 
results in a problem for RADOS that other replication solutions don't 
have: How to distribute data optimally in a large cluster.</p>

<p>Conventional
 storage solutions generally "only" make sure data is copied from one 
server to another, so in the worst case, a failover can be executed. 
Usually, such solutions only run within one cluster with two, or at most
 three, nodes. The possibility of adding more nodes is ruled out from 
the beginning.</p> 
<p>With RADOS, theoretically, any 
number of nodes could be added to the cluster, and for each one, the 
object store must ensure that its contents are available redundantly 
within the whole cluster. Not the least of developers' problems is 
dealing with "rack awareness." If you have a 20-node cluster with RADOS 
in your data center, you will ideally have it distributed over different
 multiple compartments or buildings for additional security. For that 
setup to work properly, the storage solution must know where each node 
is and where and how which data can be accessed. The solution RADOS 
developers - above all RADOS guru Sage A. Weil - came up with consists 
of two parts: placement groups and the Crush map.</p>

<p><b>Placement Groups</b></p> 
<p>Three
 different maps exist within a RADOS cluster: the MONmap, which is a 
list of all monitoring servers; the OSDmap, in which all physical Object
 Storage Devices (OSDs) are found; and the Crush map. OSDs themselves 
contain the binary objects - that is, the data actually saved in the 
object store.</p>

<p>Here is where placement groups (PGs) come into play: From 
the outside, it seems as if the 
<u><a href="http://www.ssrc.ucsc.edu/Papers/weil-osdi06.pdf" 
target="_blank">allocation of storage objects to specific OSDs</a></u>
 occurs randomly. In reality, however, the allocation is done by means 
of the placement groups. Each object belongs to such a group. Simply 
speaking, a placement group is a list of different objects that are 
placed in the RADOS store. RADOS computes which placement group an 
object belongs to by using the name of the object, the desired 
replication level, and a bitmask that determines the sum of all PGs in 
the RADOS cluster.</p>
<p><b>The Crush Map</b></p>

<p>The
 Crush map, which is the second part of this system, contains 
information about where each placement group is found in the cluster - 
that is, on which OSD (Figure 2). Replication is always executed at the 
PG level: All objects of a placement group are replicated between 
different OSDs in the RADOS cluster.</p>

<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/radceph-f02/61848-1-eng-US/radceph-F02_reference.jpg" style="border: 0px solid ;" alt="radceph-F02" title="radceph-F02" height="456" width="600">
</div>

                                        
<div class="attribute-caption" style="width: 600px">
<p>Figure 2: The elements of the RADOS universe, whose interaction is controlled by 
the Crush map.</p>
 </div>

<p>The Crush map got its name from the algorithm it uses: 
<u><a href="http://www.ssrc.ucsc.edu/Papers/weil-sc06.pdf" target="_blank">Controlled 
Replication under Scalable Hashing</a></u>.  The algorithm was developed by Weil 
specifically for such tasks in RADOS. Weil highlights one feature of Crush in 
particular: In contrast to hash algorithms, Crush remains stable when many storage 
devices leave or join the Cluster simultaneously. The rebalancing that other storage 
solutions require creates a lot of traffic with correspondingly long 
waiting periods. Crush-based clusters, on the other hand, transfer just 
enough data between storage nodes to achieve a balance.</p>

<p><b>Manipulating Data Placement</b></p>
<p>Of
 course, the admin also has a word to say about what data lands where. 
Practically all parameters that pertain to replication in RADOS can be 
configured by the admin, including, for example, how often an object 
should exist within an RADOS cluster (i.e., how many replicas of it 
should be made). Of course, the admin is also free to manipulate the 
allocation of the replicas to the OSDs. Thus, RADOS can take into 
account where specific racks are. Rules for replication specified by the
 admin control distribution of the replicas from placement groups to 
different OSD groups. A group can, for example, include all servers in 
the same data center room, another group in another room, and so on.</p>

<p>Administrators
 can define how the replication of data in RADOS is done by manipulating
 the corresponding Crush rules. The basic principle is the neither the 
allocation of the placement groups nor the results of the Crush 
calculations can be influenced directly. Instead, replication rules are 
set for each pool; subsequently, distribution of the placement groups 
and their positioning by means of the Crush map are done by RADOS.</p>

<p>By
 default, RADOS includes a rule that two replicas of each object must 
exist per pool. The number of replicas is always set for each pool; a 
typical example would be to raise this number to three, as is done here:</p>

<pre>ceph osd pool set data size 3</pre>

<p>To make the same change for the <i>test</i> pool, <i>data</i> 
would be replaced by <i>test</i>. Whether the cluster subsequently actually does 
what the admin expects can be investigated with <i>ceph -v</i>.</p>

<p>The use <i>ceph</i>
 in this kind of operation is certainly comfortable, but it does not 
provide the full range of functions. In the example, the pool continues 
to use the default Crush map, which does not consider properties such as
 the rack where the server is. If you want to distribute replicas 
according to such properties, you have to create your own Crush map.</p>

<p><b>A Crush Map of Your Own</b></p>

<p>With your own Crush map, you as administrator can have the 
objects in your pools distributed any way you want. <i>crushtool</i> is a valuable 
utility for this purpose because it creates corresponding templates. The following 
example creates a Crush map for a setup consisting of six OSDs (i.e., individual 
storage devices in servers) 
distributed over three racks:</p>
<pre>
 crushtool --num_osds 6 -o crush.example.map --build host straw 1 rack straw 2 root straw 0
</pre>

<p>The <i>--num_osds 6</i> parameter specifies that the cluster has six individual 
storage devices at its disposal. The <i>--build</i> option introduces a statement 
with three three-part parameters that follow the <i>&lt;Name&gt; &lt;Internal Crush 
Algorithm&gt; &lt;Number&gt;</i> syntax.</p> 

<p><i>&lt;Name&gt;</i> can be chosen freely; however, it is wise to choose something 
meaningful. <i>host straw 1</i> specifies that one replica is allowed per host, 
and <i>rack straw 2</i> tells RADOS that two servers exist per rack. <i>root 
straw 0</i> refers to the number of racks and determines that the replicas should be 
distributed equally on all available racks.</p> 

<p>Subsequently, if you want to see the resulting Crush map in plain text, you can 
enter</p>

<pre>
 crushtool -d crush.example.map -o crush.example
</pre>

<p>The file with the map in plain text will then be called <i>crush.example</i> 
(Listing 1).</p> 

<p><b>Listing 1: Crush Map for Six Servers in Two Racks</b></p>
<pre>
001 # begin crush map
002
003 # devices
004 device 0 device0
005 device 1 device1
006 device 2 device2
007 device 3 device3
008 device 4 device4
009 device 5 device5
010
011 # types
012 type 0 device
013 type 1 host
014 type 2 rack
015 type 3 root
016
017 # buckets
018 host host0 {
019         id -1            # do not change unnecessarily
020         # weight 1.000
021         alg straw
022         hash 0  # rjenkins1
023         item device0 weight 1.000
024 }
025 host host1 {
026         id -2            # do not change unnecessarily
027         # weight 1.000
028         alg straw
029         hash 0  # rjenkins1
030         item device1 weight 1.000
031 }
032 host host2 {
033         id -3            # do not change unnecessarily
034         # weight 1.000
035         alg straw
036         hash 0  # rjenkins1
037         item device2 weight 1.000
038 }
039 host host3 {
040         id -4            # do not change unnecessarily
041         # weight 1.000
042         alg straw
043         hash 0  # rjenkins1
044         item device3 weight 1.000
045 }
046 host host4 {
047         id -5            # do not change unnecessarily
048         # weight 1.000
049         alg straw
050         hash 0  # rjenkins1
051         item device4 weight 1.000
052 }
053 host host5 {
054         id -6            # do not change unnecessarily
055         # weight 1.000
056         alg straw
057         hash 0  # rjenkins1
058         item device5 weight 1.000
059 }
060 rack rack0 {
061         id -7            # do not change unnecessarily
062         # weight 2.000
063         alg straw
064         hash 0  # rjenkins1
065         item host0 weight 1.000
066         item host1 weight 1.000
067 }
068 rack rack1 {
069         id -8            # do not change unnecessarily
070         # weight 2.000
071         alg straw
072         hash 0  # rjenkins1
073         item host2 weight 1.000
074         item host3 weight 1.000
075 }
076 rack rack2 {
077         id -9            # do not change unnecessarily
078         # weight 2.000
079         alg straw
080         hash 0  # rjenkins1
081         item host4 weight 1.000
082         item host5 weight 1.000
083 }
084 root root {
085         id -10           # do not change unnecessarily
086         # weight 6.000
087         alg straw
088         hash 0  # rjenkins1
089         item rack0 weight 2.000
090         item rack1 weight 2.000
091         item rack2 weight 2.000
092 }
093
094 # rules
095 rule data {
096         ruleset 1
097         type replicated
098         min_size 2
099         max_size 2
100         step take root
101         step chooseleaf firstn 0 type rack
102         step emit
103 }
104
105 # end crush map
</pre>

<p>Of course, the names of the devices and hosts (<i>device0</i>, <i>device2</i>, ... 
and <i>host1</i>, <i>host2</i>, ...) must be adapted to the local conditions. Thus, 
the name of the device should correspond to the device name in <i>ceph.conf</i> (in 
this example: <i>osd.0</i>), and the hostname should agree with the hostname of the 
server.</p> 

<p><b>Distributing Replicas on the Racks</b></p> 

<p>The replication is defined in line 101 of Listing 1 by <i>step chooseleaf firstn 
0 type rack</i>, which determines that replicas are to be distributed on the racks. 
To achieve a distribution per host, <i>rack</i> would be replaced by <i>host</i>. 
The <i>min_size</i> and <i>max_size</i> parameters (lines 98 and 99) seem 
inconspicuous; however, especially in combination with <i>ruleset 1</i>
 (line 96), they are very important for RADOS to use the rule created. 
Which ruleset from the Crush map RADOS will use is specified for each 
pool; for this, RADOS not only matches names, but also the <i>min_size</i> and 
<i>max_size</i> parameters, which refer to the number of replicas.</p> 

<p>In concrete terms, this means: If RADOS is supposed to process a pool 
according to ruleset 1, which uses two replicas, then the rule in the 
example would apply. However, if the admin has used the command as 
explained above to specify that three replicas should exist for the 
objects in the pool, then RADOS would not apply the rule. To be less 
specific, it is recommended to set <i>min_size</i> to 1 and <i>max_size</i> to 
10 - this rule would then apply for all pools that use ruleset 1 and require from 
one to 10 replicas.</p> 

<p>On the basis of this example, admins will be able to create their own 
Crush maps, which must subsequently find their way back into RADOS.</p> 

<p><b>Extending the Existing Crush Map</b></p> 

<p>Practice
 has shown that it is useful to extend an existing Crush map with new 
entries instead of building a new one from scratch. The following 
command will give access to the Crush map currently in use:</p>

<pre>
 ceph osd getcrushmap -o crush.running.map
</pre>

<p>This command will save the Crush map in binary format in <i>crush.running.map</i>. 
This file can be decoded with</p>

<pre>
 crushtool -d crush.running.map -o crush.map
</pre>

<p>which transfers the plain text version to <i>crush.map</i>. After editing, the Crush 
map must be encoded again with</p>

<pre>
 crushtool -c crush.map -o crush.new.map
</pre>

<p>before it can be sent back to the RADOS cluster with</p>

<pre>
 ceph osd setcrushmap -i crush.new.map
</pre>

<p>Ideally, for a newly created pool to use the new rule, it would be set accordingly 
in <i>ceph.conf</i>; the new rule would simply become the standard value. If the new 
ruleset from the example has the ID <i>4</i> in the Crush map (<i>ruleset 4</i>), 
then the line</p>

<pre>
 osd pool default crush rule = 4
</pre>

<p>would help configuration block <i>[osd]</i>. Additionally,</p>

<pre>
 osd pool default size = 3
</pre>

<p>can be used to determine that new pools should always be created with three 
replicas.</p> 

<p><b>The 4K Limit for Ext3 and Ext4</b></p> 
<p>If you complete a RADOS installation and use ext3 or ext4 as the 
filesystem, you might run into a snare: In these filesystems, the XATTR 
attributes (i.e., the extended file attributes) are limited to a maximum
 of 4KB, and the XATTR entries from RADOS regularly take up just that. 
If no preventive measures are taken, this setup could, in the worst 
case, cause <i>ceph-osd</i> to crash or spit out cryptic error messages like 
(<i>Operation not supported</i>).</p> 

<p>This problem can be remedied using an external file to save the XATTR entries. To 
do so, the line</p>

<pre>
 filestore xattr use omap = true ; for ext3/4 filesystem
</pre>

<p>must be inserted into the <i>[osd]</i> entry in <i>ceph.conf</i> (Figure 3). In 
this way, you can protect the cluster against possible problems.</p>

<div class="attribute-image">
<img src="http://hpc.admin-magazine.com/var/ezflow_site/storage/images/media/images/radceph-f03/61852-1-eng-US/radceph-F03_reference.jpg" style="border: 0px solid ;" alt="radceph-F03" title="radceph-F03" height="600" width="442">
</div>

<div class="attribute-caption" style="width: 442px">
<p>Figure 3: Special care is needed when operating RADOS with ext3 or ext4.</p>
</div>

<p><b>Alternative: The RADOS Block Device</b></p> 
<p>In the first part of this workshop [1], I took an in-depth look at the 
Ceph filesystem, which is a front end for RADOS. Ceph is not, however, 
the only way to access the data deposited in a RADOS store - the RADOS 
block driver (RBD) is an alternative. With <i>rbd</i>, objects in RADOS can be 
addressed as if they were on a hard disk.</p> 

<p>This functionality is especially helpful in the context of virtualization 
because presenting a block device to KVM as a hard disk saves the 
virtualization a detour over container formats like qcow2. Additionally,
 using <i>rbd</i> is very easy - an <i>rbd</i> pool is already included that admins 
can take advantage of. For example, to create an <i>rbd</i> drive with a size of 1GB, 
you only need to use the command</p>

<pre>
 rbd create test --size 1024
</pre>

<p>Subsequently, the block device can be used on any host with an <i>rbd</i> kernel 
module, which is now part of the mainline kernel and is found on practically every 
system. Because <i>ceph.conf</i>  from the first part of this workshop has already 
specified that users must authenticate themselves to gain access to the RADOS services, 
that is also true for <i>rbd</i>. Note that login credentials can be called with:</p>

<pre>
 ceph-authtool -l /etc/ceph/admin.keyring
</pre>

<p>Then, on a machine that has loaded <i>rbd</i>, you can type</p>

<pre>
 echo "IP addresses of the MONs, separated by commas name=admin,secret=Authkeyrbd test" &gt; /sys/bus/rbd/add
</pre>

<p>to activate the RBD drive.</p> 

<p><b>Conclusions</b></p> 

<p>RADOS lets administrators replace two-node storage with seamlessly scalable 
RADOS-based storage. Currently, the project developers plan to turn 
version 0.48 into version 1.0, which will then receive the official 
"Ready for Enterprise" stamp. Because the previous version 0.47 has 
already been released, the enterprise version may be expected soon.</p> 
<p><b>Info</b></p> 

<p><b>[1]</b><u><a href="http://hpc.admin-magazine.com/Articles/The-RADOS-Object-Store-and-Ceph-Filesystem/%28language%29/eng-US" target="_blank">"RADOS and Ceph" by Martin 
Loschwitz</a></u>, ADMIN, Issue 09, pg. 28</p> 

<p><b>The Author</b></p> 

<p>Martin
 Gerhard Loschwitz is Principal Consultant at hastexo, where he is 
intensively involved with high-availability solutions. In his spare 
time, he maintains the Linux cluster stack for Debian GNU/Linux.</p>
	</div>



<a name="CephKvm"></a><h3 class="title entry-title" itemprop="name">
A Basic Ceph Storage &amp; KVM Virtualisation Tutorial
<a href="http://blog.bob.sh/2012/02/basic-ceph-storage-kvm-virtualisation.html"
target="newwindow">(Source Origin)</a></h3>  

<div class="article-content entry-content" itemprop="articleBody">So I had been 
meaning to give <a href="http://ceph.com/">CEPH</a>
 &amp; KVM Virtualisation a whirl in the lab for quite some time now. 
Here I have provided for you all a set of command-by-command 
instructions I used for setting it up on a single host. The goal here is
 really just to get it to the 'working' stage for some basic functional 
experimentation.

<P> Ideally you would want to set this up on multiple hosts with proper 
network separation so you can see how it performs with real-world 
attenuation.

<h4>Overview</h4>

For those who don't know it, CEPH is a distributed storage solution that
 allows you to scale horizontally with multiple machines/heads instead 
of the more traditional methodologies which use centralised heads with 
large amounts of storage attached to them.

<P> The principle here is that you should be able to buy lots of inexpensive
 computers with a bunch of direct attached storage and just cluster them
 to achieve scalability. Also, without a central point of failure or 
performance bottleneck you should be able to scale beyond the 
limitations of our past storage architectures.

<P> So CEPH like most distributed storage solutions, really has 3 main components:

<ul>
   <li><b>Object Storage Device (OSD):</b> this is where the blocks get stored. You 
       would usually have lots of these.</li>
   <li><b>Meta-Data Server (MDS):</b> This where the metadata gets stored. 
       You would have less of these. They are used for looking up where the 
       blocks are stored and storing meta-data about files and blocks.</li>
    <li><b>Monitor (MON):</b> Cluster management, configuration and state. This 
       component keeps track of the state of the clustering really.</li>
</ul>

<P> Here is a basic diagram provided from the official site (so yes I stole it - I 
hope thats okay):

<div class="separator" style="clear: both; text-align: center;">
<img src="http://ceph.com/w/images/f/f6/Ceph_architecture.png" 
height="259" border="0" width="640"></div>


<P> As you can see, ideally these components are meant to be ran on different sets 
of systems, with the OSD component being the most frequent. I'm just going to run 
them all on the same host for this demo, which is useful for a functional test, but 
not for a destructive or performance test.

<P> By the way the OSD part can used different types of backends and filesystems 
for storage, but in this example I've chosen BTRFS.

<P> So CEPH itself supports multiple different ways of mounting its storage, which 
makes it quite a flexible solution.

<P> In this demo I'm going to concentrate only on the RBD and Ceph DFS mechanisms.

<P> This installation was tested with:

<ul>
   <li>ceph 0.4.0</li>
   <li>qemu-kvm 1.0</li>
   <li>libvirt 0.9.8</li>
   <li>debian 7.0 (wheezy)</li>
</ul>


<P> I'm using the bleeding edge versions of these components because CEPH is really 
in heavy development its better to see how the main-line of development works to get 
a clearer picture.

<h4>OS Preparation</h4>

<P> It was tested with real hardware hosted by Hetzner in Germany. The box specs were 
roughly something like:

<ul>
<li>Quad-Core processor</li>
<li>16 GB of RAM</li>
<li>2 x 750 GB disks (no hardware raid)</li>
</ul>

<P> To begin, I personally built a Debian 6.0 system (because thats all Hetzner 
offers you within its Robot tool) with a spare partition that I later use for the 
OSD/BTRFS volume. The layout was something like:

<ul>
   <li><b>/dev/md0:</b> /boot, 512MB</li>
   <li><b>/dev/md1:</b> LVM, 74 GB</li>
   <li><b>/dev/md2:</b> the rest of the disk</li>
</ul>

<P> And in the LVM partition I defined the following logical volumes:

<ul>
   <li><b>/:</b> 50 GB (ext4)</li>
   <li><b>swap:</b> 20 GB</li>
</ul>

<P> The device /dev/md2 I reserved for BTRFS. I believe a more optimal configuration 
is to not use an MD device but just use /dev/sda2 &amp; /dev/sda3 and let BTRFS do 
the mirroring. I however have no data or performance statistics to prove this at the 
moment.

<P> To get the system upgraded from Debian 6 to 7 is fairly straight-forward. First 
update the APT sources list.

<br>
<b>/etc/apt/sources.list:</b>

<PRE>
 deb http://ftp.de.debian.org/debian/ wheezy main contrib non-free
 deb http://ftp.de.debian.org/debian/ wheezy-proposed-updates main contrib non-free
</PRE>

<P>Then run the following to then get the latest updates:

<PRE>
 $ apt-get update
 $ apt-get -y dist-upgrade
</PRE>

<P> The kernel would have been upgraded so you should reboot at this point.

<h4>CEPH Installation</h4>

<P> Install CEPH using the ceph package. This should pull in all the dependencies 
  you need.

<PRE>
 $ apt-get install ceph</span>
</PRE>

<P> Create some directories for the various CEPH components:

<PRE>
 $ mkdir -p /srv/ceph/{osd,mon,mds}
</PRE>

<P>I used a configuration file like this below. Obviously you will need to change the 
various parts to suit your environment. I've left out authentication in this demo for 
simplicity, although if you want to do real destructive and load-testing you should 
always include this.

<div>
<b>/etc/ceph/ceph.conf:</b></div>

<PRE>
[global]
    log file = /var/log/ceph/$name.log
    pid file = /var/run/ceph/$name.pid
[mon]
    mon data = /srv/ceph/mon/$name
    [mon.&lt;your_short_hostname&gt;]
    host = &lt;your_hostname&gt;
    mon addr = &lt;your_ip_address&gt;:6789
[mds]
    [mds.&lt;your_short_hostname&gt;]
    host = &lt;your_hostname&gt;
[osd]
    osd data = /srv/ceph/osd/$name
    osd journal = /srv/ceph/osd/$name/journal
    osd journal size = 1000 ; journal size, in megabytes
[osd.0]
    host = &lt;your_hostname&gt;
    btrfs devs = /dev/md2
    btrfs options = rw,noatime
</PRE>

<P> Now for configuration, CEPH chooses to try and SSH into remote boxes and configure 
things. I believe this is nice for people who are just getting started, but I'm not 
sure if this is correct going forward if you already have your own Configuration 
Management tool like <a href="http://puppetlabs.com/puppet/whats-new/" 
target="newwindow">Puppet</a>, <a href="http://docs.puppetlabs.com/pe/" 
target="newwindow">Puppet Docs</a>, <a href="http://www.opscode.com/chef/" 
target="newwindow">Chef</a> or <a href="https://cfengine.com/manuals/cf3-tutorial" 
target="newwindow">CFEngine Turo</a> <a href="https://cfengine.com/" 
target="newwindow">CFEngine</a>.


<P> So to begin with, there is a command that will initialise your CEPH filesystems 
based on the configuration you have provided:

<PRE>
 $ /sbin/mkcephfs -a -c /etc/ceph/ceph.conf --mkbtrfs --no-copy-conf
</PRE>


<P> Starting the daemon was a little strange (see the -a switch?):

<PRE>
 $ /etc/init.d/ceph -a start
</PRE>


<P> So just to test its all working, lets mount the CEPH DFS volume onto the 
local system:

<PRE>
 $ mount -t ceph &lt;your_hostname&gt;:/ /mnt
</PRE>


<P> What you are looking at here is the CEPH object store mounted in /mnt. This is 
a shared object store - and you should be able to have multiple hosts mount it just 
like NFS. As mentioned before-hand however, this is not the only way of getting 
access to the CEPH storage cluster.

<h4>CEPH DFS &amp; Directory Snapshots</h4>


<P> So I just wanted to segway a little and talk about this neat feature. 
CEPH &amp; the ceph based mount point above has the capability to do 
per-directory snapshots which could come in useful. The interface is 
quite simple as well.

<div>
<b>making a snapshot:</b></div>

<PRE>
 $ mkdir /mnt/test
 $ cd /mnt/test
 $ touch a b c
 $ mkdir .snap/my_snapshot
</PRE>

<div>
<b>deleting a snapshot:</b></div>

<PRE>
 $ rmdir .snap/my_snapshot
</PRE>

<div>
<b>finding a snapshot:</b></div>


<P> The .snap directory won't show up when you do a ls -la in the dir.


<P> Simply assume its there and do something like:

<PRE>
 $ ls -la .snap
</PRE>


<P> ... in the directory and the snapshots should show up under the names you created 
them with.

<h4>RADOS Block Device</h4>


<P> So an alternative way of using your CEPH storage is by using RBD. The RBD interface 
gives you the capability to expose an object onto a remote system as a block device. 
Obviously this has the same caveats as any block device, so multiple hosts that mount 
the same device must ensure they use some sort of clustered file system such as OCFS2.


<P> So first if its not already, load the rbd kernel module:

<PRE>
 $ modprobe rbd
</PRE>


<P> Using the 'rbd' command line tool, create an image (size is in megs):

<PRE>
 $ rbd create mydisk --size 10000
</PRE>


<P> You can list the current images if you want:

<PRE>
 $ rbd list
</PRE>


<P> Now to mount the actually device, you just have to tell the kernel first:

<PRE>
 $ echo "&lt;your_ip_address&gt; name=admin rbd mydisk" &gt; /sys/bus/rbd/add
</PRE>


<P> It should create a device like /dev/rbd/rbd/mydisk. Lets now&nbsp;format it with 
a real filesystem and mount it:

<PRE>
  $ mkfs -t ext4 /dev/rbd/rbd/mydisk
  $ mkdir /srv/mydisk
  $ mount -t ext4 /dev/rbd/rbd/mydisk /srv/mydisk
</PRE>

<h4>KVM/Qemu Support</h4>


<P> QEMU (and libvirt for that matter) at some point merged in patched to 
allow you to specify an 'rbd' store as a backend to a QEMU virtual 
instance. I'm going to focus on using an Intel/KVM image for this 
tutorial.


<P> So lets start by installing KVM &amp; Qemu and the various other pieces 
we'll need:

<PRE>
 $ apt-get install kvm libvirt-bin virtinst iptables-persistent
</PRE>


<P> We want to probably create a pool for vm disks separate from the pre-existing 
ones. You can create as many of these as you need:

<PRE>
 $ rados mkpool vm_disks
</PRE>


<P> Now create a qemu image inside the pool. Notice we are just using 'qemu-image' 
to do this?

<PRE>
 $ qemu-img create -f rbd rbd:vm_disks/box1_disk1 10G
</PRE>


<P> Create yourself a bridge network by modifying the correct Debian configuration 
file.

<div>
<b>/etc/network/interfaces:</b></div>

<PRE>
auto virbr0
iface virbr0 inet static
&nbsp; bridge_ports none
&nbsp; address 192.168.128.1
&nbsp; netmask 255.255.255.0
&nbsp; network 192.168.128.0
&nbsp; broadcast 192.168.128.255
</PRE>


<P> And now bring up the interface:</div>

<PRE>
 $ ifup --verbose virbr0
</PRE>


<P> We'll need some firewall rules so that NAT works in this case. Obviously your 
network needs may vary here.

<div>
<b>/etc/iptables/rules.v4:</b></div>


<PRE>
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A FORWARD -s 192.168.128.0/24 -m comment --comment "100 allow forwarding from internal" -j ACCEPT
-A FORWARD -d 192.168.128.0/24 -m comment --comment "100 allow forwarding to internal" -j ACCEPT
COMMIT
*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
-A POSTROUTING -s 192.168.128.0/24 -o eth0 -m comment --comment "500 outbound nat for internal" -j MASQUERADE
COMMIT
</PRE>


<P> And restart iptables-persistent to load the rules:

<PRE>
 $ service iptables-persistent restart
</PRE>


<P> Turn on forwarding for IPv4:</div>

<PRE>
 $ echo 1 &gt; /proc/sys/net/ipv4/ip_forward
</PRE>


<P> Now that the network is done, we want to create a script to help us launch our 
VM instance.


<P> First of all create a device definition file called disk.xml with the following 
contents. This allows us to work-around limitations in virt-install, as it doesn't 
yet support these extra options as command-line arguments.

<div>
<b>/root/disk.xml:</b></div>


<PRE>
&lt;disk type='network' device='disk'&gt;
&nbsp; &lt;source protocol='rbd' name='vm_disks/box1_disk1'/&gt;
&nbsp; &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;
</PRE>


<P> Now lets create our script.

<div>
<b>/root/virt.sh:</b></div>

<div>
<PRE>
#!/bin/bash

set -x

virt-install \
  --name=box1 \
  --ram=512 \
  --vcpus=1 \
  --location=http://ftp.de.debian.org/debian/dists/wheezy/main/installer-amd64/ \
  --extra-args="console=ttyS0" \
  --serial=pty \
  --console=pty,target_type=serial \
  --os-type=linux \
  --os-variant=debiansqueeze \
  --network=bridge=virbr0,model=virtio \
  --graphics=none \
  --virt-type=kvm \
  --noautoconsole \
  --nodisks

# This is because virt-install doesn't support passing rbd&nbsp;
# style disk settings yet.
# Attaching it quickly before system boot however seems to work
virsh attach-device box1 disk.xml --persistent
</PRE>


<P> And finally we should be able to run it:

<PRE>
 ./virt.sh
</PRE>


<P> Now attach to the console and go through the standard installation steps for 
the OS.

<PRE>
 virsh console box1
</PRE>

<div class="separator" style="clear: both; text-align: center;">
<img src="http://4.bp.blogspot.com/-KPRCdNFZ44A/Ty748m3pcQI/AAAAAAAAB2o/Yxll4XwL7tA/s640/Screen+Shot+2012-02-05+at+21.46.25.png" height="388" border="0" width="640"></div>


<P> <b>Note:</b>&nbsp;There is no DHCP or DNS server setup - for this test I just 
provided a static IP and used my own DNS servers.


<P> As you go through the setup, the RBD disk we defined and created should 
be available like a normal disk as you would expect. After installation 
you shouldn't really notice any major functional difference.


<P> Once installation is complete, you should be able to boot the system:

<PRE>
 virsh start box 1
</PRE>

<P>And then access the console:

<PRE>
# virsh console box1
Connected to domain box1
Escape character is ^]

Debian GNU/Linux 6.0 box1 ttyS0
box1 login:
</PRE>

And then your done.</div>

<h4>Summary</h4>


<P> So this is quite an interesting exercise and one worth doing, but the software 
is still very much early-release. They even admit this themselves.


<P> I'm wary of performance and stability more then anything, something I 
can't test with just a single host - so if I ever get the time I'd 
really like to run this thing properly.


<P> I had a brief look at the operations guide, and it seems the 
instructions for adding and removing a host to the OSD cluster looks not
 as automatic as I would like it. Ideally, you really want the kind of 
behaviour that ElasticSearch offers on this level so that adding and 
removing nodes is almost a brain-dead task. Having said that, adding a 
node seems easier then some of the storage systems/solutions I've seen 
about the place :-).


<P> So regardless of my concerns - I think this kind of storage is 
definitely the future and I'm certainly cheering the CEPH team on for 
this one. The functionality was fun (and yes kind of exciting) to play 
with and I can see that the real-world possibilities of such a solution 
in the open-source arena are quite probable now.

<h4>Other things to try from here</h4>

<ul>
   <li>Check out another alternative: 
   <a href="http://www.osrg.net/sheepdog/">Sheepdog</a>, which also seems to be 
      gaining ground but only on the QEMU storage front. Its a very specific solution 
      as apposed to CEPH's generic storage solution.</li>
   <li>Test CEPH integration with OpenNebula and OpenStack so you can see it within a 
      full cloud provisioning case. This might require some custom scripts to support 
      cloning RBD-stored base images etc. but should be interesting.</li>
   <li>Test the <a href="http://ceph.com/w/index.php?title=S3-compatible_Gateway">S3 
      emulation</a> provided by the RadosGW component.</li>
</ul>

<h4>References</h4>

<OL>

  <LI> Formal (yet incomplete) Documentation:
&nbsp;&nbsp;<a href="http://ceph.com/docs/next/">http://ceph.com/docs/next/</a>

  <LI> Wiki:

&nbsp;&nbsp;<a href="http://ceph.com/w/index.php?title=Main_Page">http://ceph.com/w/index.php?title=Main_Page</a>

<LI> Installation on Debian:
&nbsp; <a href="http://ceph.com/w/index.php?title=Installing_on_Debian">http://ceph.com/w/index.php?title=Installing_on_Debian</a>

<LI> RBD:
&nbsp; <a href="http://ceph.com/w/index.php?title=Rbd">http://ceph.com/w/index.php?title=Rbd</a>

<LI> QEMU-RBD:
&nbsp;&nbsp;<a href="http://ceph.com/w/index.php?title=QEMU-RBD">http://ceph.com/w/index.php?title=QEMU-RBD</a>

<LI> Snapshots:
&nbsp; <a href="http://ceph.com/w/index.php?title=Snapshots">http://ceph.com/w/index.php?title=Snapshots</a>
</OL>

<div class="publish-info">
    Posted <abbr class="time published" title="2012-02-05T21:54:00.001Z" itemprop="datePublished">6th February</abbr> by <a class="url fn" href="https://profiles.google.com/113052903761064527606" rel="author" itemprop="author">Ken Barber</a>
</div>

<h3><a name="CephTutorial"></a>Setting up a Ceph cluster and exporting a RBD volume to a KVM guest <a href="http://berrange.com/posts/2011/10/12/setting-up-a-ceph-cluster-and-exporting-a-rbd-volume-to-a-kvm-guest/">(Source Origin)</a></h3>

			<div class="meta"><b>Posted:</b> October 12th, 2011 | 
<b>Author:</b> <a href="http://berrange.com/posts/author/admin/" title="Posts by 
Daniel Berrange" rel="author">Daniel Berrange</a> 

<h4>Host Cluster Setup, the easy way</h4>
<p>Fedora has included Ceph for a couple of releases, but since my hosts
 are on Fedora 14/15, I grabbed the latest ceph 0.3.1 sRPMs from Fedora 
16 and rebuilt those to get something reasonably up2date. In the end I 
have the following packages installed, though to be honest I don't 
really need anything except the base 'ceph' RPM:</p>
<pre>
# rpm -qa | grep ceph | sort
ceph-0.31-4.fc17.x86_64
ceph-debuginfo-0.31-4.fc17.x86_64
ceph-devel-0.31-4.fc17.x86_64
ceph-fuse-0.31-4.fc17.x86_64
ceph-gcephtool-0.31-4.fc17.x86_64
ceph-obsync-0.31-4.fc17.x86_64
ceph-radosgw-0.31-4.fc17.x86_64</pre>
<p>Installing the software is the easy bit, configuring the cluster is 
where the fun begins. I had three hosts available for testing all of 
which are virtualization hosts. Ceph has at least 3 daemons it needs to 
run, which should all be replicated across several hosts for redundancy.
 There's no requirement to use the same hosts for each daemon, but for 
simplicity I decided to run every Ceph daemon on every virtualization 
host.</p>
<p>My hosts are called <code>lettuce</code>, <code>avocado</code> and 
<code>mustard</code>. Following the 
<a href="http://ceph.com/w/index.php?title=Cluster_configuration">Ceph wiki 
instructions</a>, I settled on a configuration file that looks like this:</p>
<pre>
[global]
&nbsp;&nbsp; &nbsp;auth supported = cephx
&nbsp;&nbsp;&nbsp;&nbsp;keyring = /etc/ceph/keyring.admin

[mds]
&nbsp;&nbsp; &nbsp;keyring = /etc/ceph/keyring.$name
[mds.lettuce]
&nbsp;&nbsp; &nbsp;host = lettuce
[mds.avocado]
&nbsp;&nbsp; &nbsp;host = avocado
[mds.mustard]
&nbsp;&nbsp; &nbsp;host = mustard

[osd]
&nbsp;&nbsp; &nbsp;osd data = /srv/ceph/osd$id
&nbsp;&nbsp; &nbsp;osd journal = /srv/ceph/osd$id/journal
&nbsp;&nbsp;&nbsp; osd journal size = 512
&nbsp;&nbsp;&nbsp; osd class dir = /usr/lib64/rados-classes
&nbsp;&nbsp; &nbsp;keyring = /etc/ceph/keyring.$name
[osd.0]
&nbsp;&nbsp; &nbsp;host = lettuce
[osd.1]
&nbsp;&nbsp; &nbsp;host = avocado
[osd.2]
&nbsp;&nbsp; &nbsp;host = mustard

[mon]
&nbsp;&nbsp; &nbsp;mon data = /srv/ceph/mon$id
[mon.0]
&nbsp;&nbsp; &nbsp;host = lettuce
&nbsp;&nbsp; &nbsp;mon addr = 192.168.1.1:6789
[mon.1]
&nbsp;&nbsp; &nbsp;host = avocado
&nbsp;&nbsp; &nbsp;mon addr = 192.168.1.2:6789
[mon.2]
&nbsp;&nbsp; &nbsp;host = mustard
&nbsp;&nbsp; &nbsp;mon addr = 192.168.1.3:6789</pre>

<p>The <code>osd class dir</code> bit should not actually be required, but the OSD 
code <a href="https://bugzilla.redhat.com/show_bug.cgi?id=745460">looks in the wrong 
place</a> (/usr/lib instead of /usr/lib64) on x86_64 arches.</p>

<p>With the configuration file written, it is time to actually 
initialize the cluster filesystem / object store. This is the really fun
 bit. The Ceph wiki has 
<a href="http://ceph.com/w/index.php?title=Creating_a_new_file_system">a very basic 
page</a> which talks about the <code>mkcephfs</code> tool, along with a scary warning 
about how it'll 'rm -rf' all the data on the filesystem it is initializing. It turns 
out that it didn't mean your entire host filesystem, As far as I can tell, it only 
blows away the contents of the directory configured for '<code>osd data</code>' and 
'<code>mon data</code>', in my case both under <code>/srv/ceph</code>.</p>

<p>The recommended way is to let <code>mkcephfs</code> ssh into each of your hosts 
and run all the configuration tasks automatically. Having tried the non-recommended 
way and failed several times before finally getting it right, I can recommend 
following the recommended way :-P There are some caveats not mentioned in the wiki 
page though:</p>
<ul>
<li>The configuration file above must be copied to <code>/etc/ceph/ceph.conf</code> 
on <strong>every</strong> node <strong>before</strong> attempting to run 
<code>mkcephfs</code>.</li>
<li>The configuration file on the host where you run <code>mkcephfs</code> must be 
in /etc/ceph/ceph.conf or it will get rather confused about where it is in the other 
nodes.</li>
<li>The <code>mkcephfs</code> command must be run as root since, it doesn't specify 
'-l root' to ssh, leading to an inability to setup the nodes.</li>
<li>The directories <code>/srv/ceph/osd$i</code> must be pre-created, since it is 
unable to do that itself, despite being able to creat the 
<code>/srv/ceph/mon$i</code> directories.</li>
<li>The Fedora RPMs have also 
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=745462">forgotten to create 
<code>/etc/ceph</code></a></li>
</ul>

<p>With that in mind, I ran the following commands from my laptop, as root</p>
<pre>
 # n=0
 # for host in lettuce avocado mustard ; \
   do \
       ssh root@$host mkdir -p /etc/ceph /srv/ceph/mon$n; \
       n=$(expr $n + 1); \
       scp /etc/ceph/ceph.conf root@$host:/etc/ceph/ceph.conf
   done
 # mkcephfs -a -c /etc/ceph/ceph.conf -k /etc/ceph/keyring.bin</pre>

<p>On the host where you ran <code>mkcephfs</code> there should now be a file 
<code>/etc/ceph/keyring.admin</code>. This will be needed for mounting filesystems. 
I copied it across to all my virtualization hosts</p>

<pre>
 # for host in lettuce avocado mustard ; \
   do \
       scp /etc/ceph/keyring.admin root@$host:/etc/ceph/keyring.admin; \
   done
</pre>

<h4>Host Cluster Usage</h4>

<p>Assuming the setup phase all went to plan, the cluster can now be started. A word of 
warning though, Ceph <strong>really wants your clocks VERY well synchronized</strong>.
 If your NTP server is a long way away, the synchronization might not be good enough 
to stop Ceph complaining. You really want a NTP server on your local LAN for hosts to 
sync against. Sort this out before trying to start the cluster.</p>

<pre>
 # for host in lettuce avocado mustard ; \
   do \
       ssh root@$host service ceph start; \
   done
</pre>

<p>The <code>ceph</code> tool can show the status of everything. The 'mon', 'osd' and 
'msd' lines in the status ought to show all 3 host present &amp; correct</p>
<pre>
# ceph -s
2011-10-12 14:49:39.085764    pg v235: 594 pgs: 594 active+clean; 24 KB data, 94212 MB used, 92036 MB / 191 GB avail
2011-10-12 14:49:39.086585   mds e6: 1/1/1 up {0=lettuce=up:active}, 2 up:standby
2011-10-12 14:49:39.086622   osd e5: 3 osds: 3 up, 3 in
2011-10-12 14:49:39.086908   log 2011-10-12 14:38:50.263058 osd1 192.168.1.1:6801/8637 197 : [INF] 2.1p1 scrub ok
2011-10-12 14:49:39.086977   mon e1: 3 mons at {0=192.168.1.1:6789/0,1=192.168.1.2:6789/0,2=192.168.1.3:6789/0}
</pre>

<p>The cluster configuration I chose has authentication enabled, so to actually mount 
the ceph filesystem requires a secret key. This key is stored in the 
<code>/etc/ceph/keyring.admin</code> file that was created earlier. To view the keyring 
contents, the <code>cauthtool</code> program must be used</p>

<pre>
# cauthtool -l /etc/ceph/keyring.admin 
[client.admin]
	key = AQDLk5VOeHkHLxAAfGjcaUsOXOhJr7hZCNjXSQ==
	auid = 18446744073709551615
</pre>

<p>The base64 key there will be passed to the <code>mount</code> command, repeating on 
every host needing a filesystem present:</p>

<pre>
 # mount -t ceph 192.168.1.1:6789:/ /mnt/ -o name=admin,secret=AQDLk5VOeHkHLxAAfGjcaUsOXOhJr7hZCNjXSQ==
error adding secret to kernel, key name client.admin: No such device
</pre>

<p>For some reason, that error message is always printed on my Fedora hosts, and 
despite that, the mount <strong>has actually succeeded</strong></p>
<pre>
# grep /mnt /proc/mounts 
192.168.1.1:6789:/ /mnt ceph rw,relatime,name=admin,secret= 0 0
</pre>

<p>Congratulations, <code>/mnt</code> is now a distributed filesystem. If you create 
a file on one host, it should appear on the other hosts &amp; vica-verca.</p>

<h4>RBD Volume setup</h4>

<p>A shared filesystem is very nice, and can be used to hold regular virtual disk 
images in a variety of formats (raw, qcow2, etc). What I really wanted to try was the 
RBD virtual block device functionality in QEMU. Ceph includes a tool called 
<code>rbd</code> for manipulating those. The syntax of this tool is pretty 
self-explanatory</p>

<pre>
# rbd create --size 100 demo
# rbd ls
demo
# rbd info demo
rbd image 'demo':
	size 102400 KB in 25 objects
	order 22 (4096 KB objects)
	block_name_prefix: rb.0.0
	parent:  (pool -1)
</pre>

<p>Alternatively RBD volume creation can be done using <code>qemu-img</code> ..., 
at least once the Fedora QEMU package is fixed to enable RBD support.</p>

<pre>
# qemu-img create -f rbd rbd:rbd/demo  100M
Formatting 'rbd:rbd/foo', fmt=rbd size=104857600 cluster_size=0 
# qemu-img info rbd:rbd/demo
image: rbd:rbd/foo
file format: raw
virtual size: 100M (104857600 bytes)
disk size: unavailable
</pre>

<h4>KVM guest setup</h4>

<p>The syntax for configuring a RBD block device in libvirt, is very similar to that 
used for Sheepdog. In Sheepdog, every single virtualization node is also a storage 
node, so there is no hostname required. Not so for RBD. Here it is necessary to 
specify one or more host names, for the RBD servers.</p>

<pre>&lt;disk type='network' device='disk'&gt;
  &lt;driver name='qemu' type='raw'/&gt;
  &lt;source protocol='rbd' name='demo/wibble'&gt;
    &lt;host name='lettuce.example.org' port='6798'/&gt;
    &lt;host name='mustard.example.org' port='6798'/&gt;
    &lt;host name='avocado.example.org' port='6798'/&gt;
  &lt;/source&gt;
  &lt;target dev='vdb' bus='virtio'/&gt;
&lt;/disk&gt;
</pre>

<p>More observant people might be wondering how QEMU gets permission to connect to the 
RBD server, given that the configuration earlier enabled authentication. This is thanks 
to the magic of the <code>/etc/ceph/keyring.admin</code> file which must exist on any 
virtualization server. Patches are currently being discussed which will allow 
authentication credentials to be set via libvirt, avoiding the need to store the 
credentials on the virtualization hosts permanently.</p>

<a name="CephOpenStack"><h3 class="entry-title">Introducing Ceph to OpenStack
<a href="http://www.sebastien-han.fr/blog/2012/06/10/introducing-ceph-to-openstack/">(Source Origin)</a></h3>
<p class="meta">
<time datetime="2012-06-10T00:04:00+02:00" pubdate="" data-updated="true">Jun 10<span>th</span>, 2012</time>
| <a href="#disqus_thread">Comments</a>
</p>
</header>
<div class="entry-content"><p>
<img src="http://www.sebastien-han.fr/images/ceph-openstack.png"></p>
 
<p></p>
<h3>I. Ceph introduction</h3>
<p>Ceph is a massively scalable, open source, distributed storage 
system. It is comprised of an object store, block store, and a 
POSIX-compliant distributed file system. The platform is capable of 
auto-scaling to the exabyte level and beyond, it runs on commodity 
hardware, it is self-healing and self-managing, and has no single point 
of failure. Ceph is in the Linux kernel and is integrated with the 
OpenStack cloud operating system. As a result of its open source 
nature, this portable storage platform may be installed and used in 
public or private clouds.</p>
<p><img src="http://www.sebastien-han.fr/images/Ceph-architecture.png"></p>
<h4>I.1. RADOS?</h4>
<p>You can easily get confused by the denomination: Ceph? RADOS?</p>
<p><strong>RADOS: Reliable Autonomic Distributed Object Store</strong> 
is an object storage. RADOS takes care of distributing the objects 
across the whole storage cluster and replicating them for fault 
tolerance. It is built with 3 major components:</p>
<ul>
<li><strong>Object Storage Device (OSD)</strong>: the storage daemon - 
RADOS service, the location of your data. You must have this daemon 
running on each server of your cluster. For each OSD you can have an 
associated hard drive disks. For performance purpose it's usually better
 to pool your hard drive disk with raid arrays, LVM or btrfs pooling. 
With that, for one server your will have one daemon running. By default,
 three pools are created: data, metadata and RBD.</li>
<li><strong>Meta-Data Server (MDS)</strong>: this is where the metadata 
are stored. MDSs build POSIX file system on top of objects for Ceph 
clients. However if you are not using the Ceph File System, you do not 
need a meta data server.</li>
<li><strong>Monitor (MON)</strong>: this lightweight daemon handles all 
the communications with the external applications and the clients. It 
also provides a consensus for distributed decision making in a 
Ceph/RADOS cluster. For instance when you mount a Ceph shared on a 
client you point to the address of a MON server. It checks the state and
 the consistency of the data. In an ideal setup you will at least run 3 
<code>ceph-mon</code> daemons on separate servers. Quorum decisions and 
calculs are elected by a majority vote, we expressly need odd number.</li>
</ul>
<p>Ceph devoloppers recommend to use btrfs as a filesystem for the 
storage. Using XFS is also possible and might be a better alternative 
for production environements. Neither Ceph nor Btrfs are ready for 
production. It could be really risky to put them together. This is why 
XFS is an excellent alternative to btrfs. The ext4 filesystem is also 
compatible but doesn't take advantage of all the power of Ceph.</p>
<blockquote><p>We recommend configuring Ceph to use the XFS file system 
in the near term, and btrfs in the long term once it is stable enough 
for production.</p></blockquote>
<p><a href="http://ceph.com/docs/master/rec/filesystem/">For more information 
about usable file system</a></p>

<h4>I.2. Ways to store, use and expose data</h4>
<p>Several ways to store and access your data :)</p>
<ul>
<li><strong>RADOS</strong>: as an object, default storage mecanism.</li>
<li><strong>RBD</strong>: as a block device. The linux kernel RBD (rados
 block device) driver allows striping a linux block device over multiple
 distributed object store data objects. It is compatible with the 
<a href="http://ceph.com/w/index.php?title=QEMU-RBD#Creating_rbd_image" 
target="newwindow">kvm RBD image</a>.  See also: 
<a href="http://www.sebastien-han.fr/blog/2012/06/24/use-rbd-on-a-client/" 
target="newwindow">Use RBD on a Client</a></li>.
<li><strong>CephFS</strong>: as a file, POSIX-compliant filesystem.</li>
</ul>
<p>Ceph exposes its distributed object store (RADOS) and it can be accessed via 
multiple interfaces:</p>
<ul>
<li><strong>RADOS Gateway</strong>: Swift and Amazon-S3 compatible RESTful 
interface. For <a href="http://ceph.com/w/index.php?title=RADOS_Gateway">further 
information</a>.</li>
<li><strong>librados</strong> and the related C/C++ bindings.</li>
<li><strong>rbd and QEMU-RBD</strong>: linux kernel and QEMU block devices that 
stripe data across multiple objects.</li>
</ul>

<h4>I.3. IS CEPH PRODUCTION-QUALITY?</h4>

<blockquote><p>The definition of "production quality" varies depending 
on who you ask. Because it can mean a lot of different things depending 
on how you want to use Ceph, we prefer not to think of it as a binary 
term.
At this point we support the RADOS object store, radosgw, and RBD 
because we think they are sufficiently stable that we can handle the 
support workload. There are several organizations running those parts of
 the system in production. Others wouldn't dream of doing so at this 
stage.
The CephFS POSIX-compliant filesystem is functionally-complete and has 
been evaluated by a large community of users, but has not yet been 
subjected to extensive, methodical testing.</p></blockquote>
<p><a href="http://ceph.com/docs/master/faq/">Reference ceph FAQ</a></p>

<h3>II. Ceph installation</h3>
<p>Since there is no <em>stable</em> version, I decided to version with 
the upstream version of Ceph. Thus, I used the Ceph repository, I worked
 with the last version available so <code>0.47.2</code>:
<a href="http://ceph.com/docs/master/start/quick-start/#install-debian-ubuntu" 
   target="newwindow">Add Ceph Packages</a></p>
<PRE>
 $ wget -q -O- https://raw.github.com/ceph/ceph/master/keys/release.asc | sudo apt-key add -
 $ sudo echo deb http://ceph.com/debian/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
 $ sudo apt-get update && sudo apt-get install ceph
</PRE>

<p>Since I don't have thousand nodes I decided to put every services on each node. 
Here my really basic Ceph configuration file:</p>

<pre>
<code>
; Ceph conf file!
; use semi-colon to put a comment!

[global]
    auth supported = cephx
    keyring = /etc/ceph/keyring.admin

[mds]
    keyring = /etc/ceph/keyring.$name
[mds.0]
    host = server-03
[mds.1]
    host = server-04
[mds.2]
    host = server-06

[osd]
    osd data = /srv/ceph/osd$id
    osd journal = /srv/ceph/osd$id/journal
    osd journal size = 512
    osd class dir = /usr/lib/rados-classes
    keyring = /etc/ceph/keyring.$name

    ; working with ext4
    filestore xattr use omap = true

    ; solve rbd data corruption
    filestore fiemap = false

[osd.0]
    host = server-03
    devs = /dev/mapper/nova--volumes-lvol0
[osd.1]
    host = server-04
    devs = /dev/mapper/server-04-lvol0
[osd.2]
    host = server-06
    devs = /dev/sdb

[mon]
    mon data = /srv/ceph/mon$id
[mon.0]
    host = server-03
    mon addr = 172.17.1.4:6789
[mon.1]
    host = server-04
    mon addr = 172.17.1.5:6789
[mon.2]
    host = server-06
    mon addr = 172.17.1.7:6789
</code>
</pre>

<p>Generate the keyring authentication, deploy the configuration and 
configure the nodes. I will highly recommand to previously setup a SSH 
key authentication based because mkcephfs will attempt to connect via 
SSH to each servers (hostnames) you provided in the ceph configuration 
file. It can be a pain in the arse to enter the ssh password for every 
command run by mkcephfs!</p>
<p>Directory creation is not managed by the script so you have to create them 
manually on each server:</p>

<PRE>
server-03:~$ sudo mkdir -p /srv/ceph/{osd0,mon0}
server-04:~$ sudo mkdir -p /srv/ceph/{osd1,mon1}
server-06:~$ sudo mkdir -p /srv/ceph/{osd2,mon2}
</PRE>


<p>Don't forget to mount your OSD directory according to your disk map 
otherwise Ceph will by default use the root filesystem. It's up to you 
to use ext4 or XFS. For those of you who want to setup an ext4 cluster I
 extremly recommend to use the following mount options for your hard 
drive disks:</p>

<pre>
user_xattr,rw,noexec,nodev,noatime,nodiratime,data=writeback,barrier=0
</pre>

<p>Now run the <code>mkcephfs</code> to deploy your cluster:</p>

<PRE>
 $ sudo mkcephfs -a -c /etc/ceph/ceph.conf -k /etc/ceph/keyring.admin
</PRE>

<p>Ceph doesn't need root permission to execute his command, it simply 
needs to access the keyring. Each Ceph command you execute on the 
command line assumes that you are the <code>client.admin</code> default user. 
The <code>client.admin</code> key has been generated during the <code>mkcephfs</code>
 process. The interesting thing to know about cephx is that it's based 
on Kerberos ticket trust mecanism. If you want to go further with the 
cephx authentication check the 
<a href="http://ceph.com/docs/master/rados/configuration/ceph-conf/#authentication">ceph 
documentation</a> about it. Just make sure that your keyring is readable by 
<strong>everyone</strong>:</p>

<PRE>
 $ sudo chmod +r /etc/ceph/keyring.admin
</PRE>

<p>And launch all the daemons:</p>

<PRE>
 $ sudo service ceph start
</PRE>

<p>This will run every Ceph daemons, namely OSD, MON and MDS (<code>-a</code> flag), 
but you can specify a particular daemon with an extra parameter as <code>osd</code>, 
<code>mon</code> or <code>mds</code>. Now check the status of your cluster by running 
the following command:</p>

<PRE>
 $ ceph -k /etc/ceph/keyring.admin -c /etc/ceph/ceph.conf health
HEALTH_OK
</PRE>

<p>As you can see I'm using the <code>-k</code> option, indeed Ceph 
supports cephx secure authentication between the nodes within the 
cluster, each connection and communication are initiated with this 
authentication mecanism. It depends on your setup but it can be overkill
 to use this system...</p>
<p>All the daemons are running (extract from the server-04):</p>

<PRE>
 $ ps aux | grep ceph
root     22403  0.0  0.1 126204  7748 ?        Ssl  May23   0:35 /usr/bin/ceph-mon -i 1 --pid-file /var/run/ceph/mon.1.pid -c /etc/ceph/ceph.conf
root     22596  0.0  0.3 148680 13876 ?        Ssl  May23   0:08 /usr/bin/ceph-mds -i server-04 --pid-file /var/run/ceph/mds.server-04.pid \
                                                 -c /etc/ceph/ceph.conf
root     22861  0.0 59.8 2783680 2421900 ?     Ssl  May23   2:03 /usr/bin/ceph-osd -i 1 --pid-file /var/run/ceph/osd.1.pid -c /etc/ceph/ceph.conf
</PRE>

<p>Summarize of your ceph cluster status:</p>

<PRE>
 $ ceph -s
pg v623: 576 pgs: 497 active+clean, 79 active+clean+replay; 11709 bytes data, 10984 MB used, 249 GB / 274 GB avail
mds e13: 1/1/1 up {0=server-06=up:active}, 4 up:standby
osd e15: 3 osds: 3 up, 3 in
log 2012-05-23 22:54:00.018319 mon.0 172.17.1.4:6789/0 10 : [INF] mds.0 172.17.1.7:6804/1187 up:active
mon e1: 3 mons at {0=172.17.1.7:6789/0,1=172.17.1.4:6789/0,2=172.17.1.5:6789/0}
</PRE>


<p>You can also use the <code>-w</code> option to provide an endless and live output.</p>

<h4>II.2. Make it grow!</h4>
<p>It's really easy to expand your Ceph cluster. Here I will add a logical volume.</p>

<PRE>
 $ ceph osd create
3
</PRE>


<p>Copy this into your <code>ceph.conf</code> file:</p>

<pre>
[osd.3]
    host = server-03
    devs = /dev/mapper/nova--volumes-lvol0
</pre>

<p>Format, create the OSD directory, mount it:</p>

<PRE>
 $ sudo mkfs.ext4 /dev/mapper/nova--volumes-lvol0
 $ sudo mkdir /srv/ceph/osd3
 $ sudo mount /dev/mapper/nova--volumes-lvol0 /srv/ceph/osd3
</PRE>


<p>Configure the authentifation, permission and grow the crunchmap:</p>

<PRE>
 $ ceph-osd -i 3 --mkfs --mkkey
 $ ceph auth add osd.3 osd 'allow *' mon 'allow rwx' -i /etc/ceph/keyring.osd.3
 $ sudo service ceph start osd.3
</PRE>

<p>At the moment, the OSD is part of the cluster but doesn't store any data, you 
need to add to the crush map:</p>

<PRE>
 $ ceph osd crush set 3 osd.3 1.0 pool=default host=server-03
</PRE>

<p>The migration starts, wait a few seconds and verify the space available with 
the <code>ceph -s</code> command, you should notice that your cluster is growing.</p>
<p>You can also perform this check and see that your storage tree has grown as well:</p>

<PRE>
 $ ceph osd tree
dumped osdmap tree epoch 43
# id  weight  type name   up/down reweight
-1    4   pool default
-3    4       rack unknownrack
-2    1           host server-03
0 1               osd.0   up  1   
-4    2           host server-04
1 1               osd.1   up  1   
3 1               osd.3   up  1   
-5    1           host server-06
2 1               osd.2   up  1
</PRE>

<p>I have 2 'resources' on the server-04 because I added a logical volume.</p>

<h4>II.3. Shrink your cluster</h4>
<p>It's remarkably simple to shrink your ceph cluster. First you need to 
<strong>stop your OSD daemon</strong> and <strong>wait</strong> until the OSD 
is marked as <strong>down</strong>.</p>

<PRE>
 $ ceph osd crush remove osd.1
removed item id 1 name 'osd.1' from crush map
 $ ceph osd rm 1
marked dne osd.1
 $ sudo rm -r /srv/ceph/osd1/
</PRE>

<p>Remove this line from the <code>ceph.conf</code> file:</p>

<pre>
<code>
[osd.1]
    host = server-03
</code>
</pre>

<p>When you work with OSD you will often see the <code>crushmap</code> term. But 
what is the crushmap?</p>
<p>CRUSH is a pseudo-random placement algorithm which tells where data 
(objects) should remain. The crush map contains these information.</p>

<h4>II.4. Re build an OSD from scratch</h4>
<p>Here I rebuilt the OSD number 1:</p>

<PRE>
 $ sudo service ceph stop osd
 $ sudo umount /srv/ceph/osd1/
 $ sudo mkfs.ext4 /dev/mapper/nova--volumes-lvol0
 $ sudo tune2fs -o journal_data_writeback /dev/mapper/nova--volumes-lvol0
</PRE>

<p>Copy this in your <code>fstab</code>:</p>

<pre>
<code>/dev/mapper/nova--volumes-lvol0 /srv/ceph/osd1 ext4 rw,noexec,nodev,noatime,nodiratime,user_xattr,data=writeback,barrier=0 0 0
</code>
</pre>

<PRE>
 $ sudo mount -a
 $ ceph mon getmap -o /tmp/monmap
 $ ceph-osd -c /etc/ceph/ceph.conf --monmap /tmp/monmap -i 1 --mkfs
</PRE>

<p>Finally run the OSD daemon:</p>

<PRE>
 $ sudo service ceph start osd
</PRE>

<h4>II.5. Resize an OSD</h4>
<p>On an LVM based setup, stop the OSD server:</p>

<PRE>
 $ mount | grep osd
/dev/mapper/server4-lvol0 on /srv/ceph/osd1 type ext4 (rw,noexec,nodev,noatime,nodiratime,user_xattr,data=writeback,barrier=0)

 $ sudo service ceph stop osd1
 $ sudo umount /srv/ceph/osd1
</PRE>

<p>Check your LVM status, here I resized my logical volume from 90G to 50G:</p>

<PRE>
 $ sudo lvs
  LV     VG      Attr   LSize  Origin Snap%  Move Log Copy%  Convert
  lvol0  server4 -wi-ao 50.00g
  root   server4 -wi-ao 40.00g
  swap_1 server4 -wi-ao  4.00g

 $ sudo vgs
  VG      #PV #LV #SN Attr   VSize   VFree 
  server4   1   3   0 wz--n- 135.73g 1.73 g

 $ sudo e2fsck -f /dev/server4/lvol0
 $ sudo lvresize /dev/server4/lvol0 -L 50G --resizefs
fsck from util-linux 2.20.1
e2fsck 1.42 (29-Nov-2011)
/dev/mapper/server4-lvol0: clean, 3754/2621440 files, 3140894/10485760 blocks
resize2fs 1.42 (29-Nov-2011)
Resizing the filesystem on /dev/dm-2 to 13107200 (4k) blocks.
The filesystem on /dev/dm-2 is now 13107200 blocks long.

  Reducing logical volume lvol0 to 50.00 GiB
  Logical volume lvol0 successfully resized
</PRE>

<p>Re-mount your device in the OSD directory and launch the OSD daemon:</p>

<PRE>
 $ sudo mount -a
 $ sudo service ceph osd1 start
</PRE>

<p>Check the status <code>ceph -w</code>, you should noticed that the size changed 
and that everything is back to normal.</p>

<h4>II.6. Adjust the replication level</h4>
<p>The replication level is set to 2 by default, you can easily check this with 
the <code>size 2</code> value:</p>

<PRE>
 $ ceph osd dump | grep ^pool
pool 0 'data' rep size 2 crush_ruleset 0 object_hash rjenkins pg_num 192 pgp_num 192 last_change 1 owner 0 crash_replay_interval 45
pool 1 'metadata' rep size 2 crush_ruleset 1 object_hash rjenkins pg_num 192 pgp_num 192 last_change 1 owner 0
pool 2 'rbd' rep size 2 crush_ruleset 2 object_hash rjenkins pg_num 192 pgp_num 192 last_change 1 owner 0
pool 3 'nova' rep size 2 crush_ruleset 0 object_hash rjenkins pg_num 8 pgp_num 8 last_change 22 owner 0
pool 4 'images' rep size 2 crush_ruleset 0 object_hash rjenkins pg_num 8 pgp_num 8 last_change 10 owner 0
</PRE>

<p>Of course each pool might store more critical data, for instance my pool called 
<code>nova</code> store the RBD volume of each virtual machine, so I increased the 
replication level like this:</p>

<PRE>
 $ ceph osd pool set nova size 3
set pool 3 size to 3
</PRE>

<h4>II.7. Connect your client</h4>
<p>Clients can access the RADOS cluster either directly via <code>librados</code> 
with <code>rados</code> command. The usage of <code>librbd</code> is possible as 
well with the RBD tool (via <code>rbd</code>
 command), which creates an image / volume abstraction over the object 
store. To achieve highly available monitor, simply put all of them in 
the mount option:</p>

<PRE>
 $ ceph-authtool --print-key /etc/ceph/keyring.admin
AQARB71PUCuuAxAAPhlUGzkRdDdjNDJy1w8MQQ==

client:~$ sudo mount -t ceph 172.17.1.4:6789,172.17.1.5:6789,172.17.1.7:6789:/ /mnt/ -vv -o \
  name=admin,secret=AQDVGc5P0LXzJhAA5C019tbdrgypFNXUpG2cqQ==
parsing options: rw,name=admin,secret=AQDVGc5P0LXzJhAA5C019tbdrgypFNXUpG2cqQ==
</PRE>

<p>Monitor reliability?</p>
<p>I tried to simulate a MON failure while CephFS is mounted. I stopped 
one of my MON server but precisely the one used for mounting CephFS. Oh 
yes.. I forgot to tell you I used only one monitor to mount Ceph... And 
the result was really unexpected, after I stopped the monitor the CephFS
 didn't crashed and stayed alive :). There is some <em>magic</em> 
performed under the hood by Ceph. I don't really know how but Ceph and 
monitors are clever enough to figure out MON failure and re-initiate a 
connection to an another monitor and thus keep the the mounting 
filesystem alive.</p>
<p>Check this:</p>

<PRE>
client:~$ mount | grep ceph
client:~$ sudo mount -t ceph 172.17.1.7:6789:/ /mnt -vv -o \
  name=admin,secret=AQDVGc5P0LXzJhAA5C019tbdrgypFNXUpG2cqQ==
client:~$ mount grep ceph
172.17.1.7:6789:/ on /mnt type ceph (rw,name=admin,secret=AQDVGc5P0LXzJhAA5C019tbdrgypFNXUpG2cqQ==)
client:~$ ls /mnt/
client:~$ touch /mnt/mon-ok
client:~$ ls /mnt/
mon-ok
client:~$ sudo netstat -plantu | grep EST | grep 6789
tcp        0      0 172.17.1.2:60462        172.17.1.7:6789         ESTABLISHED -
server6:~$ sudo service ceph stop mon
=== mon.2 ===
Stopping Ceph mon.2 on server6...kill 532...done
client:~$ touch /mnt/mon-3-down
client:~$ sudo netstat -plantu | grep EST | grep 6789
tcp        0      0 172.17.1.2:60462        172.17.1.5:6789         ESTABLISHED -
server6:~$ sudo service ceph start mon
=== mon.2 ===
Starting Ceph mon.2 on server6...
starting mon.2 rank 2 at 172.17.1.7:6789/0 mon_data /srv/ceph/mon2 fsid caf6e927-e87e-4295-ab01-3799d6e24be1
server4:~$ sudo service ceph stop mon
=== mon.1 ===
Stopping Ceph mon.1 on server4...kill 4049...done
client:~$ touch /mnt/mon-2-down
client:~$ sudo netstat -plantu | grep EST | grep 6789
tcp        0      0 172.17.1.2:60462        172.17.1.4:6789         ESTABLISHED -
client:~$ touch /mnt/mon-2-down
client:~$ ls /mnt/
mon-ok mon-3-down mon-2-down
</PRE>

<p>Impressive!</p>
<h3>III. Openstack integration</h3>
<h4>III.1. RDB and nova-volume</h4>
<p>Before starting, here is my setup, I voluntary installed nova-volume on a node of 
my Ceph cluster:</p>

<pre>
<code>
   --- - ceph-node-01
      |
       - nova-volume
   --- - ceph-node-02
   --- - ceph-node-03
</code>
</pre>

<p>According to the OpenStack documentation on RBD I just added those lines in 
<code>nova.conf</code>:</p>

<pre>
<code>
--volume_driver=nova.volume.driver.RBDDriver
--rbd_pool=nova
</code>
</pre>

<p>By default, the RBD pool named <code>rbd</code> will be use by OpenStack if 
nothing is specified. I prefered to use <code>nova</code> as a pool, so I created it:</p>

<PRE>
 $ rados lspools
data
metadata
rbd
 $ rados mkpool nova
 $ rados lspools
data
metadata
rbd
nova
 $ rbd --pool nova ls
volume-0000000c

 $ rbd --pool nova info volume-0000000c
rbd image 'volume-0000000c':
  size 1024 MB in 256 objects
  order 22 (4096 KB objects)
  block_name_prefix: rb.0.0
  parent:  (pool -1)
</PRE>

<p>Restart your nova-volume:</p>

<PRE>
 $ sudo service nova-volume restart
</PRE>

<p>Try to create a volume, you shouldn't have any problem :)</p>

<PRE>
 $ nova volume-create --display_name=rbd-vol 1
</PRE>

<p>Check this via:</p>

<PRE>
 $ nova volume-list
+----+-----------+--------------+------+-------------+-------------+
| ID |   Status  | Display Name | Size | Volume Type | Attached to |
+----+-----------+--------------+------+-------------+-------------+
| 51 | available | rbd-vol      | 1    | None        |             |
+----+-----------+--------------+------+-------------+-------------+
</PRE>

<p>Check in RBD:</p>

<PRE>
 $ rbd --pool nova ls
volume-00000033

 $ rbd --pool nova info volume-00000033
rbd image 'volume-00000033':
  size 1024 MB in 256 objects
  order 22 (4096 KB objects)
  block_name_prefix: rb.0.3
  parent:  (pool -1)
</PRE>

<p>Everything looks great, but wait.. can I attach it to an instance?</p>
<p>Since we are using cephx authentication, nova and libvirt require a couple 
more steps.</p>
<p>For security and clarity purpose you may want to create a new user and give 
it access to your Ceph cluster with fine permissions. Let's say that you want 
to use a user called <code>nova</code>, each connection to your MON server will 
be initiate as <code>client.nova</code> instead of <code>client.admin</code>.
This behavior is define by the <code>rados_create</code> function which create a 
handle for communicating with your RADOS cluster. Ceph environment variables are 
read when this is called, so if <code>$CEPH_ARGS</code> specifies everything you 
need to connect, no further configuration is necessary. The trick is to add the 
following lines at the beginning of 
the <code>/usr/lib/python2.7/dist-packages/nova/volume/driver.py</code> file:</p>

<PRE>
# use client.nova instead of nova.admin
import os
os.environ["CEPH_ARGS"] = "--id nova"
</PRE>

<p>Adding the variable via the init script of nova-volume should also 
work, it's up to you. The nova user needs this environment variable.</p>
<p>Here I assume that you use <code>client.admin</code>, if you use 
<code>client.nova</code> change every values called <code>admin</code> to 
<code>nova</code>. Now we can start to configure the secret in libvirt, create 
a file <code>secret.xml</code> and add this content:</p>

<PRE>
&lt;secret ephemeral='no' private='no'>
   &lt;usage type='ceph'>
     &lt;name>client.admin secret&lt;/name>
   &lt;/usage>
&lt;/secret>
</PRE>

<p>Import it into virsh:</p>

<PRE>
 $ sudo virsh secret-define --file secret.xml
Secret 83a0e970-a18b-5490-6fce-642f9052f976 created
</PRE>

<p>Virsh tells you the UUID of the secret, which is how you reference it for other 
libvirt commands. Now set this value with the <code>client.admin</code> key:</p>

<PRE>
 $ sudo virsh secret-set-value --secret 83a0e970-a18b-5490-6fce-642f9052f976 --base64 AQDVGc5P0LXzJhAA5C019tbdrgypFNXUpG2cqQ==
Secret value set
</PRE>

<p>At this point you should be able to attach a disk manually with virsh using 
this <code>disk.xml</code> file. I used the RBD volume previously created:</p>

<PRE>
&lt;disk type='network'>
  &lt;driver name="qemu" type="raw"/>
  &lt;source protocol="rbd" name="nova/volume-00000033">
    &lt;host name='172.17.1.4' port='6789'/>
    &lt;host name='172.17.1.5' port='6789'/>
    &lt;host name='172.17.1.7' port='6789'/>
  &lt;/source>
  &lt;target dev="vdb" bus="virtio"/>
  &lt;auth username='admin'>
    &lt;secret type='ceph' uuid='83a0e970-a18b-5490-6fce-642f9052f976'/>
  &lt;/auth>
&lt;/disk>
</PRE>


<p>Some explanations about this file:</p>

<ul>
<li><code>name</code> argument in <code>&lt;source&gt;</code> corresponds to the 
    pool and the volume: <code>name="your-pool/your-volume"</code>.</li>
<li>This line <code>&lt;host name='172.17.1.4' port='6789'/&gt;</code> points to a 
    Ceph monitor server.</li>
</ul>

<p>The xml syntax is documented on the <a href="http://libvirt.org/formatdomain.html#elementsDisks">libvirt website</a>.</p>
<p>Login to your compute node where the instance is running and check 
the id of the running instance. If you don't know where the instance is 
running launch the following commands:</p>

<PRE>
 $ nova list
+--------------------------------------+-------------------+--------+---------------------+
|                  ID                  | Name              | Status |       Networks      |
+--------------------------------------+-------------------+--------+---------------------+
| e1457eea-ef67-4df3-8ba4-245d104d2b11 | instance-over-rbd | ACTIVE | vlan1=192.168.22.36 |
+--------------------------------------+-------------------+--------+---------------------+

 $ nova show e1457eea-ef67-4df3-8ba4-245d104d2b11
+-------------------------------------+----------------------------------------------------------+
|               Property              |                          Value                           |
+-------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                   |
| OS-EXT-SRV-ATTR:host                | server-02                                                |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                                     |
| OS-EXT-SRV-ATTR:instance_name       | instance-000000d6                                        |
| OS-EXT-STS:power_state              | 1                                                        |
| OS-EXT-STS:task_state               | None                                                     |
| OS-EXT-STS:vm_state                 | active                                                   |
| accessIPv4                          |                                                          |
| accessIPv6                          |                                                          |
| config_drive                        |                                                          |
| created                             | 2012-06-07T12:25:48Z                                     |
| flavor                              | m1.tiny                                                  |
| hostId                              | 30dec431592ca96c90bb4990d0df235f4face63907a7fc2ecdcb36d3 |
| id                                  | e1457eea-ef67-4df3-8ba4-245d104d2b11                     |
| image                               | precise-ceph                                             |
| key_name                            | seb                                                      |
| metadata                            | {}                                                       |
| name                                | instance-over-rbd                                        |
| progress                            | 0                                                        |
| status                              | ACTIVE                                                   |
| tenant_id                           | d1f5d27ccf594cdbb034c8a4123494e9                         |
| updated                             | 2012-06-07T13:06:43Z                                     |
| user_id                             | 557273155f8243bca38f77dcdca82ff6                         |
| vlan1 network                       | 192.168.22.36                                            |
+-------------------------------------+----------------------------------------------------------+
</PRE>

<p>As you can see my instance is running on the <code>server-02</code>, pick up the 
id of your instance here <code>instance-000000d6</code> in virsh. Attach it manually 
with virsh:</p>

<PRE>
server-02:~$ sudo virsh attach-device instance-000000d6 rbd.xml
Device attached successfully
</PRE>

<p>Now check inside your instance, for this use your credential and log into it 
via ssh. You will see a new device called <code>vdb</code>:</p>

<PRE>
server-02:~$ ssh -i seb.pem ubuntu@192.168.22.36
ubuntu@instance-over-rbd:~$ sudo fdisk -l

Disk /dev/vda: 2147 MB, 2147483648 bytes
255 heads, 63 sectors/track, 261 cylinders, total 4194304 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *       16065     4192964     2088450   83  Linux

Disk /dev/vdb: 1073 MB, 1073741824 bytes
16 heads, 63 sectors/track, 2080 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

Disk /dev/vdb doesn't contain a valid partition table
</PRE>

<p>Now you are ready to use it:</p>

<PRE>
ubuntu@instance-over-rbd:~$ sudo mkfs.ext4 /dev/vdb
mke2fs 1.42 (29-Nov-2011)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=0 blocks, Stripe width=0 blocks
65536 inodes, 262144 blocks
13107 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=268435456
8 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks:
  32768, 98304, 163840, 229376

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done

ubuntu@instance-over-rbd:~$ sudo mount /dev/vdb /mnt

ubuntu@instance-over-rbd:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda1       2.0G  668M  1.3G  35% /
udev            242M   12K  242M   1% /dev
tmpfs            99M  212K   99M   1% /run
none            5.0M     0  5.0M   0% /run/lock
none            246M     0  246M   0% /run/shm
/dev/vdb       1022M   47M  924M   5% /mnt

ubuntu@instance-over-rbd:~$ sudo touch /mnt/test

ubuntu@instance-over-rbd:~$ ls /mnt/
lost+found  test
</PRE>

<p>Last but not least, edit your nova.conf on each nova-compute server 
with the authentication value. It's working without those options, since
 we added them manually to libvirt, but I think it good to tell them to 
nova. You will be able to attach a volume to an instance from the nova 
cli with <code>nova volume-attach</code> command and from the dashboard as well :D</p>

<pre>
<code>
--rbd_user=admin
--rbd_secret_uuid=83a0e970-a18b-5490-6fce-642f9052f976
</code>
</pre>

<p>Here we go!</p>

<PRE>
 $ nova volume-create --display_name=nova-rbd-vol 1

 $ nova volume-list
+----+-----------+--------------+------+-------------+--------------------------------------+
| ID |   Status  | Display Name | Size | Volume Type |             Attached to              |
+----+-----------+--------------+------+-------------+--------------------------------------+
| 51 | available | rbd-vol      | 1    | None        |                                      |
| 57 | available | nova-rbd-vol | 1    | None        |                                      |
+----+-----------+--------------+------+-------------+--------------------------------------+

 $ nova volume-attach e1457eea-ef67-4df3-8ba4-245d104d2b11 57 /dev/vdd

 $ nova volume-list
+----+-----------+--------------+------+-------------+--------------------------------------+
| ID |   Status  | Display Name | Size | Volume Type |             Attached to              |
+----+-----------+--------------+------+-------------+--------------------------------------+
| 51 | available | rbd-vol      | 1    | None        |                                      |
| 57 | in-use    | nova-rbd-vol | 1    | None        | e1457eea-ef67-4df3-8ba4-245d104d2b11 |
+----+-----------+--------------+------+-------------+--------------------------------------+
</PRE>

<p>The first disk is marked as <code>available</code> simply because it has been attached manually with virsh and not with nova. Have a look inside your virtual machine :)</p>
<p>Detach the manually attached disk:</p>

<PRE>
 $ sudo virsh detach-device instance-000000d6 rbd.xml
Device detached successfully
</PRE>

<p><strong>/!\ Important note, the secret.xml needs to be added on each 
nova-compute, more precisely to libvirt. Keep the first secret (uuid) 
and put it into your <code>secret.xml</code>. The file below becomes your new <code>secret.xml</code> reference file.</strong></p>

<PRE>
&lt;secret ephemeral='no' private='no'>
   &lt;uuid>83a0e970-a18b-5490-6fce-642f9052f976&lt;/uuid>
   &lt;usage type='ceph'>
     &lt;name>client.admin secret&lt;/name>
   &lt;/usage>
&lt;/secret>
</PRE>

<p>Attaching error found:</p>

<pre>
<code>
error : qemuMonitorJSONCheckError:318 : internal error unable to execute QEMU command 'device_add': Device 'virtio-blk-pci' could not be initialized 
error : qemuMonitorJSONCheckError:318 : internal error unable to execute QEMU command 'device_add': Duplicate ID 'virtio-disk2' for device 
</code>
</pre>

<p>The first one occured when I tried to mount a volume with <code>/dev/vdb</code> as device name and the second occured with <code>/dev/vdc</code>. Solved by using a different device name than <code>/dev/vdc/</code>,
 I think libvirt remembers 'somewhere' and 'somehow' that a device was 
previously attached (the manually one). I didn't really investigate 
since it can be simply solved.</p>

<p><strong>EDIT: 11/07/2012</strong></p>
<p>Some people reported tp me a common issue. There were unable to 
attach a RBD device with nova, but with libvirt it's fine. If you have 
difficulties to make it working, you will probably need to update the 
libvirt AppArmor profile. If you check your <code>/var/log/libvirt/qemu/your_instance_id.log</code>, you should see:</p>

<pre>
<code>
unable to find any monitors in conf. please specify monitors via -m monaddr or -c ceph.conf
</code>
</pre>

<p>And if you dive into the debug mode:</p>

<pre>
<code>debug : virJSONValueFromString:914 : string={"return": "error connecting\r\ncould not \
  open disk image rbd:nova/volume-00000050: No such file or directory\r\n", "id": "libvirt-12"}
</code>
</pre>

<p>And of course it's log in AppArmor and it's pretty explicit:</p>

<PRE>
 $ sudo grep -i denied /var/log/kern.log
server-01 kernel: [28874.202700] type=1400 audit(1341957073.795:51): apparmor="DENIED" 
  operation="open" parent=1 profile="libvirt-bd261aa7-728b-4edb-bd18-2ae2370b6549" 
  name="/etc/ceph/ceph.conf" pid=5833 comm="kvm" requested_mask="r" denied_mask="r" 
  fsuid=108 ouid=0
</PRE>

<p>Now edit the libvirt AppArmor profile, you need to adjust access controls for all 
VMs, new or existing:</p>

<PRE>
 $ sudo echo "/etc/ceph/** r," | sudo tee -a /etc/apparmor.d/abstractions/libvirt-qemu
 $ sudo service libvirt-bin restart
 $ sudo service apparmor reload
</PRE>

<p>That's all, after this libvirt/qemu will be able to read your ceph.conf and your 
keyring (if you use cephx) ;-).</p>

<h4>III.2. RBD and Glance</h4>
<h5>III.2.1. RBD as Glance storage backend</h5>
<p>I followed the official instructions from <a href="http://glance.openstack.org/configuring.html">OpenStack documentation</a>.
 I recommend to follow the upstream package from Ceph since the Ubuntu 
repo doesn't provide a valid version. This issue has been recently 
reported by Florian Haas in the OpenStack and Ceph mailing list however 
the bug has already been tracked <a href="https://bugs.launchpad.net/ubuntu/+source/ceph/+bug/981130">here</a>.
 It has been uploaded to precise-proposed for SRU review and waiting for
 approval, this shouldn't be too long. Be sure to add the Ceph repo (<code>deb http://ceph.com/debian/ precise main</code>) on your Glance server (as I did earlier).</p>

<PRE>
 $ sudo apt-get install python-ceph
</PRE>

<p>Modify your <code>glance-api.conf</code> like so:</p>

<pre>
<code>
# Set the rbd storage
default_store = rbd

# ============ RBD Store Options =============================

# Ceph configuration file path
# If using cephx authentication, this file should
# include a reference to the right keyring
# in a client.&lt;USER&gt; section
rbd_store_ceph_conf = /etc/ceph/ceph.conf

# RADOS user to authenticate as (only applicable if using cephx)
rbd_store_user = glance

# RADOS pool in which images are stored
rbd_store_pool = images

# Images will be chunked into objects of this size (in megabytes).
# For best performance, this should be a power of two
rbd_store_chunk_size = 8
</code>
</pre>

<p>According to the glance configuration, I created a new pool and a new user for RADOS:</p>

<PRE>
 $ rados mkpool images
successfully created pool images

 $ ceph-authtool --create-keyring /etc/glance/rbd.keyring
creating rbd.keyring

 $ ceph-authtool --gen-key --name client.glance --cap mon 'allow r' --cap osd 'allow rwx pool=images' /etc/glance/rbd.keyring

 $ ceph auth add client.glance -i /etc/glance/rbd.keyring
2012-05-24 10:45:58.101925 7f7097c31780 -1 read 122 bytes from /etc/glance/rbd.keyring
added key for client.glance

 $ sudo chown glance:glance /etc/glance/rbd.keyring
</PRE>

<p>After this you should see a new key in ceph:</p>

<PRE>
 $ ceph auth list
installed auth entries:
mon.
  key: AQDVGc5PaLVfKBAAqWFONvImdw7WSu4Sf/e4qg==
mds.0
  key: AQDPGc5PGGXXNxAAoMr9ebDaCwhWo+xbv7cm7A==
  caps: [mds] allow
  caps: [mon] allow rwx
  caps: [osd] allow *
mds.1
  key: AQC6Gc5PGK4cJxAAxRnNC0rRNGPqpJd3lNYWNA==
  caps: [mds] allow
  caps: [mon] allow rwx
  caps: [osd] allow *
mds.2
  key: AQDUGc5PWBRiHRAAUMp2s78p1C31Q0D8MjZS+Q==
  caps: [mds] allow
  caps: [mon] allow rwx
  caps: [osd] allow *
osd.0
  key: AQDJGc5PiGvTCxAAlV4WvTTeGgI2SpR7Vl2V2g==
  caps: [mon] allow rwx
  caps: [osd] allow *
osd.1
  key: AQC0Gc5PoDLwGRAAjVvMaLhklPfzSfN1K91xOA==
  caps: [mon] allow rwx
  caps: [osd] allow *
osd.2
  key: AQDOGc5PgDhwLBAAxuwS9w5d3nlVsm6ACMZJ2g==
  caps: [mon] allow rwx
  caps: [osd] allow *
client.admin
  key: AQDVGc5P0LXzJhAA5C019tbdrgypFNXUpG2cqQ==
  caps: [mds] allow
  caps: [mon] allow *
  caps: [osd] allow *
client.glance
  key: AQDeJc5PwDqpCxAAdggTbAVxTDxGLqjTV5pJdg==
  caps: [mon] allow r
  caps: [osd] allow rwx pool=images
</PRE>

<p>Now restart your glance server:</p>

<PRE>
 $ sudo service glance-api restart && sudo service glance-registry restart
</PRE>

<p>Before uploading check your images pools:</p>

<PRE>
 $ rados --pool=images ls
rbd_directory
rbd_info
</PRE>

<p>Try to upload a new image.</p>

<PRE>
 $ wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img
 $ glance add name="precise-ceph" is_public=True disk_format=qcow2 container_format=ovf architecture=x86_64 < precise-server-cloudimg-amd64-disk1.img
Uploading image 'precise-ceph'
======================================================================================================[100%] 26.2M/s, ETA  0h  0m  0s
Added new image with ID: 70685ad4-b970-49b7-8bde-83e58b255d95
</PRE>

<p>Check in glance:</p>

<PRE>
 $ glance index
ID                                   Name                           Disk Format          Container Format     Size
------------------------------------ ------------------------------ -------------------- -------------------- --------------
60beab84-81a7-46d1-bb4a-19947937dfe3 precise-ceph                   qcow2                ovf                       227213312
</PRE>

<p>Recheck your images pool, oh! objects :D</p>

<PRE>
 $ rados --pool=images ls
rb.0.2.000000000001
rb.0.2.000000000004
rb.0.2.000000000008
rb.0.2.000000000006
rb.0.2.00000000000d
rb.0.2.00000000000e
rb.0.2.000000000011
rb.0.2.00000000000b
rb.0.2.000000000016
rb.0.2.000000000017
rb.0.2.000000000010
rb.0.2.000000000018
rb.0.2.000000000015
rb.0.2.000000000019
rb.0.2.00000000000c
rb.0.2.00000000001a
rb.0.2.00000000000f
rb.0.2.00000000001b
rb.0.2.000000000000
rb.0.2.000000000003
rb.0.2.000000000012
rb.0.2.00000000000a
rb.0.2.000000000013
60beab84-81a7-46d1-bb4a-19947937dfe3.rbd
rbd_directory
rb.0.2.000000000009
rb.0.2.000000000007
rb.0.2.000000000005
rb.0.2.000000000014
rbd_info
rb.0.2.000000000002
</PRE>

<p>Size of the pool:</p>

<PRE>
 $ du precise-server-cloudimg-amd64.img
221888    precise-server-cloudimg-amd64.img

 $ rados --pool=images df
pool name       category                 KB      objects       clones     degraded      unfound           rd        rd KB           wr        wr KB
images          -                    221889           31            0            0           0           11            9          326      1333327
  total used        24569260         2267
  total avail      259342756
  total space      298878748
</PRE>

<p>Check the image in the glance database:</p>

<PRE>
mysql> use glance;
mysql> select * from images where status='active' \G;
*************************** 1. row ***************************
              id: cc7167d6-6dbe-4a2b-8609-b599a48ebbb6
            name: precise-cephAAA
            size: 227213312
          status: active
       is_public: 1
        location: rbd://cc7167d6-6dbe-4a2b-8609-b599a48ebbb6
      created_at: 2012-06-04 15:29:22
      updated_at: 2012-06-04 15:29:31
      deleted_at: NULL
         deleted: 0
     disk_format: qcow2
container_format: ovf
        checksum: fa7325f35ab884c6598154dcd4548063
           owner: d1f5d27ccf594cdbb034c8a4123494e9
        min_disk: 0
         min_ram: 0
       protected: 0
</PRE>

<p>As you can see, it's stored in RBD: <code>rbd://cc7167d6-6dbe-4a2b-8609-b599a48ebbb6</code></p>
<p>From now, you should be able to launch new instance, Glance will retrieve image from the RBD pool.</p>

<h5>III.2.2. Instance snapshot to RBD</h5>
<p>Testing the snapshots:</p>

<pre>
<code>
instance ad0e6a24-9648-406f-b86d-6312ea905888: snapshotting 
sudo nova-rootwrap qemu-img snapshot -c 56642cf3d09b49a7aa400b6bc07494b9 \
  /var/lib/nova/instances/instance-00000097/disk 
qemu-img convert -f qcow2 -O qcow2 -s 56642cf3d09b49a7aa400b6bc07494b9 \
  /var/lib/nova/instances/instance-00000097/disk /tmp/tmpt7EriB/56642cf3d09b49a7aa400b6bc07494b9
sudo nova-rootwrap qemu-img snapshot -d 56642cf3d09b49a7aa400b6bc07494b9 \
  /var/lib/nova/instances/instance-00000097/disk
</code>
</pre>

<p>Let's describe the process:</p>
<ol>
<li>The first command initiates and create the snapshot with name 
<code>56642cf3d09b49a7aa400b6bc07494b9</code> from the image disk of the instance 
located here <code>/var/lib/nova/instances/instance-00000097/disk</code>.</li>
<li>The second command will convert the image from qcow2 to qcow2 and 
store the backup into Glance thus in RBD. Here the image is stored as 
<code>qcow2</code> format, this is not really what we want! I want to store an RBD 
(format) image.</li>
<li>The third command deletes the local file of the snapshot, no longer 
needed since the image has been stored in the Glance backend.</li>
</ol>
<p>When you attempt to perform a snapshot of an instance from the dashboard or via the <code>nova image-create</code> command, nova executes a local copy of changes in a qcow2 file, however this file will be stored in Glance.</p>
<p>If you want to run a RBD snapshot through OpenStack, you need to take
 a volume snapshot. These functionnality is not exposed in the dashboard
 yet.</p>
<p>Snapshot a RBD volume:</p>

<pre>
<code>
snapshot snapshot-00000004: creating 
snapshot snapshot-00000004: creating from (pid=18829) create_snapshot
rbd --pool nova snap create --snap snapshot-00000004 volume-00000042
snapshot snapshot-00000004: created successfully from (pid=18829) create_snapshot 
</code>
</pre>

<p>Verify:</p>

<PRE>
 $ rbd --pool=nova snap ls volume-00000042
2 snapshot-00000004   1073741824
</PRE>

<p>Full RBD managment?</p>

<PRE>
 $ qemu-img info precise-server-cloudimg-amd64.img
image: precise-server-cloudimg-amd64.img
file format: qcow2
virtual size: 2.0G (2147483648 bytes)
disk size: 217M
cluster_size: 65536

 $ sudo qemu-img convert -f qcow2 -O rbd precise-server-cloudimg-amd64.img rbd:images/glance

 $ qemu-img info rbd:nova/ceph-img-cli
image: rbd:nova/ceph-img-cli
file format: raw
virtual size: 2.0G (2147483648 bytes)
disk size: unavailable
</PRE>

<p>There is a surprising value here, why does the image appear as <code>raw</code> format? And why does the file size become unavailable? For those of you, you want to go further with Qemu-RBD snapshot, <a href="http://ceph.com/w/index.php?title=QEMU-RBD#Snapshotting">see the documentation from Ceph</a></p>

<h4>III.3. Does the dream come true?</h4>
<p>Boot from a RBD image?
I uploaded a new image in the glance RBD backend and try to boot with 
this image and it works. Glance is able to retrieve images over the RBD 
backend configured. You will usually see this log message:</p>

<pre>
<code>
INFO nova.virt.libvirt.connection [-] [instance: ce230d11-ddf8-4298-a7d9-40ae8690ff11] Instance spawned successfully. 
</code>
</pre>

<h4>III.4. Boot from a volume</h4>
<p>Booting from a volume will require specifying a dummy image id, as shown in these scripts:</p>
<p><a href="https://github.com/ceph/ceph-openstack-tools/blob/master/start-on-rbd.sh">start-on-rbd on Github</a></p>

<PRE>
start-rbd
#!/bin/bash
set -e

DIR=`dirname $0`

if [ ! -f $DIR/debian.img ]; then
        echo "Downloading debian image..."
        wget http://ceph.com/qa/debian.img -O $DIR/debian.img
fi
touch $DIR/dummy_img
glance add name="dummy_raw_img" is_public=True disk_format=rawi container_format=ovf architecture=x86_64 < dummy_img

echo "Waiting for image to become available..."
while true; do
        if ( timeout 5 nova image-list | egrep -q 'dummy_raw_img|ACTIVE' ) then
                break
        fi
        sleep 2
done

echo "Creating volume..."
nova volume-create --display_name=dummy 1
echo "Waiting for volume to be available..."
while true; do
        if ( nova volume-list | egrep -q 'dummy|available' ) then
                break
        fi
        sleep 2
done

echo "Replacing blank image with real one..."
# last created volume id, assuming pool nova
DUMMY_VOLUME_ID=$(rbd --pool=nova ls | sed -n '$p')
rbd -p nova rm $VOLUME_ID
rbd -p nova import $DIR/debian.img $DUMMY_VOLUME_ID
echo "Requesting an instance..."
$DIR/boot-from-volume
echo "Waiting for instance to start..."
while true; do
        if ( nova list | egrep -q "boot-from-rbd|ACTIVE" ) then
                break
        fi
        sleep 2
done
</PRE>

<p><a href="https://github.com/ceph/ceph-openstack-tools/blob/master/boot-from-volume">boot-from-volume on Github</a></p>

<span>Boot-from-volume</span>
<PRE>
#!/usr/bin/env python

import argparse
import httplib2
import json
import os
def main():
    http = httplib2.Http()
    parser = argparse.ArgumentParser(description='Boot an OpenStack instance from RBD')
    parser.add_argument(
        '--endpoint',
        help='the Nova API endpoint (http://IP:port/vX.Y/)',
        default=os.getenv("http://172.17.1.6:8774/v2/"),
        )
    parser.add_argument(
        '--image-id',
        help="The image ID Nova will pretend to boot from (ie, 1 -- not ami-0000001)",
        default=4,
        )
    parser.add_argument(
        '--volume-id',
        help='The RBD volume ID (ie, 1 -- not volume-0000001)',
        default=1,
        )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        default=False,
        help='be more verbose',
        )
    args = parser.parse_args()

    headers = {
        'Content-Type': 'application/json',
        'x-auth-project-id': 'admin',
        'x-auth-token': 'admin:admin',
        'Accept': 'application/json',
        }
    req = {
        'server':
            {
            'min_count': 1,
            'flavorRef': 1,
            'name': 'test1',
            'imageRef': args.image_id,
            'max_count': 1,
            'block_device_mapping': [{
                    'virtual': 'root',
                    'device_name': '/dev/vda',
                    'volume_id': args.volume_id,
                    'delete_on_termination': False,
                    }]
            }}

    resp, body = http.request(
        '{endpoint}/volumes/os-volumes_boot'.format(endpoint=args.endpoint),
        method='POST',
        headers=headers,
        body=json.dumps(req),
        )
    if resp.status == 200:
        print "Instance scheduled successfully."
        if args.verbose:
            print json.dumps(json.loads(body), indent=4, sort_keys=True)
    else:
        print "Failed to create an instance: response status", resp.status
        print json.dumps(json.loads(body), indent=4, sort_keys=True)

if __name__ == '__main__':
    main()
</PRE>

<p>Both are a little bit deprecated so I re-wrote some parts, it's not 
that demanding. I barely spent much time on it, so there's still work to
 be done. For example, I don't use euca API, so I simply re-wrote 
according to nova-api.</p>
<p>Josh Durgin from Inktank said the following:</p>
<blockquote><p>What's missing is that OpenStack doesn't yet have the 
ability to initialize a volume from an image. You have to put an image 
on one yourself before you can boot from it currently. This should be 
fixed in the next version of OpenStack. Booting off of RBD is nice 
because you can do live migration, although I haven't tested that with 
OpenStack, just with libvirt. For Folsom, we hope to have copy-on-write 
cloning of images as well, so you can store images in RBD with glance, 
and provision instances booting off cloned RBD volumes in very little 
time.</p></blockquote>
<p><a href="https://blueprints.launchpad.net/nova/+spec/auto-create-boot-volumes">It's already in the Folsom's roadmap</a></p>
<p>I quickly tried this manipulation, but without success:</p>

<PRE>
 $ nova volume-create --display_name=dummy 1
 $ nova volume-list
+----+-----------+--------------+------+-------------+--------------------------------------+
| ID |   Status  | Display Name | Size | Volume Type |             Attached to              |
+----+-----------+--------------+------+-------------+--------------------------------------+
| 69 | available | dummy        | 2    | None        |                                      |
+----+-----------+--------------+------+-------------+--------------------------------------+
 $ rbd -p nova ls
volume-00000045
 $ rbd import debian.img volume-00000045
Importing image: 13% complete...2012-06-08 13:45:34.562112 7fbb19835700  0 client.4355.objecter  pinging osd that serves lingering tid 1 (osd.1)
Importing image: 27% complete...2012-06-08 13:45:39.563358 7fbb19835700  0 client.4355.objecter  pinging osd that serves lingering tid 1 (osd.1)
Importing image: 41% complete...2012-06-08 13:45:44.563607 7fbb19835700  0 client.4355.objecter  pinging osd that serves lingering tid 1 (osd.1)
Importing image: 55% complete...2012-06-08 13:45:49.564244 7fbb19835700  0 client.4355.objecter  pinging osd that serves lingering tid 1 (osd.1)
Importing image: 69% complete...2012-06-08 13:45:54.565737 7fbb19835700  0 client.4355.objecter  pinging osd that serves lingering tid 1 (osd.1)
Importing image: 83% complete...2012-06-08 13:45:59.565893 7fbb19835700  0 client.4355.objecter  pinging osd that serves lingering tid 1 (osd.1)
Importing image: 97% complete...2012-06-08 13:46:04.567426 7fbb19835700  0 client.4355.objecter  pinging osd that serves lingering tid 1 (osd.1)
Importing image: 100% complete...done.
 $ nova boot --flavor m1.tiny --image precise-ceph --block_device_mapping vda=69:::0 --security_groups=default boot-from-rbd
</PRE>


<h4>III.5. Live migration with CephFS!</h4>
<p>I was brave enough to also experimented the live migration with the 
Ceph Filesystem. Some of these pre-requites are obvious but just to be 
sure, with the live-migration comes mandatory requirements as:</p>
<ul>
<li>2 compute nodes</li>
<li>a distributed file system, here CephFS</li>
</ul>
<p>For the live-migration configuration I followed the <a href="http://docs.openstack.org/trunk/openstack-compute/admin/content/configuring-live-migrations.html">official OpenStack documentation</a>. The following actions need to be performed <strong>on each compute node</strong>:</p>
<p>Update the libvirt configurations. Modify <code>/etc/libvirt/libvirtd.conf</code>:</p>

<pre>
<code>
listen_tls = 0
listen_tcp = 1
auth_tcp = "none"
</code>
</pre>

<p>Modify <code>/etc/init/libvirt-bin.conf</code> and add the <code>-l</code> option:</p>

<pre>
<code>
 libvirtd_opts=" -d -l"
</code>
</pre>
<p>Restart libvirt. After executing the command, ensure that libvirt is succesfully restarted.</p>

<PRE>
 $ sudo stop libvirt-bin && sudo start libvirt-bin
 $ ps -ef | grep libvirt
</PRE>

<p>Make sure that you see the <code>-l</code> flag in the <code>ps</code> command. You should be able to retrieve the information (passwordless) from an hypervisor to another, to test it simply run:</p>

<PRE>
server-02:/$ sudo virsh --connect qemu+tcp://server-01/system list
Id Name                 State
----------------------------------
   1 instance-000000af    running
   3 instance-000000b5    running
</PRE>

<p>My nova.conf options:</p>

<pre>
<code>
--live_migration_retry_count=30
--live_migration_uri=qemu+tcp://%s/system
--live_migration_bandwidth=0
</code>
</pre>

<p>Mount the nova instance directory with CephFS and assign nova as the owner of the directory:</p>

<PRE>
 $ sudo mount -t ceph 172.17.1.4:6789:/ /var/lib/nova/instances -vv -o name=admin,secret=AQARB71PUCuuAxAAPhlUGzkRdDdjNDJy1w8MQQ==
 $ sudo chown nova:nova /var/lib/nova/instances
</PRE>

<p>Check your nova services:</p>

<PRE>
server-01:~$ sudo nova-manage service l
Binary           Host                                 Zone             Status     State Updated_At
nova-consoleauth server-05                            nova             enabled    :-)   2012-05-29 15:34:15
nova-cert        server-05                            nova             enabled    :-)   2012-05-29 15:34:15
nova-scheduler   server-05                            nova             enabled    :-)   2012-05-29 15:34:14
nova-compute     server-02                            nova             enabled    :-)   2012-05-29 15:34:14
nova-network     server-02                            nova             enabled    :-)   2012-05-29 15:34:18
nova-volume      server-03                            nova             enabled    :-)   2012-05-29 15:34:23
nova-compute     server-01                            nova             enabled    :-)   2012-05-29 15:33:50
nova-network     server-01                            nova             enabled    :-)   2012-05-29 15:33:51

server-01:~$ nova list
+--------------------------------------+---------------+--------+----------------------------------+
|                  ID                  |      Name     | Status |             Networks             |
+--------------------------------------+---------------+--------+----------------------------------+
| 1ff0f8c4-bdc9-48d4-95ea-515f3a2ff6d4 | pouet         | ACTIVE | vlan1=192.168.22.42, 172.17.1.43 |
| 5e7618a1-15df-45e8-86b6-02698e143b92 | boot-from-rbd | ACTIVE | vlan1=192.168.22.36              |
| ce230d11-ddf8-4298-a7d9-40ae8690ff11 | medium-rbd    | ACTIVE | vlan1=192.168.22.39              |
| ea68ee9a-7b0b-48d7-a9ce-a9328077ca9d | test          | ACTIVE | vlan1=192.168.22.41              |
+--------------------------------------+---------------+--------+----------------------------------+

server-01:~$ nova show ce230d11-ddf8-4298-a7d9-40ae8690ff11
+-------------------------------------+----------------------------------------------------------+
|               Property              |                          Value                           |
+-------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                   |
| OS-EXT-SRV-ATTR:host                | server-01                                                |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                                     |
| OS-EXT-SRV-ATTR:instance_name       | instance-000000b5                                        |
| OS-EXT-STS:power_state              | 1                                                        |
| OS-EXT-STS:task_state               | None                                                     |
| OS-EXT-STS:vm_state                 | active                                                   |
| accessIPv4                          |                                                          |
| accessIPv6                          |                                                          |
| config_drive                        |                                                          |
| created                             | 2012-05-29T13:50:45Z                                     |
| flavor                              | m1.medium                                                |
| hostId                              | ec2890ed9e2f998820c4f767b66822c60910a293d0a63723177fff74 |
| id                                  | ce230d11-ddf8-4298-a7d9-40ae8690ff11                     |
| image                               | precise-cephA                                            |
| key_name                            | seb                                                      |
| metadata                            | {}                                                       |
| name                                | medium-rbd                                               |
| progress                            | 0                                                        |
| status                              | ACTIVE                                                   |
| tenant_id                           | d1f5d27ccf594cdbb034c8a4123494e9                         |
| updated                             | 2012-05-29T15:31:27Z                                     |
| user_id                             | 557273155f8243bca38f77dcdca82ff6                         |
| vlan1 network                       | 192.168.22.39                                            |
+-------------------------------------+----------------------------------------------------------+

server-01:~$ sudo virsh list
Id Name                 State
----------------------------------
1 instance-000000af    running
3 instance-000000b5    running
</PRE>

<p>Run the <code>live-migration</code> command in debug mode:</p>

<PRE>
server-01:~$ nova --debug live-migration ce230d11-ddf8-4298-a7d9-40ae8690ff11 server-02
connect: (172.17.1.6, 5000)
send: 'POST /v2.0/tokens HTTP/1.1\r\nHost: 172.17.1.6:5000\r\nContent-Length: 100\r\ncontent-type: 
  application/json\r\naccept-encoding: gzip, deflate\r\naccept: 
  application/json\r\nuser-agent: python-novaclient\r\n\r\n{"auth": 
  {"tenantName": "admin", "passwordCredentials": {"username": "admin", 
  "password": "admin"}}}'
reply: 'HTTP/1.1 200 OK\r\n'
header: Content-Type: application/json
header: Vary: X-Auth-Token
header: Date: Tue, 29 May 2012 15:31:39 GMT
header: Transfer-Encoding: chunked
connect: (172.17.1.6, 8774)
send: u'GET /v2/d1f5d27ccf594cdbb034c8a4123494e9/servers/ce230d11-ddf8-4298-a7d9-40ae8690ff11 HTTP/1.1\r\nHost: 172.17.1.6:8774\r\nx-auth-project-id: 
  admin\r\nx-auth-token: 8758eb02f8f24810a6c8f11c7434f0b1\r\naccept-encoding: 
  gzip, deflate\r\naccept: application/json\r\nuser-agent: 
  python-novaclient\r\n\r\n'
reply: 'HTTP/1.1 200 OK\r\n'
header: X-Compute-Request-Id: req-4043a2da-4ed1-4c2e-a9c5-b73e81bbfe99
header: Content-Type: application/json
header: Content-Length: 1377
header: Date: Tue, 29 May 2012 15:31:39 GMT
send: u'GET /v2/d1f5d27ccf594cdbb034c8a4123494e9/servers/ce230d11-ddf8-4298-a7d9-40ae8690ff11 HTTP/1.1\r\nHost: 172.17.1.6:8774\r\nx-auth-project-id: 
  admin\r\nx-auth-token: 8758eb02f8f24810a6c8f11c7434f0b1\r\naccept-encoding: 
  gzip, deflate\r\naccept: application/json\r\nuser-agent: python-novaclient\r\n\r\n'
reply: 'HTTP/1.1 200 OK\r\n'
header: X-Compute-Request-Id: req-b86ccd91-a0ea-4c0c-9523-0f0f3a0a3a86
header: Content-Type: application/json
header: Content-Length: 1377
header: Date: Tue, 29 May 2012 15:31:39 GMT
send: u'POST /v2/d1f5d27ccf594cdbb034c8a4123494e9/servers/ce230d11-ddf8-4298-a7d9-40ae8690ff11/action HTTP/1.1\r\nHost: 172.17.1.6:8774\r\nContent-Length: 
  92\r\nx-auth-project-id: admin\r\naccept-encoding: gzip, deflate\r\naccept: 
  application/json\r\nx-auth-token: 8758eb02f8f24810a6c8f11c7434f0b1\r\nuser-agent: 
  python-novaclient\r\ncontent-type: application/json\r\n\r\n{"os-migrateLive": 
  {"disk_over_commit": false, "block_migration": false, "host": "server-02"}}'
reply: 'HTTP/1.1 202 Accepted\r\n'
header: Content-Type: text/html; charset=UTF-8
header: Content-Length: 0
header: Date: Tue, 29 May 2012 15:31:52 GMT
</PRE>

<p>Sometimes you can get this message from the nova-scheduler logs:</p>

<pre>
<code>
Casted 'live_migration' to compute 'server-01' from (pid=10963) cast_to_compute_host /usr/lib/python2.7/dist-packages/nova/scheduler/driver.py:80 
</code>
</pre>

<p>And somehow you <strong>must</strong> get something from the logs, so check:</p>

<ul>
  <li>nova-compute logs</li>
  <li>nova-scheduler logs</li>
  <li>libvirt logs</li>
</ul>

<p>The libvirt logs could show those errors:</p>

<pre>
<code>
error : virExecWithHook:328 : Cannot find 'pm-is-supported' in path: No such file or directory
error : virNetClientProgramDispatchError:174 : Unable to read from monitor: Connection reset by peer
</code>
</pre>

<p>The first issue (<code>pm</code>) was solved by installing this package:</p>

<PRE>
 $ sudo apt-get install pm-utils -y
</PRE>

<p>The second one is a little bit more tricky, the only glue I found was to disable the VNC console according to <a href="https://lists.launchpad.net/openstack/msg12389.html">this thread</a>.
Finally check the log and see:</p>

<pre>
<code>
instance: 962c222f-2280-43e9-83be-c27a31f77946] Migrating instance to server-02 finished successfully. 
</code>
</pre>

<p>Sometimes this message doesn't appear, but the live-migration 
successfully performed, the best check is to wait and watch on the 
distant server:</p>

<PRE>
 $ watch sudo virsh list

Every 2.0s: sudo virsh list
Id Name                  State
----------------------------------

Every 2.0s: sudo virsh list
Id Name                  State
----------------------------------
6 instance-000000dc    shut off

Every 2.0s: sudo virsh list
Id Name                  State
----------------------------------
6 instance-000000dc    paused

Every 2.0s: sudo virsh list
Id Name                  State
----------------------------------
6 instance-000000dc    running
</PRE>

<p>During the live migration, you should see those states in virsh:</p>

<ul>
  <li>shut off</li>
  <li>paused</li>
  <li>running</li>
</ul>
 
<p>That's all!
<strong>The downtime for m2.tiny instance was approximatively 3 sec.</strong></p>

<h4>III.6. Virtual instances disk's errors - Solved!</h4>
<p>When I use Ceph to store the <code>/var/lib/nova/instances</code> directory of each nova-compute server I have these I/O errors inside the virtual machines...</p>

<pre>
<code>
Buffer I/O error on device vda1, logical block 593914
Buffer I/O error on device vda1, logical block 593915
Buffer I/O error on device vda1, logical block 593916
EXT4-fs warning (device vda1): ext4_end_bio:251: I/O error writing to inode 31112 (offset 7852032 size 524288 starting block 595925)
JBD2: Detected IO errors while flushing file data on vda1-8
</code>
</pre>

<p>Logs from the kernel during the boot sequence of the instance:</p>

<pre>
<code>server-01 kernel: [  400.354943]  nbd15: p1
server-01 kernel: [  405.710253] EXT4-fs (dm-2): mounted filesystem with ordered data mode. Opts: (null)
server-01 kernel: [  410.400054] block nbd15: NBD_DISCONNECT
server-01 kernel: [  410.400190] block nbd15: Receive control failed (result -32)
server-01 kernel: [  410.400656] block nbd15: queue cleared
</code>
</pre>

<p>This issue appears everytime I launched a new instance. Sometimes 
waiting for the ext4 auto mecanism recovery solve temporary the problem 
but the filesystem still stays unstable. This error is probably due to 
the ext4 filesystem. It happens really often and I don't have any clue 
at the moment maybe a filesystem option or switching from ext4 to XFS 
will do the trick. At the moment I tried several mount options inside 
the VM like <code>nobarrier</code> or <code>noatime</code> but nothing changed. This is what I got when I tried to perform a basic operation like installing a package:</p>

<pre>
<code>
Reading package lists... Error!
E: Unable to synchronize mmap - msync (5: Input/output error)
E: The package lists or status file could not be parsed or opened.
</code>
</pre>

<p>This can be solved by the following commands but it's neither useful nor relevant since this error will occur again and again...</p>

<PRE>
 $ sudo apt-get clean
 $ sudo apt-get update
 $ sudo apt-get install 'your_package'
</PRE>

<p>Filesystem check on each Ceph node:</p>

<PRE>
server6:~$ sudo service ceph stop osd
=== osd.2 ===
Stopping Ceph osd.2 on server6...kill 26140...done
server6:~$ sudo umount /srv/ceph/osd2/
server6:~$ sudo fsck.ext4 -fy /dev/server6/ceph-ext4
e2fsck 1.42 (29-Nov-2011)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
/dev/server6/ceph-ext4: 4567/1310720 files (58.0% non-contiguous), 3370935/5242880 blocks
</PRE>

<p>ext4 check on the second server:</p>

<PRE>
server4:~$ sudo fsck.ext4 -fy /dev/server4/lvol0
e2fsck 1.42 (29-Nov-2011)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
/dev/server4/lvol0: 3686/3276800 files (5.2% non-contiguous), 2935930/13107200 blocks
</PRE>

<p>ext4 check on the third server:</p>

<PRE>
server-003:~$ sudo fsck.ext4 -fy /dev/nova-volumes/lvol0
e2fsck 1.42 (29-Nov-2011)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
/lost+found not found.  Create? yes

Pass 4: Checking reference counts
Pass 5: Checking group summary information

/dev/nova-volumes/lvol0: ***** FILE SYSTEM WAS MODIFIED *****
/dev/nova-volumes/lvol0: 3435/6553600 files (6.7% non-contiguous), 2783459/26214400 blocks
</PRE>

<p>Nothing relevant, everything is properly working.</p>

<p><strong>This issue is unsolved, it's simply related to the fact that 
CephFS is not stable enough. It can't handle this amount of I/O. A 
possible work around, <a href="http://comments.gmane.org/gmane.comp.file-systems.ceph.devel/6557">here</a> and <a href="http://marc.info/?l=ceph-devel&amp;m=133758599712768&amp;w=2">here</a>.
 I don't even thing that using XFS instead of ext4 will change the 
outcome. It seems that this issue also occur with RBD volume, 
<a href="http://tracker.newdream.net/issues/2535">see on the ceph 
tracker</a>.</strong></p>

<p>According to this <a href="http://tracker.newdream.net/issues/2535">reported bug</a>
 (and the mailing list discussion) this issue affects rbd volumes inside
 virtual machine, the workaround here is to active the rbd caching, an 
option should be added inside the xml file while attaching a device.</p>

<pre>
<code>&lt;source protocol='rbd' name='your-pool/your-volume:rbd_cache=true'&gt;
</code>
</pre>

<p>I didn't check this workaround yet, but it seems to be solved by enabling the cache.</p>
<p><strong>UPDATE: 13/06/2012 - I/O ISSUES SOLVED</strong></p>
<p>It seems that Ceph has a lot of difficulties with the direct I/O support, see below:</p>

<PRE>
 $ mount | grep ceph
172.17.1.4:6789,172.17.1.5:6789,172.17.1.7:6789:/ on /mnt type ceph (name=admin,key=client.admin)
 $ dd if=/dev/zero of=/mnt/directio bs=8M count=1 oflag=direct
1+0 records in
1+0 records out
8388608 bytes (8.4 MB) copied, 0.36262 s, 23.1 MB/s
 $ dd if=/dev/zero of=/mnt/directio bs=9M count=1 oflag=direct
dd: writing `/mnt/directio': Bad address
1+0 records in
0+0 records out
0 bytes (0 B) copied, 1.20184 s, 0.0 kB/s
</PRE>

<p>This bug has been tracked on the <a href="http://tracker.newdream.net/issues/2657">Ceph tracker</a></p>
<p>It seems that Ceph doesn't support the creation of blocks superior at
 9M. And? And if you check your libvirt of an instance you will see this
 section:</p>

<PRE>
&lt;disk type='file' device='disk'>
   &lt;driver type='qcow2' cache='none'/>
   &lt;source file='/var/lib/nova/instances/instance-000000f9/disk'/>
   &lt;target dev='vda' bus='virtio'/>
&lt;/disk>
</PRE>

<p>Setting the cache to <code>none</code> means using direct I/O... Note from the libvirt documentation:</p>
<blockquote><p>The optional cache attribute controls the cache 
mechanism, possible values are "default", "none", "writethrough", 
"writeback", "directsync" (like "writethrough", but it bypasses the host
 page cache) and "unsafe" (host may cache all disk io, and sync requests
 from guest are ignored). Since 0.6.0, "directsync" since 0.9.5, 
"unsafe" since 0.9.7</p></blockquote>
<p>Cache parameters explained:</p>
<ul>
<li><strong>none</strong>: uses O_DIRECT I/O that bypasses the filesystem cache on the host</li>
<li><strong>writethrough</strong>: uses O_SYNC I/O that is guaranteed to
 be commited to disk on return to userspace. Only cache read requests 
and immediately write to disk.</li>
<li><strong>writeback</strong>: uses normal buffered I/O that is written
 back later by the operating system. It caches the write requests in 
RAM, which bring high-performance but also increase the data loss 
probability.</li>
</ul>
<p>Actually there is already a function to test if direct I/O are supported:</p>

<PRE>
@staticmethod
    def _supports_direct_io(dirpath):
        testfile = os.path.join(dirpath, ".directio.test")
        hasDirectIO = True
        try:
            f = os.open(testfile, os.O_CREAT | os.O_WRONLY | os.O_DIRECT)
            os.close(f)
            LOG.debug(_("Path '%(path)s' supports direct I/O") %
                      {'path': dirpath})
        except OSError, e:
            if e.errno == errno.EINVAL:
                LOG.debug(_("Path '%(path)s' does not support direct I/O: "
                            "'%(ex)s'") % {'path': dirpath, 'ex': str(e)})
                hasDirectIO = False
            else:
                LOG.error(_("Error on '%(path)s' while checking direct I/O: "
                            "'%(ex)s'") % {'path': dirpath, 'ex': str(e)})
                raise e
        except Exception, e:
            LOG.error(_("Error on '%(path)s' while checking direct I/O: "
                        "'%(ex)s'") % {'path': dirpath, 'ex': str(e)})
            raise e
        finally:
            try:
                os.unlink(testfile)
            except:
                pass

        return hasDirectIO
</PRE>

<p>Somehow it's not detected, mainly because the issue is related to the block size.</p>
<p>If direct I/O are supported it will specified in this file <code>/usr/lib/python2.7/dist-packages/nova/virt/libvirt/connection.py</code>, on line 1036:</p>

<PRE>
@property
def disk_cachemode(self):
    if self._disk_cachemode is None:
        # We prefer 'none' for consistent performance, host crash
        # safety & migration correctness by avoiding host page cache.
        # Some filesystems (eg GlusterFS via FUSE) don't support
        # O_DIRECT though. For those we fallback to 'writethrough'
        # which gives host crash safety, and is safe for migration
        # provided the filesystem is cache coherant (cluster filesystems
        # typically are, but things like NFS are not).
        self._disk_cachemode = "none"
        if not self._supports_direct_io(FLAGS.instances_path):
            self._disk_cachemode = "writethrough"
    return self._disk_cachemode
</PRE>

<p>The first trick was to modify this line:</p>

<pre>
<code>
self._disk_cachemode = "none"
</code>
</pre>

<p>With</p>

<pre>
<code>
self._disk_cachemode = "writethrough"
</code>
</pre>

<p>With this change, every instances will have the libvirt cache option set to <code>writethrough</code> even if the filesystem supports direct I/O.</p>
<p>Fix a corrumpted VM:</p>

<pre>
<code>
FSCKFIX=yes
</code>
</pre>

<p>Reboot the VM :)</p>
<p>Note: <code>writeback</code> is also supported with Ceph, it offers better performance than <code>writethrough</code> but <code>writeback</code> stays the safest way for your data. It depends on your need :)</p>

<h3>IV. Benchmarks</h3>

<p>Thoses benchmarks have been performed under ext4 filesystem and on 15K RPM hard drive disks.</p>

<h4>IV.1. Rados builtin benchmark</h4>

<h5>IV.1.1. Cluster benchmark</h5>

<PRE>
 $ uname -r
3.2.0-24-generic
 $ ceph -v
ceph version 0.47.2 (commit:f5a9404445e2ed5ec2ee828aa53d73d4a002f7a5)
 $ rados -p nova bench 100 write
Maintaining 16 concurrent writes of 4194304 bytes for at least 100 seconds.
  sec Cur ops   started  finished  avg MB/s  cur MB/s  last lat   avg lat
    0       0         0         0         0         0         -         0
    1      16        31        15   59.8134        60  0.988616  0.774045
    2      16        46        30   59.8908        60   1.15953  0.835025
    3      16        63        47   62.5881        68  0.914239  0.836658
    4      16        76        60   59.9416        52   1.23871  0.906893
    5      16        94        78   62.3493        72   0.92557  0.912052
    6      16       113        97   64.6216        76   1.14571  0.914297
    7      16       123       107   61.1052        40   1.08826  0.922949
    8      16       138       122   60.9663        60   0.46168  0.969207
    9      16       145       129   57.3044        28    1.0469  0.989164
   10      16       166       150    59.972        84   1.50591   1.02505
   11      16       186       170   61.7913        80   1.06359   0.99008
   12      16       197       181   60.3086        44   1.45907  0.993509
   13      16       212       196   60.2843        60   1.67142   1.01419
   14      16       218       202   57.6929        24   1.57489   1.03316
   15      16       223       207   55.1804        20  0.259759   1.03948
   16      16       239       223   55.7307        64   1.81071   1.10588
   17      16       253       237   55.7461        56   1.17068   1.10739
   18      16       267       251   55.7598        56   1.15406   1.10697
   19      16       280       264   55.5616        52   1.26379   1.10818
min lat: 0.124888 max lat: 2.50869 avg lat: 1.11042
  sec Cur ops   started  finished  avg MB/s  cur MB/s  last lat   avg lat
   20      16       293       277    55.383        52   1.19662   1.11042
   21      16       304       288   54.8409        44   1.21306   1.11133
   22      16       324       308   55.9839        80  0.633551   1.11404
   23      16       337       321   55.8104        52  0.155063   1.10398
   24      16       350       334   55.6514        52   1.54921    1.1165
   25      16       364       348   55.6651        56   1.26814   1.12392
   26      16       367       351   53.9858        12   1.89539   1.13046
   27      16       384       368   54.5045        68   1.13766   1.15098
   28      16       398       382   54.5576        56   1.46389   1.14698
   29      16       415       399   55.0208        68   1.03303   1.14274
   30      16       431       415   55.3198        64   1.24156   1.14126
   31      16       440       424   54.6965        36   1.19121   1.14321
   32      16       457       441   55.1119        68   1.23561   1.14136
   33      16       469       453   54.8963        48   1.21978   1.14207
   34      16       486       470   55.2814        68    1.2799   1.13989
   35      16       499       483   55.1874        52  0.233549      1.14
   36      16       504       488     54.21        20   1.61804   1.14024
   37      16       513       497   53.7178        36   2.10228   1.16011
   38      16       527       511   53.7776        56   1.37356   1.17257
   39      16       541       525   53.8344        56   1.40289   1.17057
min lat: 0.124888 max lat: 2.5194 avg lat: 1.17259
  sec Cur ops   started  finished  avg MB/s  cur MB/s  last lat   avg lat
   40      16       553       537   53.6883        48   1.24732   1.17259
   41      16       565       549   53.5494        48    1.5267   1.17512
   42      16       578       562   53.5124        52   1.68045   1.17721
   43      16       594       578   53.7561        64  0.279511    1.1751
   44      16       608       592   53.8069        56   1.23636    1.1711
   45      16       619       603   53.5888        44   1.56834   1.17327
   46      16       633       617   53.6411        56   1.24921    1.1744
   47      16       644       628   53.4359        44  0.228269   1.17318
   48      16       654       638   53.1558        40   1.85967   1.18184
   49      16       667       651   53.1321        52   1.11298   1.18894
   50      16       679       663   53.0293        48   1.24697   1.19045
   51      16       691       675   52.9306        48   1.41656   1.19212
   52      16       704       688   52.9125        52   1.24629   1.19305
   53      16       719       703   53.0461        60   1.23783    1.1931
   54      16       740       724   53.6191        84  0.825043   1.18465
   55      16       750       734   53.3714        40   1.12641   1.18158
   56      16       766       750    53.561        64      1.58   1.18356
   57      16       778       762   53.4634        48   1.33114    1.1805
   58      16       779       763   52.6106         4   1.74222   1.18124
   59      16       796       780   52.8713        68   2.13181   1.20095
min lat: 0.124888 max lat: 2.68683 avg lat: 1.20162
  sec Cur ops   started  finished  avg MB/s  cur MB/s  last lat   avg lat
   60      16       805       789     52.59        36   1.36423   1.20162
   61      16       817       801   52.5147        48   1.38829   1.20521
   62      16       830       814   52.5063        52   1.26657   1.20691
   63      16       845       829   52.6251        60   1.17306   1.20415
   64      16       853       837   52.3028        32   1.73082   1.20619
   65      16       864       848    52.175        44   1.99292   1.21222
   66      16       880       864    52.354        64   1.09513   1.21345
   67      16       892       876   52.2889        48   1.17609   1.21056
   68      16       908       892    52.461        64   1.21753   1.21081
   69      16       921       905   52.4542        52   1.07357   1.20978
   70      16       936       920   52.5619        60  0.160182   1.20659
   71      16       952       936   52.7229        64  0.251266    1.2015
   72      16       965       949   52.7128        52   1.48819   1.20271
   73      16       986       970   53.1412        84  0.940281   1.19764
   74      16       994       978   52.8554        32  0.873665   1.19506
   75      16      1000       984   52.4707        24   2.18796   1.20107
   76      16      1012       996   52.4117        48   2.58551   1.21175
   77      16      1029      1013    52.614        68   1.12385   1.20813
   78      16      1042      1026    52.606        52   1.22075   1.20693
   79      16      1056      1040   52.6489        56  0.285843   1.20635
min lat: 0.120974 max lat: 2.68683 avg lat: 1.20498
  sec Cur ops   started  finished  avg MB/s  cur MB/s  last lat   avg lat
   80      16      1067      1051   52.5407        44  0.182956   1.20498
   81      16      1076      1060   52.3365        36   1.74162   1.20995
   82      16      1090      1074   52.3811        56   1.18474   1.21345
   83      16      1103      1087   52.3764        52   1.45589   1.21301
   84      16      1119      1103   52.5146        64   1.20541   1.20995
   85      16      1134      1118   52.6026        60      1.27   1.20745
   86      16      1145      1129   52.5025        44  0.173344    1.2067
   87      16      1162      1146   52.6805        68   1.56221   1.20783
   88      16      1174      1158   52.6273        48   0.12839   1.20479
   89      16      1189      1173     52.71        60   1.27274   1.20651
   90      16      1201      1185   52.6576        48   1.11873   1.20648
   91      16      1211      1195   52.5185        40   1.32622   1.20716
   92      16      1224      1208   52.5128        52   1.49926   1.21086
   93      16      1234      1218   52.3782        40  0.163716   1.21123
   94      16      1251      1235   52.5443        68   1.32683    1.2104
   95      16      1264      1248   52.5385        52   1.01523   1.21017
   96      16      1279      1263   52.6161        60   1.31704   1.20815
   97      16      1294      1278   52.6921        60   1.45825   1.20717
   98      16      1314      1298   52.9707        80  0.281634    1.2014
   99      16      1325      1309     52.88        44   1.45331   1.20097
min lat: 0.120974 max lat: 2.68683 avg lat: 1.20099
  sec Cur ops   started  finished  avg MB/s  cur MB/s  last lat   avg lat
  100      16      1340      1324   52.9511        60   1.43721   1.20099
  101       2      1341      1339   53.0208        60   1.66956   1.20448
Total time run:        101.114344
Total writes made:     1341
Write size:            4194304
Bandwidth (MB/sec):    53.049

Average Latency:       1.20432
Max latency:           2.68683
Min latency:           0.120974
</PRE>

<h5>IV.1.2. OSD Benchmarks</h5>
<p>From a console run:</p>

<PRE>
 $ for i in 0 1 2; do ceph osd tell $i bench; done
ok
ok
ok
</PRE>

<p>Monitor the output from an another terminal:</p>

<PRE>
 $ ceph -w
osd.0 172.17.1.4:6802/22135 495 : [INF] bench: wrote 1024 MB in blocks of 4096 KB in 4.575725 sec at 223 MB/sec
osd.1 172.17.1.5:6801/8713 877 : [INF] bench: wrote 1024 MB in blocks of 4096 KB in 22.559266 sec at 46480 KB/sec
osd.2 172.17.1.7:6802/737 1274 : [INF] bench: wrote 1024 MB in blocks of 4096 KB in 20.011638 sec at 52398 KB/sec
</PRE>

<p>As you can see, I have pretty bad performance on 2 OSDs. Both of them
 will bring down the performance of my whole cluster. (this statment 
will be verified bellow)</p>

<h4>IV.2. Servers benchmarks</h4>
<h5>IV.2.1. server-03</h5>

<PRE>
server-03:~$ for ((i=0 ; 10 -$i ; i++)) ; do dd if=/dev/zero of=pouet bs=1000M count=1; rm pouet; done
1048576000 bytes (1.0 GB) copied, 2.23271 s, 470 MB/s
1048576000 bytes (1.0 GB) copied, 2.12575 s, 493 MB/s
1048576000 bytes (1.0 GB) copied, 2.12901 s, 493 MB/s
1048576000 bytes (1.0 GB) copied, 2.13956 s, 490 MB/s
1048576000 bytes (1.0 GB) copied, 2.14999 s, 488 MB/s
1048576000 bytes (1.0 GB) copied, 2.12281 s, 494 MB/s
1048576000 bytes (1.0 GB) copied, 2.12963 s, 492 MB/s
1048576000 bytes (1.0 GB) copied, 2.13597 s, 491 MB/s
1048576000 bytes (1.0 GB) copied, 2.14659 s, 488 MB/s
1048576000 bytes (1.0 GB) copied, 2.15181 s, 487 MB/s
</PRE>

<p><strong>Average: 488,6 MB/s</strong></p>
<h5>IV.2.2. server-04</h5>

<PRE>
server-04:~$ for ((i=0 ; 10 -$i ; i++)) ; do dd if=/dev/zero of=pouet bs=1000M count=1; rm pouet; done
1048576000 bytes (1.0 GB) copied, 4.676 s, 224 MB/s
1048576000 bytes (1.0 GB) copied, 4.62314 s, 227 MB/s
1048576000 bytes (1.0 GB) copied, 4.93966 s, 212 MB/s
1048576000 bytes (1.0 GB) copied, 10.5936 s, 99.0 MB/s
1048576000 bytes (1.0 GB) copied, 4.94419 s, 212 MB/s
1048576000 bytes (1.0 GB) copied, 4.70893 s, 223 MB/s
1048576000 bytes (1.0 GB) copied, 8.94163 s, 117 MB/s
1048576000 bytes (1.0 GB) copied, 4.79279 s, 219 MB/s
1048576000 bytes (1.0 GB) copied, 8.39481 s, 125 MB/s
1048576000 bytes (1.0 GB) copied, 8.97216 s, 117 MB/s
</PRE>

<p><strong>Average: 154,8 MB/s</strong></p>
<h5>IV.2.3. server-06</h5>

<PRE>
server-06:~$ for ((i=0 ; 10 -$i ; i++)) ; do dd if=/dev/zero of=pouet bs=1000M count=1; rm pouet; done
1048576000 bytes (1.0 GB) copied, 2.35758 s, 445 MB/s
1048576000 bytes (1.0 GB) copied, 2.37689 s, 441 MB/s
1048576000 bytes (1.0 GB) copied, 4.94374 s, 212 MB/s
1048576000 bytes (1.0 GB) copied, 2.55669 s, 410 MB/s
1048576000 bytes (1.0 GB) copied, 6.08993 s, 172 MB/s
1048576000 bytes (1.0 GB) copied, 2.2573 s, 465 MB/s
1048576000 bytes (1.0 GB) copied, 2.29013 s, 458 MB/s
1048576000 bytes (1.0 GB) copied, 5.67836 s, 185 MB/s
1048576000 bytes (1.0 GB) copied, 2.39934 s, 437 MB/s
1048576000 bytes (1.0 GB) copied, 5.87929 s, 178 MB/s
</PRE>

<p><strong>Average: 340,3 MB/s</strong></p>
<h4>IV.3. Bandwidth benchmarks</h4>
<p>Quick bandwidth test between 2 servers:</p>

<PRE>
server-03:~$ time dd if=/dev/zero of=test bs=2000M count=1; time scp test root@server-04:/dev/null;
2097152000 bytes (2.1 GB) copied, 4.46267 s, 470 MB/s

root@server-04's password:
test                                                         100% 2000MB  52.6MB/s   00:47

real  0m49.298s
user  0m43.915s
sys   0m5.172s
</PRE>

<p>It's not really surprising since Ceph showed an average of 53MB/s. I 
clairly have a network bottlenck because all my servers are connected 
with GBit. I also test a copy from the root partition to the ceph shared
 mount directory to see how long does it take to write data into ceph:</p>

<PRE>
 $ time dd if=/dev/zero of=pouet bs=2000M count=1; time sudo cp pouet /var/lib/nova/instances/;
1+0 records in
1+0 records out
2097152000 bytes (2.1 GB) copied, 4.27012 s, 491 MB/s

real  0m4.465s
user  0m0.000s
sys   0m4.456s

real  0m5.778s
user  0m0.000s
sys   0m3.580s
</PRE>

<p>Monitor from ceph:</p>
<pre>
<code>
16:24:01.943710    pg v11430: 592 pgs: 592 active+clean; 30471 MB data, 71127 MB used, 271 GB / 359 GB avail
16:24:04.129263    pg v11431: 592 pgs: 592 active+clean; 30591 MB data, 71359 MB used, 271 GB / 359 GB avail
16:24:06.187816    pg v11432: 592 pgs: 592 active+clean; 30691 MB data, 71632 MB used, 271 GB / 359 GB avail
16:24:07.345031    pg v11433: 592 pgs: 592 active+clean; 30815 MB data, 71932 MB used, 270 GB / 359 GB avail
16:24:08.283969    pg v11434: 592 pgs: 592 active+clean; 30967 MB data, 72649 MB used, 270 GB / 359 GB avail
16:24:11.458523    pg v11435: 592 pgs: 592 active+clean; 31079 MB data, 72855 MB used, 270 GB / 359 GB avail
16:24:12.543626    pg v11436: 592 pgs: 592 active+clean; 31147 MB data, 73007 MB used, 269 GB / 359 GB avail
16:24:15.447718    pg v11437: 592 pgs: 592 active+clean; 31195 MB data, 73208 MB used, 269 GB / 359 GB avail
16:24:18.258197    pg v11438: 592 pgs: 592 active+clean; 31319 MB data, 73260 MB used, 269 GB / 359 GB avail
16:24:23.187243    pg v11439: 592 pgs: 592 active+clean; 31467 MB data, 73488 MB used, 269 GB / 359 GB avail
16:24:24.680864    pg v11440: 592 pgs: 592 active+clean; 31574 MB data, 73792 MB used, 269 GB / 359 GB avail
16:24:25.299714    pg v11441: 592 pgs: 592 active+clean; 31622 MB data, 74013 MB used, 268 GB / 359 GB avail
16:24:27.015503    pg v11442: 592 pgs: 592 active+clean; 31626 MB data, 74101 MB used, 268 GB / 359 GB avail
16:24:28.554417    pg v11443: 592 pgs: 592 active+clean; 31810 MB data, 74237 MB used, 268 GB / 359 GB avail
16:24:32.029909    pg v11444: 592 pgs: 592 active+clean; 31827 MB data, 74333 MB used, 268 GB / 359 GB avail
16:24:32.814380    pg v11445: 592 pgs: 592 active+clean; 32231 MB data, 74586 MB used, 268 GB / 359 GB avail
16:24:33.803356    pg v11446: 592 pgs: 592 active+clean; 32291 MB data, 74900 MB used, 268 GB / 359 GB avail
16:24:36.476405    pg v11447: 592 pgs: 592 active+clean; 32291 MB data, 74938 MB used, 267 GB / 359 GB avail
16:24:37.674590    pg v11448: 592 pgs: 592 active+clean; 32292 MB data, 75054 MB used, 267 GB / 359 GB avail
16:24:38.711816    pg v11449: 592 pgs: 592 active+clean; 32292 MB data, 75108 MB used, 267 GB / 359 GB avail
</code>
</pre>

<p>The information reported by the <code>-w</code> option are asynchronous and not really significant. For instances we <strong>can't</strong> tell that storing 2GB in Ceph DFS took 37 seconds.</p>

<h4>IV.4. Instance benchmarks</h4>
<p>Flavor details:</p>

<ul>
  <li>CPU: 2</li>
  <li>RAM: 4GB</li>
  <li>Root partition: 10GB</li>
</ul>

<PRE>
ubuntu@instance-over-rbd:~$ for ((i=0 ; 10 -$i ; i++)) ; do dd if=/dev/zero of=pouet bs=1000M count=1; rm pouet; done
1048576000 bytes (1.0 GB) copied, 23.1742 s, 45.2 MB/s
1048576000 bytes (1.0 GB) copied, 33.765 s, 31.1 MB/s
1048576000 bytes (1.0 GB) copied, 39.409 s, 26.6 MB/s
1048576000 bytes (1.0 GB) copied, 22.8567 s, 45.9 MB/s
1048576000 bytes (1.0 GB) copied, 37.5275 s, 27.9 MB/s
1048576000 bytes (1.0 GB) copied, 18.422 s, 56.9 MB/s
1048576000 bytes (1.0 GB) copied, 20.1792 s, 52.0 MB/s
1048576000 bytes (1.0 GB) copied, 19.4536 s, 53.9 MB/s
1048576000 bytes (1.0 GB) copied, 15.5978 s, 67.2 MB/s
1048576000 bytes (1.0 GB) copied, 15.7292 s, 66.7 MB/s
</PRE>

<p><strong>Average: 47,34 MB/s</strong></p>
<p>Benchmark your filesystem in order to detect I/O errors (ext4 oriented):</p>

<PRE>
I/O stress <a href="http://sebastien-han.fr/down/io.c">Download me</a>
/*
 * Copyright (C) 2010 Canonical
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version 2
 * of the License, or (at your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
 * 
 */

/*
 *  Author Colin Ian King,  colin.king@canonical.com
 */

#include &lt;stdlib.h>
#include &lt;stdio.h>
#include &lt;string.h>
#include &lt;unistd.h>
#include &lt;fcntl.h>

#include &lt;sys/types.h>
#include &lt;sys/stat.h>
#include &lt;sys/ioctl.h>

#include &lt;linux/fs.h>
#include "fiemap.h"

#define FS_IOC_FIEMAP                      _IOWR('f', 11, struct fiemap)

void syntax(char **argv)
{
  fprintf(stderr, "%s [filename]...\n",argv[0]);
}

struct fiemap *read_fiemap(int fd)
{
  struct fiemap *fiemap;
  int extents_size;

  if ((fiemap = (struct fiemap*)malloc(sizeof(struct fiemap))) == NULL) {
      fprintf(stderr, "Out of memory allocating fiemap\n");    
      return NULL;
  }
  memset(fiemap, 0, sizeof(struct fiemap));

  fiemap->fm_start = 0;
  fiemap->fm_length = 2*1024*1024;        /* Lazy */
  fiemap->fm_flags = 0;
  fiemap->fm_extent_count = 0;
  fiemap->fm_mapped_extents = 0;

  /* Find out how many extents there are */
  if (ioctl(fd, FS_IOC_FIEMAP, fiemap) &lt; 0) {
      fprintf(stderr, "fiemap ioctl() failed\n");
      return NULL;
  }

  /* Read in the extents */
  extents_size = sizeof(struct fiemap_extent) *
                              (fiemap->fm_mapped_extents);

  /* Resize fiemap to allow us to read in the extents */
  if ((fiemap = (struct fiemap*)realloc(fiemap,sizeof(struct fiemap) +
                                         extents_size)) == NULL) {
      fprintf(stderr, "Out of memory allocating fiemap\n");    
      return NULL;
  }

  memset(fiemap->fm_extents, 0, extents_size);
  fiemap->fm_extent_count = fiemap->fm_mapped_extents;
  fiemap->fm_mapped_extents = 0;

  if (ioctl(fd, FS_IOC_FIEMAP, fiemap) &lt; 0) {
      fprintf(stderr, "fiemap ioctl() failed\n");
      return NULL;
  }
  
  return fiemap;
}

void dump_fiemap(struct fiemap *fiemap, char *filename)
{
  int i;

  printf("File %s has %d extents:\n",filename, fiemap->fm_mapped_extents);

  printf("#\tLogical          Physical         Length           Flags\n");
  for (i=0;i&lt;fiemap->fm_mapped_extents;i++) {
      printf("%d:\t%-16.16llx %-16.16llx %-16.16llx %-4.4x\n",
          i,
          fiemap->fm_extents[i].fe_logical,
          fiemap->fm_extents[i].fe_physical,
          fiemap->fm_extents[i].fe_length,
          fiemap->fm_extents[i].fe_flags);
  }
  printf("\n");
}

int main(int argc, char **argv)
{
  int i;

  if (argc &lt; 2) {
      syntax(argv);
      exit(EXIT_FAILURE);
  }

  for (i=1;i&lt;argc;i++) {
      int fd;

      if ((fd = open(argv[i], O_RDONLY)) &lt; 0) {
          fprintf(stderr, "Cannot open file %s\n", argv[i]);
      }
      else {
          struct fiemap *fiemap;

          if ((fiemap = read_fiemap(fd)) != NULL)
              dump_fiemap(fiemap, argv[i]);
          close(fd);
      }
  }
  exit(EXIT_SUCCESS);
}

</PRE>

<hr>

<h3>Final results</h3>
<p><link rel="stylesheet" href="IntroducingCeph2OpenStack_files/skin.css" type="text/css"></p>
<div id="tableWrapper" style="width: 100%; "><table id="vsTable"><tbody><tr><td class="cat title" style="width: 50%;"></td><td class="title" style="width: 50%;"><div class="">Openstack + Ceph</div></td></tr><tr class="second"><td class="cat" style="width: 50%;"><div class="">Create RBD volume</div></td><td style="width: 50%;"><div class="yes"></div></td></tr><tr><td class="cat" style="width: 50%;"><div class="">Delete RBD volume</div></td><td style="width: 50%;"><div class="yes"></div></td></tr><tr class="second"><td class="cat" style="width: 50%;"><div class="">Snapshot RBD volume</div></td><td style="width: 50%;"><div class="yes"></div></td></tr><tr><td class="cat" style="width: 50%;"><div class="">Attaching RBD volume</div></td><td style="width: 50%;"><div class="yes"></div></td></tr><tr class="second"><td class="cat" style="width: 50%;"><div class="">Glance images storage backend (import)</div></td><td style="width: 50%;"><div class="yes"></div></td></tr><tr><td class="cat" style="width: 50%;"><div class="">Snapshot running instance to RBD</div></td><td style="width: 50%;"><div class="partial"></div></td></tr><tr class="second"><td class="cat" style="width: 50%;"><div class="">Booting from RBD</div></td><td style="width: 50%;"><div class="partial"></div></td></tr><tr><td class="cat" style="width: 50%;"><div class="">Booting from a snapshoted image</div></td><td style="width: 50%;"><div class="no"></div></td></tr><tr class="second"><td class="cat" style="width: 50%;"><div class="">Boot VMs from shared /var/lib/nova/instances</div></td><td style="width: 50%;"><div class="partial"></div></td></tr><tr class="second"><td class="cat" style="width: 50%;"><div class="">Live migration with CephFS</div></td><td style="width: 50%;"><div class="yes"></div></td></tr></tbody></table></div>

<p>This table was created with <a href="http://www.compareninja.com/" 
target="_blank">Compare Ninja</a>.</p>

<h4>Observations</h4>
<p>I hope I will be able to go further and use Ceph for production. Ceph
 seems fearly stable enough at the moment, for RBD and RADOS, CephFS 
doesn't seem capable to handle huge I/O traffic. Also keep in mind that a
 company called <a href="http://www.inktank.com/">Inktank</a> offers a 
commercial support for Ceph, I don't thing it's a coincidence. Ceph will
 have a bright future. The recovery procedure is excellent, of course 
there is a lot of component which I would loved to play like fine 
crushmap tunning. This article could be updated at any time since I'm 
taking my research further :).</p>

<p><strong>This article wouldn't have been possible without the tremendous help of 
Josh Durgin from <a href="http://www.inktank.com/">Inktank</a>, many many thanks to 
him :)</strong></p>

</div>
<footer>
<p class="meta">
<span class="byline author vcard">Posted by <span class="fn">Sebastien Han</span></span>
<time datetime="2012-06-10T00:04:00+02:00" pubdate="" data-updated="true">Jun 
10<span>th</span>, 2012</time>
</footer>


<a name="NFSOverRBD"></a><h3 class="entry-title">NFS Over RBD 
<a href="http://www.sebastien-han.fr/blog/2012/07/06/nfs-over-rbd/" 
target="newwindow">(Source Origin)</a></h3>
<p class="meta">
<time datetime="2012-07-06T23:08:00+02:00" pubdate="" data-updated="true">Jul 6<span>th</span>, 2012</time>
| <a href="#disqus_thread">Comments</a>
</p>
</header>
<div class="entry-content"><p><img src="http://www.sebastien-han.fr/images/NFS-RBD.png" 
title="NFS on RBD"></p>
<p>Since CephFS is not most mature component in Ceph, you won't consider
 to use it on a production platform. In this article, I offer a possible
 solution to expose RBD to a shared filesystem.</p>
 
<h3>I. Architecture</h3>
<p>My choice was turned to NFS for a couple of reasons:</p>
<ul>
<li>Old but reliable</li>
<li>Easy to setup</li>
<li>Existing RA: exportfs and good support for the LSB agent</li>
</ul>
<p>Overview of the infrastructure. For my own setup, I needed to map and
 export several pools. For examples you could have one pool for the 
customers data and one pool for storing your VMs (<code>/var/lib/nova/instances</code>). It's up to you.</p>
<p><img class="center" src="http://www.sebastien-han.fr/images/NFS-RBD-schema.jpg" 
title="NFS over RBD Schema" width="800" height="800"></p>
<br>
<h3>II. Prerequisites</h3>
<p>Install Ceph client packages and the NFS server, this needs to be performed on <strong>every nodes</strong>:</p>

<PRE>
 $ sudo apt-get install ceph-common nfs-server -y
 $ sudo <span class="nb">echo "manual" &gt; /etc/init/nfs-kernel-server.override

</PRE>


<p>Nothing more, no <code>modprobe rbd</code>, nothing. Pacemaker will manage that for us :)</p>
<p>Create your RBD volumes:</p>

<PRE>
 $ rbd create share1 --size 2048
 $ rbd create share2 --size 2048

</PRE>


<p>You will need to map it somewhere in order to put a filesystem on it:</p>

<PRE>
 $ sudo modprobe rbd
 $ sudo <span class="nb">echo "172.17.1.4:6789,172.17.1.5:6789,172.17.1.7:6789 name=admin,secret=AQDVGc5P0LXzJhAA5C019tbdrgypFNXUpG2cqQ== rbd share1" &gt; sudo tee /sys/bus/rbd/add
 $ sudo mkfs.xfs /dev/rbd0
 $ rbd unmap /dev/rbd0

</PRE>


<p>And so on for <code>share2</code>.</p>
<p>In order to manage our RBD device we are going to use the RA written 
by Florian Haas for Ceph which map RBD device. You can have a look at it
 in the <a href="https://github.com/ceph/ceph/blob/master/src/ocf/rbd.in">Ceph Github</a>. Integrate the RA to Pacemaker:</p>

<PRE>
 $ sudo mkdir /usr/lib/ocf/resource.d/ceph
 $ <span class="nb">cd /usr/lib/ocf/resource.d/ceph/
 $ wget https://raw.github.com/ceph/ceph/master/src/ocf/rbd.in
 $ chmod +x rbd.in

</PRE>


<p>Minor change to the resource agent. According to the 
<a href="http://www.linux-ha.org/doc/dev-guides/_convenience_functions.html">official 
OCF documentation</a>.</p>

<PRE>
@@ -144,7 +144,7 @@ find_rbd_dev() {
 rbd_validate_all() {	
     # Test for configuration errors first
     if [ -z "$OCF_RESKEY_name" ]; then  	
-       ocf_log err 'Required parameter "name" is unset!'	
+       ocf_log err "Required parameter "name" is unset!"	
        exit $OCF_ERR_CONFIGURED
</PRE>


<p>The pull request is <a href="https://github.com/leseb/ceph/commit/3ab34c2e049570177c07b3563f55209590e48de1">waiting here</a>.</p>
<br>
<h3>III. Setup</h3>
<h4>III.1. Common</h4>
<p>This initial setup only containts 2 nodes so you need to setup Pacemaker according to this number.</p>

<PRE>
 $ sudo crm configure property stonith-enabled=<span class="nb">false
 $ sudo crm configure property no-quorum-policy=ignore

</PRE>


<p>Of course if you plan to expand your active/active with a third node, you must unset the <code>no-quorum-policy</code>.</p>
<h4>III.2. Primitives</h4>
<p>In order to make things really clear I will setup the primitive from <em>the bottom layer to the top</em>, something like:</p>
<ol>
<li>Map the RBD device</li>
<li>Mount it!</li>
<li>Export it!</li>
<li>Reach it with the virtual IP address</li>
<li>Setup the NFS server</li>
</ol>
<p>Note: for more comprehension and clarity I always name:</p>
<ul>
<li>the primitive with a <code>p_</code> prefix</li>
<li>the group with a <code>g_</code> prefix</li>
<li>the location rule with a <code>l_</code></li>
<li>and so on for every parameters</li>
</ul>
<p>All the operation needs to be performed within the <code>crm</code> shell or simply <code>sudo crm configure</code> before every commands below. You can also do <code>sudo crm configure edit</code> and copy/paste.</p>
<p>First, map RBD:</p>

<PRE>
primitive p_rbd_map_1 ocf:ceph:rbd.in \
        params  user="admin"  pool="rbd"  name="share1"  cephconf="/etc/ceph/ceph.conf" \
        op monitor  interval="10s"  timeout="20s"

primitive p_rbd_map_2 ocf:ceph:rbd.in \
        params  user="admin"  pool="rbd"  name="share2"  cephconf="/etc/ceph/ceph.conf" \
        op monitor  interval="10s"  timeout="20s"

</PRE>


<p>Second, filesystem:</p>

<PRE>
primitive p_fs_rbd_1 ocf:heartbeat:Filesystem \
        params  directory="/mnt/share1"  fstype="xfs"  device="/dev/rbd/rbd/share1"  fast_stop="no" \
        op monitor  interval="20s"  timeout="40s" \
        op start  interval="0"  timeout="60s" \
        op stop  interval="0"  timeout="60s"

primitive p_fs_rbd_2 ocf:heartbeat:Filesystem \
        params  directory="/mnt/share2"  fstype="xfs"  device="/dev/rbd/rbd/share2"  fast_stop="no" \
        op monitor  interval="20s"  timeout="40s" \
        op start  interval="0"  timeout="60s" \
        op stop  interval="0"  timeout="60s"

</PRE>


<p>Third, export directories:</p>

<PRE>
primitive p_export_rbd_1 ocf:heartbeat:exportfs \
  params  directory="/mnt/share1"  clientspec="192.168.146.0/24"  options="rw,async,no_subtree_check,no_root_squash"  fsid="1" \
  op monitor  interval="10s"  timeout="20s" \
  op start  interval="0"  timeout="40s"

primitive p_export_rbd_2 ocf:heartbeat:exportfs \
  params  directory="/mnt/share2"  clientspec="192.168.146.0/24"  options="rw,async,no_subtree_check,no_root_squash"  fsid="2" \
  op monitor  interval="10s"  timeout="20s" \
  op start  interval="0"  timeout="40s"

</PRE>


<p>Fourth, virtual IP addresses:</p>

<PRE>
primitive p_vip_1 ocf:heartbeat:IPaddr \
        params  ip="192.168.146.245"  cidr_netmask="24" \
        op monitor  interval="5"

primitive p_vip_2 ocf:heartbeat:IPaddr \
        params  ip="192.168.146.246"  cidr_netmask="24" \
        op monitor  interval="5"

</PRE>


<p>Fifth, NFS server:</p>

<PRE>
primitive p_nfs_server lsb:nfs-kernel-server \
  op monitor  interval="10s"  timeout="30s"

primitive p_portmap lsb:portmap \
  op monitor  interval="10s"  timeout="30s"

primitive p_statd lsb:statd \
        op monitor  interval="10s"  timeout="30s"

</PRE>


<h4>III.3. Resources group and clone</h4>
<p>Groups contain a set of resources that need to be located together, 
started sequentially and stopped in the reverse order. You need to 
create a group of resource for each NFS shared first and also for all 
the NFS dependencies services:</p>

<PRE>
group g_rbd_share_1 p_rbd_map_1 p_fs_rbd_1 p_export_rbd_1 p_vip_1
group g_rbd_share_2 p_rbd_map_2 p_fs_rbd_2 p_export_rbd_2 p_vip_2
group g_nfs p_portmap p_statd p_nfs_server

</PRE>


<p>Clones are resources that can be active on multiple hosts. We have to
 clone the NFS server, it will act as active/active. It means that the 
NFS daemon will be running/active on <strong>both</strong> nodes.</p>

<PRE>
clone clo_nfs g_nfs \
  meta globally-unique="false" target-role="Started"

</PRE>


<h4>III.4. Location rules</h4>
<p>In this setup, each export must run on a specific server, <strong>always</strong>.
 The resource will always remain in its current location unless forced 
off because the node is no longer eligible to run the resource. These 2 
contraints define a <strong>Score</strong> to determine the location 
relationship between both resources. Positive values indicate the 
resources should run on the same node. Setting the score to INFINITY 
forces the resources to run on the same node.</p>

<PRE>
location l_g_rbd_share_1 g_rbd_share_1 inf: nfs1
location l_g_rbd_share_2 g_rbd_share_2 inf: nfs2

</PRE>


<br>
<p>At the end, you should see something like this:</p>

<PRE>
 $ sudo crm_mon -1
============
Last updated: Mon Jul  2 07:19:40 2012
Last change: Mon Jul  2 04:07:15 2012 via crm_attribute on nfs1
Stack: openais
Current DC: nfs2 - partition with quorum
Version: 1.1.6-9971ebba4494012a93c03b40a2c58ec0eb60f50c
2 Nodes configured, 2 expected votes
14 Resources configured.
============

Online: [ nfs1 nfs2 ]

 Resource Group: g_rbd_share_1
     p_rbd_map_1        (ocf::heartbeat:rbd.in):        Started nfs1
     p_fs_rbd_1 (ocf::heartbeat:Filesystem):    Started nfs1
     p_export_rbd_1     (ocf::heartbeat:exportfs):      Started nfs1
     p_vip_1    (ocf::heartbeat:IPaddr):        Started nfs1
 Resource Group: g_rbd_share_2
     p_rbd_map_2        (ocf::heartbeat:rbd.in):        Started nfs2
     p_fs_rbd_2 (ocf::heartbeat:Filesystem):    Started nfs2
     p_export_rbd_2     (ocf::heartbeat:exportfs):      Started nfs2
     p_vip_2    (ocf::heartbeat:IPaddr):        Started nfs2
 Clone Set: clo_nfs [g_nfs]
     Started: [ nfs1 nfs2 ]

</PRE>


<br>
<blockquote><p>Conclusion: here we have a scalable architecture, we can 
add as many NFS server (clone) as we need. This will expand the 
active/active mode. That was only one use case. You don't necessary need
 active/active mode. An active/passive mode should be enough if you only
 need to map one RBD volume.</p></blockquote>
</div>
<footer>
<p class="meta">
<span class="byline author vcard">Posted by <span class="fn">Sebastien Han</span></span>
<time datetime="2012-07-06T23:08:00+02:00" pubdate="" data-updated="true">Jul 6<span>th</span>, 2012</time>
<span class="categories">
<a class="category" href="http://www.sebastien-han.fr/blog/categories/ceph/">Ceph</a>, <a class="category" href="http://www.sebastien-han.fr/blog/categories/pacemaker/">Pacemaker</a>
</span>
</p>

<a name="CephBenchmark"></a><h3>Ceph and RBD benchmarks 
<a href="http://learnitwithme.com/?p=303">(Source 
Origin)</a></h3>
<div class="date">
	Wednesday, August 24th, 2011</div>
<div class="entry"><p><a href="http://ceph.com/">Ceph</a>, 
an up and coming distributed&nbsp;file system, has a lot of great design
 goals. In short, it aims to distribute both data and metadata among 
multiple servers, providing both fault tolerant and scalable network 
storage.&nbsp;Needless to say, this has me excited, and while it's still
 under heavy development, I've been experimenting with it and thought 
I'd share a few simple benchmarks.</p>
<p>I've tested two different 'flavors' of Ceph, the first I believe is 
referred to as "Ceph filesystem", which is similar in function to NFS, 
where the file metadata (in addition to the file data) is handled by 
remote network services and the filesystem is mountable by multiple 
clients. The second is a "RADOS block device", or RBD. This refers to a 
virtual block device that is created from Ceph storage. This is similar 
in function to iSCSI, where remote storage is mapped into looking like a
 local SCSI device. This means that it's formatted&nbsp;and mounted 
locally and other clients can't use it without corruption (unless you 
format it with a cluster filesystem like GFS or OCFS).</p>
<p>If you're wondering what RADOS is, it's Ceph's acronym version of 
RAID. I believe it stands for "Reliable Autonomous Distributed Object 
Store". Technically, the Ceph filesystem is implemented on top of RADOS,
 and other things are capable of using it directly as well, such as the 
RADOS gateway, which is a proxy server that provides object store 
services like that of Amazon's S3. A librados library is also available 
that provides an API for customizing your own solutions.</p>
<p>I've taken the approach of comparing cephfs to nfs, and rbd to both 
iscsi and multiple iscsi devices striped over different servers. Mind 
you, Ceph provides many more features, such as snapshots and thin 
provisioning, not to mention the fault tolerance, but if we were to 
replace the function of NFS we'd put Ceph fs in its place; likewise if 
we replaced iSCSI, we'd use RBD. It's good to keep this in mind because 
of the penalties involved with having metadata at the server; we don't 
expect Ceph fs or NFS to have the metadata performance of a local 
filesystem.</p>
<ul>
<li>Ceph (version .032) systems were 3 servers running mds+mon services.
 These were quad core servers, 16G RAM. The storage was provided by 3 
osd servers (24 core AMD box, 32GB RAM, 28 available 2T disks, LSI 
9285-8e), each server used 10 disks, one osd daemon for each 2T disk, 
and an enterprise SSD partitioned up with 10 x 1GB journal devices. 
Tried both btrfs and xfs on the osd devices, for these tests there was 
no difference. CRUSH placement defined that no replica should be on the 
same host, 2 copies of data and 3 copies of metadata. All servers had 
gigabit NICs.</li>
</ul>
<ul>
<li>Second Ceph system has monitors, mds, and osd all on one box. This 
was intended to be a more direct comparison to the NFS server below, and
 used the same storage device served up by a single osd daemon.</li>
</ul>


<ul>
  <li>NFS server was one of the above osd servers with a group of 12 2T drives in 
      RAID50 formatted xfs and exported.</li>
</ul>

<ul>
  <li>RADOS benchmarks ran on the same two Ceph systems above, from which a 20T RBD 
      device was created.</li>
</ul>

<ul>
  <li>ISCSI server was tested with one of the above osd servers exporting a 12 disk 
      RAID50 as a target.</li>
</ul>
<ul>
   <li>ISCSI-md was achieved by having all three osd servers export a 12 disk RAID50 
       and the client striping across them.</li>
</ul>

<ul>
  <li>All filesystems were mounted noatime,nodiratime whether available or not.  All 
      servers were running kernel 3.1.0-rc1 on centos 6.&nbsp;Benchmarks were performed 
      using bonnie++, as well as a few simple real world tests such as copying data 
      back and forth.</li>
</ul>

<p><a href="http://learnitwithme.com/wp-content/uploads/2011/08/ceph-nfs-iscsi-benchmarks.ods">ceph-nfs-iscsi-benchmarks.ods</a></p>
<p>The sequential character writes were cpu bound on the client in all 
instances; the sequential block writes (and most sequential reads) were 
limited by the gigabit network. The Ceph fs systems seem to do well on 
seeks, but this did not translate directly into better performance in 
the create/read/delete tests. It seems that RBD is roughly in a position
 where it can replace iSCSI, but the Ceph fs performance needs some work
 (or at least some heavy tuning on my part) in order to get it up to 
speed.</p>
<p>It will take some digging to determine where the bottlenecks lie, but
 in my quick assessment most of the server resources were only 
moderately used, whether it be the monitors, mds, or osd devices. Even 
the fast journal SSD disk only ever hit 30% utilization, and didn't help
 boost performance significantly over the competitors who don't rely on 
it.</p>
<p>Still, there's something to be said for this, as Ceph allows storage 
to fail, be dynamically added, thin provisioned, rebalanced, snapshots, 
and much more, with passable performance, all in pre-1.0 code. &nbsp;I 
think Ceph has a big future in open source storage deployments, and I 
look forward to it being a mature product that we can leverage to 
provide dynamic, fault-tolerant network storage.</p>

<a name="CephiScsi"></a><h3 id="firstHeading" class="firstHeading">iSCSI From Ceph wiki
<a href="http://ceph.com/w/index.php?title=ISCSI">(Source Origin)</a></h3>

<p>Since <a href="http://ceph.com/w/index.php?title=Rbd" title="Rbd">RBD</a> gives you a block 
device it should be possible to re-export this block device via 
<a href="http://en.wikipedia.org/wiki/ISCSI" class="external text" 
rel="nofollow">iSCSI</a> just like you can re-export a 
<a href="http://ceph.com/w/index.php?title=Ceph" title="Ceph">Ceph</a> filesystem via 
<a href="http://ceph.com/w/index.php?title=Re-exporting_NFS" title="Re-exporting NFS">NFS</a>.
</p>

<a name="MultipathRBD"></a><h4>iSCSI Multipath with RBD</h4>
<p>iSCSI has some nice features like multipathing, with multipathing you  can create 
multiple IP-paths to your SAN thus creating redundancy in your network. The one thing 
you have to make sure is that the data of the LUN's is synchronized somehow between 
the <i>targets</i> (server), since the <i>initiatior</i> (client) will <i>NOT</i> 
do this.
</p>
<p>Since <a href="http://ceph.com/w/index.php?title=RADOS&amp;action=edit&amp;redlink=1" 
class="new" title="RADOS (page does not exist)">RADOS</a> in combination with the 
<a href="http://ceph.com/w/index.php?title=Rbd" title="Rbd">RADOS Block Driver</a> gives you this, 
you can create a High Available multipath iSCSI setup.
</p><p>A possible setup could look like:
</p><p><img alt="Iscsi rbd.jpg" src="http://ceph.com/w/images/4/45/Iscsi_rbd.jpg" 
height="556" width="350">
</p><p>Here you have two clients with:
</p>
<ul>
  <li> <a href="http://iscsitarget.sourceforge.net/" class="external text" 
rel="nofollow">iSCSI Enterprise Target</a></li>
  <li> A kernel with the <a href="http://ceph.com/w/index.php?title=Rbd" title="Rbd">RADOS Block 
Driver</a></li>
</ul>
<p>The configuration on both hosts should be identical, which could be:
</p>

<p><i>/etc/ietd.conf</i></p>

<pre>
Target iqn.2010-08.net.newdream.ceph:rados.iscsi.001
   Lun 0 Path=/dev/rbd0,Type=blockio,ScsiId=f817ab
   Lun 1 Path=/dev/rbd1,Type=blockio,ScsiId=ah194l
   Lun 2 Path=/dev/rbd2,Type=blockio,ScsiId=dj291k
</pre>

<p>Now, both <i>targets</i> should have this configuration and have <i>/dev/rbd0</i>, 
<i>/dev/rbd1</i>, <i>/dev/rbd2</i> configured via the 
<a href="http://ceph.com/w/index.php?title=Rbd" title="Rbd">RADOS Block Driver</a> interface.
</p><p>We choose <i>Type=blockio</i> since we want <i>Direct Block I/O</i>, should we 
choose <i>fileio</i>, the kernel on the <i>target</i> will utilize it's file cache, 
which will then lead to cache-inconsistency between the <i>targets</i>. Choosing 
<i>blockio</i> will by-pass the kernel's cache, which is good in this case.</p>

<p>Switching to <i>fileio</i> could give you a performance boost, but then you 
will have to switch to a single <i>target</i> setup, but then you will loose the 
redundancy.</p>

<p>For a <a href="#MultiPathIscsi">Multipath iSCSI setup</a> you will need to specify 
a <i>ScsiId</i> for each lun, this enables <i>dm-multipath</i> on the <i>initiator</i> 
to find out which SCSI devices are the same and map them to one logical SCSI device.
</p>

<p>When this is up and running you should be able to do a discovery (send targets) on 
both targets and get your multipath setup running.</p>

<p>Since iSCSI is supported on Linux, Windows, Solaris, FreeBSD and many other 
platforms, it's a easy and reliable way to get data from your
<a href="http://ceph.com/w/index.php?title=Ceph" title="Ceph">Ceph</a> cluster shared with 
other platforms.</p>

<p>Someday there might be a driver for Windows or FreeBSD, but there
 will always be situations (for example with SLA's) where you don't want
 third-party drivers in your setup.
</p>

<a name="MultiPathIscsi"></a>
<h3>Multipath iSCSI under Linux
<a href="http://blog.widodh.nl/2009/11/multipath-iscsi-under-linux/">(Source 
Origin)</a></h3>

<p>Building a iSCSI Target (Server) under Linux is fairly simple, 
just install Enterprise iSCSI Target (IET) and you are ready. The 
Initiator (Client) is simple to, just use Open-iSCSI and you are ready 
to go, but how do you make this redundant?</p>
<p>When i first started using iSCSI i heard about the term "multipath", i
 read that you could make a redundant IP link to your iSCSI Target with 
multipath, but how?</p>
<p>Searching on the web didn't give me real practical anwsers. After 
using multipath for about 2 years now, i thought, why don't i make a 
blog post about it so other people can have redundant iSCSI to!</p>
<p>For this example i have a iSCSI Target with two IP's:</p>
<ol>
  <li>172.16.0.1/255.255.255.0</li>
  <li>172.16.1.1/255.255.255.0</li>
</ol>
<p>These IP's given to eth0 and eth1, via two switches the connectivity is given to my 
initiator with the IP's:</p>
<ol>
  <li>172.16.0.101/255.255.255.0</li>
  <li>172.16.1.101/255.255.255.0</li>
</ol>
<p>So there is a redundant network connection to the target, now we just have to start 
using this.</p>

<p>My target has as IQN: "iqn.2009-11-11.widodh.storage:iscsi-001"</p>

<p>I suppose you know how to configure IET and Open-iSCSI, so i'll just skip the 
regular configuration. In this example my Target exports one LUN of 10GB.</p>


<p>On the client (Ubuntu 9.04 (Jaunty)) you have to install:</p>
<ol>
  <li>open-iscsi</li>
  <li>multipath-tools</li>
</ol>
<p>And that's it, there is no configuration needed for multipath, this is all done 
dynamically.</p>

<p>Now we are going to discover the Target on both IP's and log on to it:</p>

<pre>
iscsiadm -m discovery -t sendtargets -p 172.16.1.1
iscsiadm -m discovery -t sendtargets -p 172.16.0.1
iscsiadm -m node -T iqn.2009-11-11.widodh.storage:iscsi-001 -p 172.16.0.1 --login
iscsiadm -m node -T iqn.2009-11-11.widodh.storage:iscsi-001 -p 172.16.1.1 --login
</pre>

<p>The nicest thing about this is, that Multipath itself discovers that 
there is a redundant connection to a SCSI device and everything is done 
for you.</p>

<p>In "/dev/mapper" you'll find (for example) 
"14945540000000000000000000100000099b2f8000f000000"and that is your multipath device.</p>
<p>You can list your multipath devices with:</p>

<pre>
 multipath -ll
</pre>

<p>In my example this looked like:</p>

<pre>14945540000000000000000000100000099b2f8000f000000dm-0 IET     ,VIRTUAL-DISK  
[size=35G][features=0][hwhandler=0]
\_ round-robin 0 [prio=1][active]
 \_ 4:0:0:0 sdd 8:48  [active][ready]
\_ round-robin 0 [prio=1][enabled]
 \_ 3:0:0:0 sdc 8:32  [active][ready]</pre>
<p>Multipath detected a redundant path for "sdc" and "sdd" and created a device which 
i could use.</p>

<p>If one of the connections goes down for what ever reason, you should see this in 
your dmesg:</p>

<pre>
[ 2070.285310] device-mapper: multipath: Failing path 8:32.
</pre>

<p>Multipath will then show:</p>

<pre>
sdc: checker msg is "directio checker reports path is down"
14945540000000000000000000100000099b2f8000f000000dm-0 IET     ,VIRTUAL-DISK  
[size=35G][features=0][hwhandler=0]
\_ round-robin 0 [prio=1][active]
 \_ 4:0:0:0 sdd 8:48  [active][ready]
\_ round-robin 0 [prio=0][enabled]
 \_ 3:0:0:0 sdc 8:32  [failed][faulty]
</pre>

<p>Yes, you will see a lot of SCSI errors in your dmesg, but since you have a redundant 
path that is nothing to be worried about.</p>

<p>Just keep in mind, use "/dev/mapper/14945540000000000000000000100000099b2f8000f000000"
as your block device for whatever you intent to use it!</p>

<p>Multipath in combination with iSCSI is really great, a simple network hickup will 
never get your services down and you can keep your network like a basic Layer-2 
network, no STP is needed, the redundant paths can go over fully seperated links which 
decreases the chance for downtime!</p>

<p>Have fun using multipath!</p>



<a name="CephFromHastexo"></a><h3>Ceph And Rados from Hastexo.com</h3>
<OL>
<li><a href="#CephAmpRados" target="newwindow">Ceph &amp; RADOS</a></li>
<li><a href="#ConfigRados" target="newwindow">Configuring radosgw to behave like 
    Amazon S3</a></li>
<li><a href="#MigrVMBSRados" target="newwindow">Migrating virtual machines from 
block-based storage to RADOS/Ceph</a>
<li><a href="#WhichOSDStoreMyRados">Which OSD stores a specific RADOS object?</a></li>
<li><a href="#TurningRbdToSan">Turning Ceph RBD Images into SAN Storage Devices</a></li>
<li><a href="http://www.hastexo.com/resources/presentations/glusterfs-und-ceph-skalierbares-storage-ohne-wenn-und-aber" title="GlusterFS und Ceph. Skalierbares Storage ohne Wenn und Aber." target="newwindow">GlusterFS und Ceph (German, CeBIT 2012)</a></li>
<li><a href="http://www.hastexo.com/resources/presentations/hands-ceph" target="newwindow">Hands-On With Ceph (LCEU 2012)</a></li>
</OL>


<a name="CephAmpRados"></a><h3 class="title">Ceph &amp; RADOS
<a href="http://www.hastexo.com/knowledge/storage-io/ceph" title="Ceph &amp; 
RADOS">(Source Origin)</a></h3>
                                
 <p><a href="http://ceph.com/" target="_blank">Ceph</a>&nbsp;is an 
open-source distributed petascale storage stack. hastexo offers 
top-notch expertise, consulting services, and training around Ceph.</p>

<h4>What is Ceph about?</h4>
<p>Ceph is an integrated storage stack offering object storage, block 
storage, and a POSIX compliant distributed filesystem. It offers massive
 scalability, configurable synchronous replication, and n-way 
redundancy.</p>
<p><a href="http://ceph.com/ceph-storage/object-storage/" target="_blank">RADOS</a>,
 the reliable autonomic distributed object store is a massively 
distributed, replicating, rack-aware object store.&nbsp;This object 
store uses a deterministic placement algorithm, CRUSH (Controlled 
Replication Under Scaleable Hashing). There's never a central instance 
to ask on every access, instead, everything can work out where objects 
are. That means the store scales out seamlessly, and can expand and 
contract on the admin's whim.</p>
<h4>What can I use it for?</h4>
<p>Lots of things.</p>
<ul>
<li><strong>radosgw</strong> provides a RESTful API for dynamic cloud 
storage. And it includes an S3 and Swift frontend to act as object 
storage for AWS/Eucalyptus and 
<a href="http://www.hastexo.com/knowledge/openstack">OpenStack</a> clouds, 
respectively.</li> 
<li><strong>Qemu-RBD</strong> is a storage driver for the Qemu/KVM 
hypervisor (fully integrated with libvirt) that allows the hypervisor to
 access replicated block devices that are also striped across the object
 store&nbsp;- with a configurable number of replicas, of course.</li>
<li><strong>RBD</strong> is a Linux block device that, again, is striped and 
replicated over the object store.</li>
<li><strong>librados</strong> (C) and <strong>libradospp</strong> (C++) 
are APIs to access the object store programmatically, and come with a 
number of scripting language bindings. Qemu-RBD builds on librados.</li>
<li><strong>Ceph</strong> (the filesystem)&nbsp;exposes POSIX filesystem
 semantics built on top of RADOS,&nbsp;where all POSIX-related metadata 
is again stored in the object store. This is a remarkably thin client 
layer at just 17,000 LOC (compare to GFS2 at 26,000 and OCFS2 at 
68,000).</li>
</ul>
<h4>How is it licensed?</h4>
<p>All of Ceph is 100% open source, everything is LGPL 2.1 licensed.</p>
<h4>How can hastexo help?</h4>
<p>We're offering Ceph instruction as part of our&nbsp;<a href="http://www.hastexo.com/services/training/high-availability-expert">High Availability Expert</a>&nbsp;and&nbsp;<a href="http://www.hastexo.com/services/training/cloud-bootcamp">Cloud Bootcamp for OpenStack</a>&nbsp;training classes. Besides that, we can help you with <a href="http://www.hastexo.com/services/custom-training">customized training</a> on the Ceph stack, and with both <a href="http://www.hastexo.com/services/onsite">on-site</a> and <a href="http://www.hastexo.com/services/remote">remote</a> consultancy. Just <a href="http://www.hastexo.com/contact">drop us a line</a> and we'll be happy to help!</p>

<a name="ConfigRados"></a><h3 class="title">Configuring radosgw to behave like 
Amazon S3 
<a href="http://www.hastexo.com/resources/hints-and-kinks/configuring-radosgw-behave-amazon-s3">(Source Origin)</a></h3>
                                
        
    <p>If you've heard of <a href="http://ceph.com/" target="_blank">Ceph</a>, you've surely heard of <a href="http://ceph.com/docs/master/radosgw/" target="_blank">radosgw</a>,
 a RESTful gateway interface to the RADOS object store. You've probably 
also heard that it provides a front-end interface that is compatible 
with Amazon's S3 API.</p>
<p>The question remains, if you have an S3 client that <em>always</em> assumes it can find objects at <code>http://<em>bucket</em>.s3.amazonaws.com</code>, how can you use such a client to interact, unmodified, with your radosgw host (or hosts)?</p>
<p>Pulling this off is actually remarkably simple, if you can control 
what nameserver your clients use to resolve DNS names. Which should be a
 given in the private cloud space.</p>
<p>First, of course, you'll need an installed and configured Ceph 
cluster with one or several radosgw nodes. The Ceph documentation <a href="http://ceph.com/docs/master/radosgw/config/" target="_blank">is an excellent reference for setting up radosgw</a>.</p>
<h4>Configuring radosgw to support virtual hosts</h4>
<p>Then, you make sure you have the following entry in your Ceph configuration (normally in <code>/etc/ceph/ceph.conf</code>):</p>
<pre>[client.radosgw.charlie]
  rgw dns name = s3.amazonaws.com
</pre><p>Substitute <code>charlie</code> with whatever name you want to use for your radosgw client when you interact with Ceph. What the <code>rgw dns name</code>&nbsp;option specifies is that radosgw will answer queries also for URLs like <code>http://<em>bucket</em>.<em>hostname</em>/<em>object</em></code>, as opposed to just <code>http://<em>hostname</em>/<em>bucket</em>/<em>object</em></code>.</p>
<h4>Configuring Apache to respond to S3 host names</h4>
<p>Also, add a wildcard record to the ServerAlias directive in the web server configuration for your radosgw host. For example:</p>
<pre> &nbsp;&lt;VirtualHost *:80&gt;
 &nbsp;&nbsp;&nbsp;ServerName radosgw.example.com
 &nbsp;&nbsp;&nbsp;ServerAlias s3.amazonaws.com
 &nbsp;&nbsp;&nbsp;ServerAlias *.amazonaws.com</pre><h4>Configuring your DNS server</h4>
<p>Then, set up your DNS server with a wildcard record in the <code>s3.amazonaws.com</code> zone, and have nameserver respond to requests in that zone. The zone file (for BIND9, in this case) could look like this:</p>
<pre>$TTL	604800
@	IN	SOA	alice.example.com. root.alice.example.com. (
			      2		; Serial
			 604800		; Refresh
			  86400		; Retry
			2419200		; Expire
			 604800 )	; Negative Cache TTL
;
@	IN	NS	alice.example.com.
@	IN	A	192.168.122.113
*	IN	CNAME	@
</pre><p>In this zone, the A record <code>s3.amazonaws.com</code> resolves to <code>192.168.122.113</code>, and any sub-domain (like <code>mybucket.s3.amazonaws.com</code>) also resolves to that same address via a CNAME record.</p>
<h4>Using your RADOS store with S3 clients</h4>
<p>And then you just configure your client hosts to resolve DNS names 
via that nameserver, and use your preferred client application to 
interact with it.</p>
<p>For example, for a user that you've created with radosgw-admin, which uses the access key <code>12345</code> with a secret of <code>67890</code>, and Mark Atwood's popular <a href="http://search.cpan.org/%7Emra/Net-Amazon-S3-Tools-0.08/" target="_blank"><code>Net::Amazon::S3::Tools</code> toolkit</a>, here's how you can interact with your RADOS objects:</p>
<pre># export AWS_ACCESS_KEY_ID=12345
# export AWS_ACCESS_KEY_SECRET=67890
# s3mkbucket mymostawesomebucket
# s3ls
mymostawesomebucket
# s3put mymostawesomebucket/foobar &lt;&lt;&lt; "hello world"
# s3ls mymostawesomebucket
foobar
# s3get mymostawesomebucket/foobar
hello world</pre><p>Simple enough. You can add one more nifty feature.</p>
<h4>Adding load balancing</h4>
<p>radosgw can scale horizontally, and all you need to do to make this 
work is to duplicate your radosgw and Apache configuration onto a 
different host, and then add a second record to your DNS zone:</p>
<pre>$TTL	604800
@	IN	SOA	alice.example.com. root.alice.example.com. (
			      3		; Serial
			 604800		; Refresh
			  86400		; Retry
			2419200		; Expire
			 604800 )	; Negative Cache TTL
;
@	IN	NS	alice.example.com.
@	IN	A	192.168.122.112
@	IN	A	192.168.122.113
*	IN	CNAME	@
</pre><p>Then, as you access more buckets, you'll hit the A records in a
 round-robin fashion, meaning your requests will be balanced across the 
servers. Add as many as you like.</p>

<h4>HTTPS support</h4>
<p>Obviously, the above steps will not work for HTTPS connections to the
 REST API. And really, making that work would amount to some pretty 
terrible SSL certificate authority and client trust hackery, so just 
don't do it.</p>

<a name="MigrVMBSRados"></a><h3>Migrating virtual machines from block-based storage 
to RADOS/Ceph 
<a href="http://www.hastexo.com/resources/hints-and-kinks/migrating-virtual-machines-block-based-storage-radosceph" 
target="newwindow">(Source Origin)</a></h3>
                                
   <p>Ceph allows you to replace existing SAN storage (or SAN drop-in 
substitutes) with a flexible storage solution with real scale-out 
capabilities.&nbsp;Here is how you migrate existing virtual machines 
managed by libvirt from block-based storage to a Ceph based storage 
solution.</p>
<h4>Prerequisites</h4>
<p>What you'll need in order to successfully manage the migration from block-based storage to a working Ceph cluster is this:</p>
<ul>
<li>A working Ceph cluster. You probably guessed this one. More specifically, you should have&nbsp;
<ul>
<li>access to the <em>client.admin</em>&nbsp;key of your RADOS installation. Usually, the key will be stored in <em>/etc/ceph/keyring</em>&nbsp;on nodes running RADOS.</li>
<li>a RADOS pool in which you can create RBD images. You can either use the standard <em>rbd</em>&nbsp;pool or <a href="http://ceph.com/docs/master/rados/operations/pools/">create your own pool</a>. We'll use the <em>libvirt</em>&nbsp;pool throughout the following example.</li>
<li>a set of credentials for a client to connect to the cluster and 
create and use RBD devices. If you use a libvirt version &lt; 0.9.7, you
 will have to use the default <em>client.admin</em> credentials for this purpose. If you run libvirt 0.9.7 or later, you should use a separate set of credentials (i.e. <a href="http://ceph.com/docs/master/man/8/ceph-authtool/">create a user</a> called e.g. <em>client.rbd</em> and use that one).</li>
<li>that user should have at least the allow r permission on your mons, 
and allow rw on your osds (the latter you can restrict to the rbd pool 
used if you wish).</li>
</ul>
</li>
<li>qemu in version 0.14 or higher</li>
<li>libvirt in version 0.8.7 or higher (0.9.7 or higher if you want to use a separate user for this)</li>
<li>Ceph 0.48 ("argonaut") or higher</li>
</ul>
<h4>Getting Started</h4>
<p>When migrating a VM from block-based storage to a Ceph cluster, you 
unfortunately can't avoid a period of downtime (after all, you won't be 
able to reliably copy a filesystem from place A to B while it's still 
changing on the go). So the first thing to do is shut down a currently 
running virtual machine, like we will do with the <em>ubuntu-amd64-alice</em> VM in this example:</p>
<pre>virsh shutdown ubuntu-amd64-alice</pre><p>Then you need to create 
an RBD image within that pool. Suppose you would like to create one that
 is 100GB in size (recall, all RBD images are thin-provisioned, so it 
won't actually use 100GB in the Ceph cluster right from the start).</p>
<pre>qemu-img create -f rbd rbd:libvirt/ubuntu-amd64-alice 100G</pre><p>This
 means you are connecting to the Ceph mon servers (defined in the 
default configuration file, /etc/ceph/ceph.conf) using the <em>client.admin</em> identity, whose authentication key should be stored in /etc/ceph/keyring. The nominal image size is 102400MB, it's part of the <em>libvirt</em> pool and its name is a hardly creative <em>ubuntu-amd64-alice</em>.</p>
<p>You can run this command from any node inside or outside your Ceph 
cluster, as long as the configuration file and authentication 
credentials are stored in the appropriate location. The next step, 
however, is one that you must complete on the node where you can 
currently access your block-based storage. This could either be the 
machine that you have your VM's device currently connected to via iSCSI 
or - if you are using a SAN drop-in replacement based on DRBD - the 
machine that currently has the VM's DRBD resource in <em>Primary</em>&nbsp;mode.</p>
<p>If you are unsure what your VM's block device is, take a look at the VM's configuration with&nbsp;</p>
<pre>virsh dumpxml ubuntu-amd64-alice</pre><p>to find out the actual device name (look out for paragraphs including a <em>&lt;disk&gt;</em> statement). In our case, the actual device is&nbsp;<em>/dev/drbd/by-res/vm-ubuntu-amd64-alice.</em>&nbsp;Now
 let's go ahead and do the actual conversion. Please note: For the 
following command to work, you need a properly populated <em>/etc/ceph </em>directory because that is where qemu-img gets its information from. This is the command that initiates the conversion:&nbsp;</p>
<pre>qemu-img convert -f raw -O rbd&nbsp;\<br>  /dev/drbd/by-res/vm-ubuntu-amd64-alice \<br>  rbd:libvirt/ubuntu-amd64-alice</pre><p>Once
 the qemu-img command has completed, the actual conversion of your data 
is already done. That was easy, wasn't it? The final step is to change 
your libvirt VM configuration file to reflect the changes.</p>
<h4>Adapting the VM's libvirt configuration (libvirt &lt; 0.9.7)</h4>
<p>If we want our VM to run on top of a Ceph object store, we need to 
tell libvirt how to start the VM appropriately. Luckily, current 
versions of libvirt support Ceph-based RBD backing devices out of the 
box. Please note: All following steps assume that you have your <em>/etc/ceph</em> set up properly. This means that a working <em>ceph.conf</em>&nbsp;and a <em>keyring</em> file containing the authentication key for <em>client.admin&nbsp;</em>is present.</p>
<p>Open up your VM's configuration for editing with</p>
<pre>virsh edit ubuntu-amd64-alice</pre><p>and scroll down to the VM's disk definition. In our example, that part of the configuration looks like this:</p>
<pre>&lt;disk type='block' device='disk'&gt;<br>&nbsp; &lt;driver name='qemu' type='raw' cache='none'/&gt;<br>&nbsp; &lt;source dev='/dev/drbd/by-res/vm-ubuntu-amd64-alice'/&gt;<br>&nbsp; &lt;target dev='vda' bus='virtio'/&gt;<br>&nbsp; &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt;<br>&lt;/disk&gt;</pre><p style="font-family: Arial, Helvetica, 'Nimbus Sans L', sans-serif; font-size: 16px; white-space: normal;">Replace it with an entry using our RBD image:</p>
<pre>&lt;disk type='network' device='disk'&gt;<br>&nbsp; &lt;driver name='qemu' type='raw'/&gt;<br>&nbsp; &lt;source protocol='rbd' name='libvirt/ubuntu-amd64-alice'&gt;<br>&nbsp; &nbsp; &lt;host name='<strong>192.168.133.111</strong>' port='6789'/&gt;<br>&nbsp; &nbsp; &lt;host name='<strong>192.168.133.112</strong>' port='6789'/&gt;<br>&nbsp; &nbsp; &lt;host name='<strong>192.168.133.113</strong>' port='6789'/&gt;<br>&nbsp; &lt;/source&gt;<br>&nbsp; &lt;target dev='vda' bus='virtio'/&gt;<br>&nbsp; &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt;<br>&lt;/disk&gt;</pre><p>Be sure to replace the three IPs marked bold in the above example with the actual IPs of your MON servers.&nbsp;</p>
<p>Finally, start your virtual machine:</p>
<pre>virsh start ubuntu-amd64-alice</pre><h4>Adapting the VM's libvirt configuration (libvirt &gt;= 0.9.7)</h4>
<p>Starting with libvirt 0.9.7, you can use a user other than <em>client.admin</em>
 to access RBD images via libvirt. We recommend to do this. Creating 
such a setup works very similar to the one without a separate user; the 
main difference is that it requires you to define a secret in libvirt 
for the VM. First of all, figure out what user you will be using from 
within libvirt and where that user's authentication key is stored. For 
this example, we will assume that the user is called <em>client.rbd</em> and that this user's key is stored in <em>/etc/ceph/keyring.client.rbd</em>. Now, create a new UUID by calling&nbsp;</p>
<pre>uuidgen</pre><p>on the command line. The UUID for our example will be <em>5cddc503-9c29-4aa8-943a-c097f87677cf.&nbsp;</em> Then, open /etc/libvirt/secrets/ubuntu-amd64-alice.xml and define a secret block in there:</p>
<pre>&lt;secret ephemeral="no" private="no"&gt;<br>&lt;uuid&gt;<strong>5cddc503-9c29-4aa8-943a-c097f87677cf</strong>&lt;/uuid&gt;<br>&lt;usage type="ceph"&gt;<br>&nbsp; &lt;name&gt;client.rbd secret&lt;/name&gt;<br>&lt;/usage&gt;<br>&lt;/secret&gt;</pre><p>Be sure to replace the example's UUID with your own, self-generated value. Make libvirt add this secret to its internal keyring:</p>
<pre>virsh secret-define \<br>  /etc/libvirt/secrets/ubuntu-amd64-alice.xml</pre><p>Now find out what your user's secret key. Do</p>
<pre>ceph auth get-or-create client.rbd</pre><p>and take note of the key. In our example, <em>AQB0Q4ZQYDB2MBAAYzWmHvpg7t1MzV1E0jkBww==</em> is the key that will allow us access as client.rbd. Then define the actual password for our secret definition:</p>
<pre>virsh secret-set-value \<br><strong>  5cddc503-9c29-4aa8-943a-c097f87677cf </strong>\<strong><br></strong><strong>  AQB0Q4ZQYDB2MBAAYzWmHvpg7t1MzV1E0jkBww==</strong></pre><p>Again,
 be sure to use your self-generated UUID instead of the one in this 
example. Also replace the example key with your real key. Finally, go 
ahead and adapt your VM settings. Open your VM confiugration with</p>
<pre>virsh edit ubuntu-amd64-alice</pre><p>and scroll down to the VM's disk definition. In our example, that part of the configuration looks like this:</p>
<pre>&lt;disk type='block' device='disk'&gt;<br>&nbsp; &lt;driver name='qemu' type='raw' cache='none'/&gt;<br>&nbsp; &lt;source dev='/dev/drbd/by-res/vm-ubuntu-amd64-alice'/&gt;<br>&nbsp; &lt;target dev='vda' bus='virtio'/&gt;<br>&nbsp; &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt;<br>&lt;/disk&gt;</pre><p>Replace it with an entry using our RBD image:</p>
<div>
<pre>&lt;disk type='network' device='disk'&gt;<br>  &lt;driver name='qemu' type='raw'/&gt;<br>  &lt;auth username='rbd'&gt;<br>    &lt;secret type='ceph' usage='client.rbd secret'/&gt;<br>  &lt;/auth&gt;<br>  &lt;source protocol='rbd' name='libvirt/ubuntu-amd64-alice'&gt;<br>    &lt;host name='<strong>192.168.133.111</strong>' port='6789'/&gt;<br>    &lt;host name='<strong>192.168.133.112</strong>' port='6789'/&gt;<br>    &lt;host name='<strong>192.168.133.113</strong>' port='6789'/&gt;<br>  &lt;/source&gt;<br>  &lt;target dev='vda' bus='virtio'/&gt;<br>  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt;<br>&lt;/disk&gt;</pre></div>
<p>Be sure to replace the three IPs marked bold in the above example with the actual IPs of your MON servers.&nbsp;</p>
<p>Finally, start your virtual machine:</p>
<pre>virsh start ubuntu-amd64-alice</pre><div>That's it. Your VM should 
now boot up and use its RBD image from Ceph instead of its original 
block-based storage backing device.&nbsp;</div>

<h3>Finding out which OSDs currently store a specific RADOS object 
<a name="WhichOSDStoreMyRados"></a><a href="http://www.hastexo.com/resources/hints-and-kinks/which-osd-stores-specific-rados-object"
 target="newwindow">(Source Origin)</a></h3>
                                
<p>Ever wanted to know just which of your OSDs a RADOS object is currently stored in? Here's how.</p>
<p>Suppose you've got an RBD device, named <code>test</code>. Then you can use the <code>rbd info</code> command to display which name prefix is used by the RADOS objects that make up the RBD:</p>
<pre>ceph04:~ # rbd info test
rbd image 'test':
	size 1024 MB in 256 objects
	order 22 (4096 KB objects)
	block_name_prefix: rb.0.0
	parent:  (pool -1)
</pre><p>In this example, the prefix we're looking for is <code>rb.0.0.</code></p>
<p>What's the RBD currently made of?</p>
<pre>ceph04:~ # rados -p rbd ls | grep "^rb.0.0."
rb.0.0.000000000000
rb.0.0.000000000020
rb.0.0.000000000021
rb.0.0.000000000040
rb.0.0.000000000042
rb.0.0.000000000060
rb.0.0.000000000063
rb.0.0.000000000080
rb.0.0.000000000081
rb.0.0.000000000082
rb.0.0.000000000083
rb.0.0.000000000084
rb.0.0.000000000085
rb.0.0.000000000086
rb.0.0.000000000087
rb.0.0.000000000088
rb.0.0.0000000000a0
rb.0.0.0000000000a5
rb.0.0.0000000000c0
rb.0.0.0000000000c6
rb.0.0.0000000000e0
rb.0.0.0000000000e7
rb.0.0.0000000000ff
</pre><p>Now suppose you're interested in where <code>rb.0.0.0000000000a5</code> is.</p>
<p>You first grab an OSD map:</p>
<pre>ceph04:~ # ceph osd getmap -o /tmp/osdmap
2012-03-09 21:31:47.055376 mon &lt;- [osd,getmap]
2012-03-09 21:31:47.056624 mon.1 -&gt; 'got osdmap epoch 187' (0)
 wrote 2273 byte payload to /tmp/osdmap</pre><p>And now you can use <code>osdmaptool</code> to test an object name against the mapfile:</p>
<pre>ceph04:~ # osdmaptool --test-map-object rb.0.0.0000000000a5 /tmp/osdmap 
osdmaptool: osdmap file '/tmp/osdmap'
 object 'rb.0.0.0000000000a5' -&gt; 0.7ea1 -&gt; [2,0]
</pre><p>... meaning the object lives in Placement Group <code>0.7ea1</code>, of which replicas currently exist in OSDs 2 and 0.</p>
<p>Why do you want to know this? Normally, really, you don't. All the 
replication and distribution happens under the covers without your 
intervention. But you can use this rather neatly if you want to watch 
your data being redistributed as you take out OSDs temporarily, and put 
them back in.</p>

<a name="TurningRbdToSan"></a><h3>Turning Ceph RBD Images into SAN Storage Devices 
<a href="http://www.hastexo.com/resources/hints-and-kinks/turning-ceph-rbd-images-san-storage-devices" target="newwindow">(Source Origin)</a></h3>
                                
    <p>RADOS Block Device (RBD) is a <a href="http://ceph.com/ceph-storage/block-storage/" target="_blank">block-layer interface</a> to the <a href="http://www.hastexo.com/knowledge/storage-io/ceph">Ceph</a> distributed storage stack. Here's how you can enhance RBD with SAN storage device compatibility, like <a href="http://en.wikipedia.org/wiki/ISCSI" target="_blank">iSCSI</a> and <a href="http://en.wikipedia.org/wiki/Fibre_Channel" target="_blank">Fibre Channel</a>, to connect systems with no native RBD support to your Ceph cluster.</p>
<h4>Prerequisites</h4>
<p>What you'll need in order to accomplish SAN compatibility for your Ceph cluster is this:</p>
<ul>
<li>A working Ceph cluster. You probably guessed this one. More specifically, you should have&nbsp;
<ul>
<li>a RADOS pool in which you can create RBD images; the default rbd pool will do nicely.&nbsp;</li>
<li>a set of credentials for a client to connect to the cluster, and 
create and map RBD devices. You can use the default client.admin 
credentials for this purpose, but I prefer to use a separate set of 
credentials for client.rbd.</li>
<li>that user should have at least the allow r permission on your mons, 
and allow rw on your osds (the latter you can restrict to the rbd pool 
if you wish).</li>
</ul>
</li>
<li>A SCSI proxy node, which will act as an intermediary between your legacy initiators and the Ceph cluster. It should have
<ul>
<li>A sufficiently recent Linux kernel. 2.6.38 is the absolute minimum, but a post-3.2.0 kernel is highly recommended.</li>
<li>A working installation of the client tools required to map RBD devices (the rbd binary is the important one).</li>
<li>A copy of the credentials for your rbd client.</li>
<li>A working installation of the LIO and target tools (lio-utils and targetcli).</li>
</ul>
</li>
<li>And finally, any number of clients supporting <a href="http://linux-iscsi.org/index.php/LIO-Target#Fabric_modules" target="_blank">any of LIO's fabric modules</a>. We'll use iSCSI in this example, but you could also use FibreChannel, FCoE, InfiniBand, and others.</li>
</ul>
<h4>Getting Started</h4>
<p>The first thing we'll need to do is create an RBD image. Suppose we 
would like to create one that is 10GB in size (recall, all RBD images 
are thin-provisioned, so we won't actually use 10GB in the Ceph cluster 
right from the start).</p>
<pre>rbd -n client.rbd -k /etc/ceph/keyring.client.rbd create --size 10240 test</pre><p>This
 means we are connecting to our Ceph mon servers (defined in the default
 configuration file, /etc/ceph/ceph.conf) using the client.rbd identity,
 whose authentication key is stored in /etc/ceph/keyring.client.rbd. The
 nominal image size is 10240MB, and its name is a hardly creative test.</p>
<p>You can run this command from any node inside or outside your Ceph 
cluster, as long as the configuration file and authentication 
credentials are stored in the appropriate location. The next step, 
however, is one that you must complete from your proxy node (the one 
with the lio tools installed):</p>
<pre>modprobe rbd<br>rbd --user rbd --secret /etc/ceph/secret.client.rbd map test</pre><p>Note
 that this syntax applies to the current "stable" Ceph release, 0.48 
"argonaut". Newer releases do away with the somewhat illogical --user 
and --secret syntax, and just allow --id and --keyring which is more in 
line with all other Ceph tools.</p>
<p>Once the map command has completed, you should see a new block device
 named /dev/rbd0 (provided this is the first device you mapped on this 
machine), and a handy symlink of the pattern 
/dev/rbd/&lt;pool&gt;/&lt;image&gt;, in our case /dev/rbd/rbd/test. This
 is a kernel-level block device like any other, and we can now proceed 
by exporting it to the Unified Target infrastructure.</p>
<h4>Exporting the Target</h4>
<p>Once we have our mapped RBD device in place, we can create a target, 
and export it via one of LIO's fabric modules. The targetcli subshell 
comes in very handy for this purpose:</p>
<pre><strong># targetcli</strong> 
Welcome to the targetcli shell:

 Copyright (c) 2011 by RisingTide Systems LLC.

Visit us at <a href="http://www.risingtidesystems.com/" title="http://www.risingtidesystems.com">http://www.risingtidesystems.com</a>.

Loaded tcm_loop kernel module.
Created '/sys/kernel/config/target/loopback'.
Done loading loopback fabric module.
Loaded tcm_fc kernel module.
Created '/sys/kernel/config/target/fc'.
Done loading tcm_fc fabric module.
Can't load fabric module qla2xxx.
Loaded iscsi_target_mod kernel module.
Created '/sys/kernel/config/target/iscsi'.
Done loading iscsi fabric module.
Can't load fabric module ib_srpt.
<strong>/&gt; cd backstores/iblock
/backstores/iblock&gt; create test /dev/rbd/rbd/test</strong>
Generating a wwn serial.
Created iblock storage object test using /dev/rbd/rbd/test.
Entering new node /backstores/iblock/test
<strong>/backstores/iblock/test&gt; status</strong>
Status for /backstores/iblock/test: /dev/rbd/rbd/liotest deactivated
</pre><p>Now we've created a backstore named <code>test</code>, 
corresponding to our mapped RBD image of the same name. At this point it
 is deactivated, as it hasn't been assigned to any iSCSI target.</p>
<p>Up next, we'll create the target, add the backstore as LUN 0, and assign the target to a Target Portal Group (TPG):</p>
<pre><strong>/backstores/iblock&gt; cd ..
/backstores&gt; cd ..
/&gt; cd iscsi 
/iscsi&gt; create</strong>
Created target iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557.
Selected TPG Tag 1.
Successfully created TPG 1.
Entering new node /iscsi/iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557/tpgt1
<strong>/iscsi/iqn.20...8ca9557/tpgt1&gt; cd luns
/iscsi/iqn.20...57/tpgt1/luns&gt; status</strong>
Status for /iscsi/iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557/tpgt1/luns: 0 LUN
<strong>/iscsi/iqn.20...57/tpgt1/luns&gt; create /backstores/iblock/test 
</strong>Selected LUN 0.
Successfully created LUN 0.
Entering new node /iscsi/iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557/tpgt1/luns/lun0
<strong>/iscsi/iqn.20...gt1/luns/lun0&gt; cd ..
/iscsi/iqn.20...57/tpgt1/luns&gt; cd ..
/iscsi/iqn.20...8ca9557/tpgt1&gt; cd portals 
/iscsi/iqn.20...tpgt1/portals&gt; create 192.168.122.117
Using default IP port 3260</strong>
Successfully created network portal 192.168.122.117:3260.
Entering new node /iscsi/iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557/tpgt1/portals/192.168.122.117:3260
</pre><p>So now we have a new target, with the IQN <code>iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557</code>, assigned to a TPG listening on <code>192.168.122.117</code>.</p>
<p>For demonstration purposes, we can now disable authentication and 
initiator filters. You should obviously not do this on a production 
system.</p>
<pre><strong>/iscsi/iqn.20....122.117:3260&gt; cd ..
/iscsi/iqn.20...tpgt1/portals&gt; cd ..
/iscsi/iqn.20...8ca9557/tpgt1&gt; set attribute authentication=0
</strong>Parameter authentication is now '0'.
<strong>/iscsi/iqn.20...8ca9557/tpgt1&gt; set attribute demo_mode_write_protect=0 generate_node_acls=1 cache_dynamic_acls=1
</strong>Parameter demo_mode_write_protect is now '0'.
Parameter generate_node_acls is now '1'.
Parameter cache_dynamic_acls is now '1'.
<strong>/iscsi/iqn.20...8ca9557/tpgt1&gt; exit</strong>
</pre><p>There. Now we have an iSCSI target, a target portal group, and a single LUN assigned to it.</p>
<h4>Using your new target</h4>
<p>And now, you can just connect to this thin-provisioned, dynamically 
replicated, self-healing and self-rebalancing, snapshot capable, striped
 and distributed block device as you would to any other iSCSI target.</p>
<p>Here's an example for the Linux standard open-iscsi tools:</p>
<pre><strong># iscsiadm -m discovery -p 192.168.122.117 -t sendtargets
</strong>192.168.122.117:3260,1 iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557
<strong># iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557 -p 192.168.122.117 --login
</strong>Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557, portal: 192.168.122.117,3260]
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557, portal: 192.168.122.117,3260]: successful
</pre><p>At this point, you'll have a shiny SCSI device showing up under
 lsscsi and in your /dev tree, and this device you can use for anything 
you please. Try partitioning it and making a filesystem on one of the 
partitions.</p>
<p>And when you're done, you just log out:</p>
<pre><strong># iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557 -p 192.168.122.117 --logout
</strong>Logging out of session [sid: 1, target: iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557, portal: 192.168.122.117,3260]
Logout of [sid: 1, target: iqn.2003-01.org.linux-iscsi.gwen.i686:sn.7d9ed8ca9557, portal: 192.168.122.117,3260]: successful
</pre><p>That's it.</p>
<h4>Where To Go From Here</h4>
<p>Now you can start doing pretty nifty stuff.</p>
<p>Have a server that needs extra storage, but runs a legacy Linux 
distro with no native RBD support? Install open-iscsi and provide that 
box with the replicated, striped, self-healing, auto-mirroring capacity 
that Ceph and RBD come with.</p>
<p>Have a Windows box that should somehow start using your massive distributed storage cluster? As long as you have <a href="http://technet.microsoft.com/en-us/library/ee338476%28v=ws.10%29.aspx" target="_blank">Microsoft iSCSI Initiator</a> installed, you can do so in an all-software solution. Or you just get the iSCSI HBA of your choice, and use its Windows driver.</p>
<p>And if you have a server that can boot off iSCSI, you can even run a 
bare-metal install, or build a diskless system that stores all of its 
data in RBD.</p>
<p>Want High Availability for your target proxy? Can do. <a href="http://www.hastexo.com/knowledge/high-availability/pacemaker">Pacemaker</a> has resource agents both for LIO and for RBD. And highly-available iSCSI servers are a relatively old hat; we've <a href="http://www.hastexo.com/products/estimates/storage-iscsi">been doing that with other, less powerful storage replication solutions</a> forever.</p>
